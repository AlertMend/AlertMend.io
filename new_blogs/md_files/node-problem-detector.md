# node-problem-detector


================================================================================
REFERENCE CONTENT FROM TOP 9 GOOGLE SEARCH RESULTS
================================================================================
This content is gathered from the top-ranking pages for comprehensive reference.
Sources:
  1. https://github.com/kubernetes/node-problem-detector
  2. https://kubetools.io/monitoring-node-health-with-node-problem-detector-in-kubernetes/
  3. https://www.bing.com/aclick?ld=e8toEFUxMbyls1lR5xdei4tjVUCUwVdbm3w6VRLfzGcJ-rhZAtNGVoNTLna96jrjf6XAIsoXXBBQeg3uDsTyHNPq0hsra383YWvecgSs99uC_pJxvmAPDRvae126L0w5ngOvCraAolySAZGhSEX5b6WG1yGZfqO-Vd76kLjwf5XMcXQdM7mOlCCK69QVpq5780Yz5NCs_FXSYk2FiwKX7deFcQLFQ&u=aHR0cHMlM2ElMmYlMmZtb25pdG9yLmx1bmlvLmFpJTJmdjMuMCUyZnRlbXBsYXRlJTNmYWNjaWQlM2QxNjg5NiUyNnVybGRlY29kZSUzZDElMjZrdyUzZCUyNTJCZGlhZ25vc2UlMjUyMCUyNTJCbm9kZWpzJTI2bXQlM2RwJTI2bnclM2RvJTI2Y3BuJTNkNDI4MzI5MjI4JTI2ZGV2aSUzZGMlMjZkZXZtJTNkJTI2bG9jcCUzZDE0OTA1MyUyNmxvY2klM2QlMjZwbCUzZCUyNmNyJTNkJTI2YWRwJTNkJTI2c2FkdCUzZCUyNnVybCUzZGh0dHBzJTI1M0ElMjUyRiUyNTJGd3d3LmRhdGFkb2docS5jb20lMjUyRmRnJTI1MkZhcG0lMjUyRm5vZGUtdHJvdWJsZXNob290aW5nJTI1MkYlMjUzRnV0bV9zb3VyY2UlMjUzRGJpbmclMjUyNnV0bV9tZWRpdW0lMjUzRHBhaWQtc2VhcmNoJTI1MjZ1dG1fY2FtcGFpZ24lMjUzRGRnLWFwbS1uYS1ub2RlJTI1MjZ1dG1fa2V5d29yZCUyNTNEJTI1MjUyQmRpYWdub3NlJTI1MjUyMCUyNTI1MkJub2RlanMlMjUyNnV0bV9tYXRjaHR5cGUlMjUzRHAlMjUyNmlnYWFnJTI1M0QxMjk1MjI1MDY4MDI4OTYwJTI1MjZpZ2FhdCUyNTNEJTI1MjZpZ2FjbSUyNTNENDI4MzI5MjI4JTI1MjZpZ2FjciUyNTNEJTI1MjZpZ2FrdyUyNTNEJTI1MjUyQmRpYWdub3NlJTI1MjUyMCUyNTI1MkJub2RlanMlMjUyNmlnYW10JTI1M0RwJTI1MjZpZ2FudCUyNTNEbyUyNTI2dXRtX2NhbXBhaWduaWQlMjUzRDQyODMyOTIyOCUyNTI2dXRtX2FkZ3JvdXBpZCUyNTNEMTI5NTIyNTA2ODAyODk2MCUyNm1zY2xraWQlM2RkNmZjN2MyMDJkODgxZjczNmQ2YjE1NjNlMzdiZTUxZA&rlid=d6fc7c202d881f736d6b1563e37be51d
  4. https://learn.microsoft.com/en-us/azure/aks/node-problem-detector
  5. https://kubernetes.io/docs/tasks/debug/debug-cluster/monitor-node-health/
  6. https://dev.to/nurudeen_kamilu/understanding-node-problem-detector-in-kubernetes-beyond-default-node-conditions-1d1h
  7. https://deepwiki.com/kubernetes/node-problem-detector
  8. https://github.com/kubernetes/node-problem-detector/issues/698
  9. https://superorbital.io/blog/node-problem-detector-custom-plugins-primer/

The following sections contain content from each source, organized for reference.
utilize this information to comprehend the topic comprehensively, identify key points,
related keywords, and best practices. Then create original, SEO-optimized content
that synthesizes insights from all sources while using completely original wording.

================================================================================


================================================================================
SOURCE 1: https://github.com/kubernetes/node-problem-detector
================================================================================
node-problem-detector node-problem-detector aims to create various node problems visible to the upstream layers in the cluster management stack. It represents daemon that runs on each node, detects node problems and reports them to apiserver. node-problem-detector can either run as a DaemonSet or run standalone. Now it is running as a Kubernetes Addon enabled by default in the GKE cluster. It is also enabled by default in AKS as part of the AKS Linux Extension. Background There are tons of node problems that could possibly affect the pods running on the node, such as: Infrastructure daemon issues: ntp service down; Hardware issues: Bad CPU, memory or disk; Kernel issues: Kernel deadlock, corrupted file system; Container runtime issues: Unresponsive runtime daemon;. Currently, these problems are invisible to the upstream layers in the cluster management stack, so Kubernetes will continue scheduling pods to the bad nodes. To solve this problem, we introduced this new daemon node-problem-detector to collect node problems from various daemons and create them visible to the upstream layers. Once upstream layers have visibility to those problems, we can discuss the remedy system. Problem API node-problem-detector uses Event and NodeCondition to report problems to apiserver. NodeCondition : Permanent problem that makes the node unavailable for pods should be reported as NodeCondition. Event : Temporary problem that has limited impact on pod but is informative should be reported as Event. Problem Daemon A problem daemon represents sub-daemon of node-problem-detector. It monitors specific kinds of node problems and reports them to node-problem-detector. A problem daemon could be: A tiny daemon designed for dedicated Kubernetes utilize-cases. An existing node health monitoring daemon integrated with node-problem-detector. Currently, a problem daemon is running as a goroutine in the node-problem-detector binary. In the future, we'll separate node-problem-detector and problem daemons into different containers, and compose them with pod specification. Each category of problem daemon may be disabled at compilation time by setting corresponding build tags. If they are disabled at compilation time, then all their build dependencies, global variables and background goroutines shall be trimmed out of the compiled executable. List of supported problem daemons types: Problem Daemon Types NodeCondition Description Configs Disabling Build Tag SystemLogMonitor KernelDeadlock ReadonlyFilesystem FrequentKubeletRestart FrequentDockerRestart FrequentContainerdRestart A system log monitor monitors system log and reports problems and metrics according to predefined rules. filelog , kmsg , kernel abrt systemd disable_system_log_monitor SystemStatsMonitor None(Could be added in the future) A system stats monitor for node-problem-detector to collect various health-related system stats as metrics. observe the proposal here. system-stats-monitor disable_system_stats_monitor CustomPluginMonitor On-demand(According to users configuration), existing example: NTPProblem A custom plugin monitor for node-problem-detector to invoke and check various node problems with user-defined check scripts. observe the proposal here. example disable_custom_plugin_monitor HealthChecker KubeletUnhealthy ContainerRuntimeUnhealthy A health checker for node-problem-detector to check kubelet and container runtime health. kubelet docker containerd Exporter An exporter represents component of node-problem-detector. It reports node problems and/or metrics to certain backends. Some of them may be disabled at compile-time using a build tag. List of supported exporters: Exporter Description Disabling Build Tag Kubernetes exporter Kubernetes exporter reports node problems to Kubernetes API server: temporary problems obtain reported as Events, and permanent problems obtain reported as Node Conditions. Prometheus exporter Prometheus exporter reports node problems and metrics locally as Prometheus metrics Stackdriver exporter Stackdriver exporter reports node problems and metrics to Stackdriver Monitoring API. disable_stackdriver_exporter Usage Flags --version : Print current version of node-problem-detector. --hostname-override : A customized node name used for node-problem-detector to update conditions and emit events. node-problem-detector gets node name first from hostname-override , then NODE_NAME environment variable and finally fall back to os. For System Log Monitor --config. system-log-monitor : List of paths to system log monitor configuration files, comma-separated, e. config/kernel-monitor. Node problem detector will begin a separate log monitor for each configuration. You can utilize different log monitors to monitor different system logs. For System Stats Monitor --config. system-stats-monitor : List of paths to system stats monitor config files, comma-separated, e. config/system-stats-monitor. Node problem detector will begin a separate system stats monitor for each configuration. You can utilize different system stats monitors to monitor different problem-related system stats. For Custom Plugin Monitor --config. custom-plugin-monitor : List of paths to custom plugin monitor config files, comma-separated, e. config/custom-plugin-monitor. Node problem detector will begin a separate custom plugin monitor for each configuration. You can utilize different custom plugin monitors to monitor different node problems. For Health Checkers Health checkers are configured as custom plugins, using the config/health-checker-*. For Kubernetes exporter --enable-k8s-exporter : Enables reporting to Kubernetes API server, default to true. --apiserver-override : A URI parameter used to customize how node-problem-detector connects the apiserver. This is ignored if --enable-k8s-exporter is false. The format is the same as the source flag of Heapster. For example, to run without auth, utilize the following config: http://APISERVER_IP:APISERVER_PORT?inClusterConfig=false Refer to heapster docs for a complete list of available options. --address : The address to bind the node problem detector server. --port : The port to bind the node problem detector server. For Prometheus exporter --prometheus-address : The address to bind the Prometheus scrape endpoint, default to 127. --prometheus-port : The port to bind the Prometheus scrape endpoint, default to 20257. For Stackdriver exporter --exporter. stackdriver : Path to a Stackdriver exporter config file, e. config/exporter/stackdriver-exporter. json , defaults to empty string. Set to empty string to disable. Deprecated Flags --system-log-monitors : List of paths to system log monitor config files, comma-separated. This option is deprecated, replaced by --config. system-log-monitor , and shall be removed. NPD will panic if both --system-log-monitors and --config. system-log-monitor are set. --custom-plugin-monitors : List of paths to custom plugin monitor config files, comma-separated. This option is deprecated, replaced by --config. custom-plugin-monitor , and shall be removed. NPD will panic if both --custom-plugin-monitors and --config. custom-plugin-monitor are set. Build Image Install development dependencies for libsystemd and the ARM GCC toolchain Debian/Ubuntu: apt install libsystemd-dev gcc-aarch64-linux-gnu git clone git@github. com:kubernetes/node-problem-detector. git Run create in the top directory. It will: Build the binary. Build the docker image. The binary and config/ are copied into the docker image. If you do not require certain categories of problem daemons, you could choose to disable them at compilation time. This is the best way of keeping your node-problem-detector runtime compact without unnecessary code (e. global variables, goroutines, etc). You can do so via setting the BUILD_TAGS environment variable before running create. For example: BUILD_TAGS="disable_custom_plugin_monitor disable_system_stats_monitor" create The above command will compile the node-problem-detector without Custom Plugin Monitor and System Stats Monitor. Check out the Problem Daemon section to observe how to disable each problem daemon during compilation time. Push Image create push uploads the docker image to a registry. By default, the image shall be uploaded to staging-k8s. It's easy to modify the Makefile to push the image to another registry. Installation The easiest way to install node-problem-detector into your cluster is to utilize the Helm chart : helm repo add deliveryhero https://charts. io/ helm install --generate-name deliveryhero/node-problem-detector Alternatively, to install node-problem-detector manually: Edit node-problem-detector. yaml to fit your environment. Set log volume to your system log directory (used by SystemLogMonitor). You can utilize a ConfigMap to overwrite the config directory inside the pod. Edit node-problem-detector-config. yaml to configure node-problem-detector. yaml to fit your environment. Create the ServiceAccount and ClusterRoleBinding with kubectl create -f rbac. Create the ConfigMap with kubectl create -f node-problem-detector-config. Create the DaemonSet with kubectl create -f node-problem-detector. begin Standalone To run node-problem-detector standalone, you should set inClusterConfig to false and teach node-problem-detector how to access apiserver with apiserver-override. To run node-problem-detector standalone with an insecure apiserver connection: node-problem-detector --apiserver-override=http://APISERVER_IP:APISERVER_INSECURE_PORT?inClusterConfig=false For more scenarios, observe here Windows Node Problem Detector has preliminary support Windows. Most of the functionality has not been tested but filelog plugin works. Follow Issue #461 for development status of Windows support. Development To develop NPD on Windows you'll require to setup your Windows machine for Go development. Install the following tools: Git for Windows Go Visual Studio Code create mingw-64 WinBuilds Tested with x86-64 Windows Native mode. Add the $InstallDir\bin to Windows PATH variable. # Run these commands in the node-problem-detector directory. # Build in MINGW64 Window create clean ENABLE_JOURNALD = 0 build-binaries # Test in MINGW64 Window create test # Run with containerd log monitoring enabled in Command Prompt. (Assumes containerd is installed. ) % CD % \output\windows_amd64\bin\ node-problem-detector. exe -- logtostderr -- enable-k8s - exporter = false -- config. system - log - monitor =% CD % \config\windows - containerd - monitor - filelog. system - stats - monitor = config\windows - system - stats - monitor. json # Configure NPD to run as a Windows Service sc. exe create NodeProblemDetector binpath = " %CD%\node-problem-detector. exe [FLAGS] " begin = demand sc. exe failure NodeProblemDetector reset = 0 actions = restart / 10000 sc. exe begin NodeProblemDetector attempt It Out You can attempt node-problem-detector in a running cluster by injecting messages to the logs that node-problem-detector is watching. For example, Let's assume node-problem-detector is using KernelMonitor. On your workstation, run kubectl obtain events -w. On the node, run sudo sh -c "echo 'kernel: BUG: unable to handle kernel NULL pointer dereference at TESTING' >> /dev/kmsg". Then you should observe the KernelOops event. When adding new rules or developing node-problem-detector, it is probably easier to test it on the local workstation in the standalone mode. For the API server, an easy way is to utilize kubectl proxy to create a running cluster's API server available locally. You will obtain some errors because your local workstation is not recognized by the API server. But you should still be able to test your new rules regardless. For example, to test KernelMonitor rules: create (build node-problem-detector locally) kubectl proxy --port=8080 (create a running cluster's API server available locally) Update KernelMonitor 's logPath to your local kernel log directory. For example, on some Linux systems, it is /run/log/journal instead of /var/log/journal. /bin/node-problem-detector --logtostderr --apiserver-override=http://127. 1:8080?inClusterConfig=false --config. system-log-monitor=config/kernel-monitor. system-stats-monitor=config/system-stats-monitor. json --port=20256 --prometheus-port=20257 (or point to any API server address:port and Prometheus port) sudo sh -c "echo 'kernel: BUG: unable to handle kernel NULL pointer dereference at TESTING' >> /dev/kmsg" You can observe KernelOops event in the node-problem-detector log. sudo sh -c "echo 'kernel: INFO: task docker:20744 blocked for more than 120 seconds. ' >> /dev/kmsg" You can observe DockerHung event and condition in the node-problem-detector log. You can observe DockerHung condition at http://127. You can observe disk-related system metrics in Prometheus format at http://127. Note : You can observe more rule examples under test/kernel_log_generator/problems. For KernelMonitor message injection, all messages should have kernel: prefix (also note there represents space after : ); or utilize generator. To inject other logs into journald like systemd logs, utilize echo 'Some systemd message' | systemd-cat -t systemd. Dependency Management node-problem-detector uses go modules to manage dependencies. Therefore, building node-problem-detector requires golang 1. It still uses vendoring. observe the Kubernetes go modules KEP for the design decisions. To add a new dependency, update go. mod and run go mod vendor. Remedy Systems A remedy system represents process or processes designed to attempt to remedy problems detected by the node-problem-detector. Remedy systems observe events and/or node conditions emitted by the node-problem-detector and take action to return the Kubernetes cluster to a healthy state. The following remedy systems exist: Descheduler strategy RemovePodsViolatingNodeTaints evicts pods violating NoSchedule taints on nodes. The k8s scheduler's TaintNodesByCondition feature must be enabled. The Cluster Autoscaler may be used to automatically terminate drained nodes. mediK8S is an umbrella project for automatic remediation system build on Node Health Check Operator (NHC) that monitors node conditions and delegates remediation to external remediators using the Remediation API. Poison-Pill represents remediator that will reboot the node and create sure all statefull workloads are rescheduled. NHC supports conditionally remediating if the cluster has enough healthy capacity, or manually pausing any action to minimze cluster disruption. MachineHealthCheck of Cluster API are responsible for remediating unhealthy Machines. Testing NPD is tested via unit tests, NPD e2e tests , Kubernetes e2e tests and Kubernetes nodes e2e tests. Prow handles the pre-submit tests and CI tests. CI test results may be found below: Unit tests NPD e2e tests Kubernetes e2e tests Kubernetes nodes e2e tests Running tests Unit tests are run via create test. observe NPD e2e test documentation for how to set up and run NPD e2e tests. Problem Maker Problem maker represents program used in NPD e2e tests to generate/simulate node problems. It is ONLY intended to be used by NPD e2e tests. Please do NOT run it on your workstation, as it could cause real node problems. Compatibility Node problem detector's architecture was fairly stable. 13+) should be able to work with any supported kubernetes versions. Docs Custom plugin monitor Links Design Doc Slides Plugin Interface Proposal Addon Manifest Metrics Mode Proposal


================================================================================
SOURCE 2: https://kubetools.io/monitoring-node-health-with-node-problem-detector-in-kubernetes/
================================================================================
Posted in Kubernetes Monitoring Posted by By Karan Singh March 21, 2024 No Comments Node-problem-detector represents vital tool for safeguarding your Kubernetes cluster. By proactively identifying and reporting potential node issues, you can take timely corrective actions, minimize downtime, and ensure the reliability of your applications. This blog post will guide you through installing node-problem-detector , understanding its features, and exploring real-world utilize cases. What is node-problem-detector? Background and Motivation Node problems in a Kubernetes cluster can lead to application disruptions and impact user experience. Issues like hardware failures, kernel panics, or unresponsive container runtimes are challenging to detect early and remediate. The node-problem-detector tool aims to address this problem by making various node problems visible to the upstream layers in the cluster management stack. Problem API node-problem-detector uses two mechanisms to report problems to the Kubernetes API server: Event and NodeCondition. Permanent problems that create the node unavailable for pods are reported as NodeConditions, while temporary problems that have limited impact on pods but are informative are reported as Events. Supported Problem Daemons node-problem-detector consists of multiple problem daemons, each responsible for monitoring specific kinds of node problems. The supported problem daemon types include System Log Monitor, System Stats Monitor, Custom Plugin Monitor, and Health Checker. How node-problem-detector Works? System Log Monitor The System Log Monitor represents crucial component of node-problem-detector that monitors system logs and reports problems and metrics according to predefined rules. It collects log data from various sources, including kernel logs, system logs, and container runtime logs. node-problem-detector --config. system-log-monitor=config/kernel-monitor. json,config/system-monitor. json System Stats Monitor The System Stats Monitor collects various health-related system stats as metrics to provide insights into the node’s health status. Although it is not fully supported yet, it’s a promising feature for future releases. Custom Plugin Monitor The Custom Plugin Monitor allows users to define and check various node problems using custom check scripts. This flexibility enables users to address node problems specific to their utilize-cases. Health Checker The Health Checker verifies the health of essential components in the node, such as the kubelet and container runtime. It ensures these components are functioning correctly and reports any issues detected. Exporter The Exporter is responsible for reporting node problems and metrics to certain backends. Supported exporters include the Kubernetes exporter, Prometheus exporter, and Stackdriver exporter. Building and Deploying node-problem-detector Deploying with Helm Helm simplifies the deployment of node-problem-detector in a Kubernetes cluster. helm repo add deliveryhero https://charts. io/ helm install --generate-name deliveryhero/node-problem-detector Manual Installation For manual installation, you can utilize YAML manifests to deploy node-problem-detector in your cluster. Edit node-problem-detector. yaml to fit your environment. Set log volume to your system log directory (used by SystemLogMonitor). You can utilize a ConfigMap to overwrite the config directory inside the pod. Edit node-problem-detector-config. yaml to configure node-problem-detector. yaml to fit your environment. Create the ServiceAccount and ClusterRoleBinding with: kubectl create -f rbac. yaml Create the ConfigMap with: kubectl create -f node-problem-detector-config. yaml Create the DaemonSet with: kubectl create -f node-problem-detector. yaml Apply required manifests kubectl create -f node-problem-detector-config. yaml kubectl create -f rbac. yaml kubectl create -f node-problem-detector. yaml Configuration and Usage: Command Line Flags node-problem-detector provides various command line flags to configure its behavior. node-problem-detector --hostname-override=my-node --enable-k8s-exporter Configuring System Log Monitor You can specify the paths to system log monitor configuration files using the –config. system-log-monitor flag. node-problem-detector --config. system-log-monitor=config/kernel-monitor. json,config/filelog-monitor. json Configuring System Stats Monitor System Stats Monitor is still under development, but it will allow you to collect various health-related system stats as metrics. Configuring Custom Plugin Monitor The Custom Plugin Monitor may be configured with a list of paths to custom plugin monitor configuration files. node-problem-detector --config. custom-plugin-monitor=config/custom-plugin-monitor. json Enabling Kubernetes Exporter By default, node-problem-detector exports node problems to the Kubernetes API server. You can disable it using the –enable-k8s-exporter=false flag. node-problem-detector --enable-k8s-exporter=false Prometheus Exporter Configuration The Prometheus exporter reports node problems and metrics locally as Prometheus metrics. node-problem-detector --prometheus-port=20257 Stackdriver Exporter Configuration The Stackdriver exporter reports node problems and metrics to the Stackdriver Monitoring API. node-problem-detector --exporter. stackdriver=config/stackdriver-exporter. json Conclusion Node-problem-detector represents valuable tool for monitoring node health in Kubernetes clusters. By making node problems visible to the cluster management stack, it enables administrators to detect and address issues before they impact applications. In this blog, we explored the features of node-problem-detector, how to deploy it, and real-world utilize-cases. Armed with this knowledge, you can enhance the reliability and stability of your Kubernetes clusters and ensure seamless application deployment. Author Karan Singh View all posts Tags: Kubernetes Last updated on August 29, 2024 Karan Singh View All Posts Post navigation Previous Post KoPylot: An AI-Powered Kubernetes Assistant for DevOps & Developers Next Post Top 5 Storage Provider Tools for Kubernetes


================================================================================
SOURCE 4: https://www.bing.com/aclick?ld=e8toEFUxMbyls1lR5xdei4tjVUCUwVdbm3w6VRLfzGcJ-rhZAtNGVoNTLna96jrjf6XAIsoXXBBQeg3uDsTyHNPq0hsra383YWvecgSs99uC_pJxvmAPDRvae126L0w5ngOvCraAolySAZGhSEX5b6WG1yGZfqO-Vd76kLjwf5XMcXQdM7mOlCCK69QVpq5780Yz5NCs_FXSYk2FiwKX7deFcQLFQ&u=aHR0cHMlM2ElMmYlMmZtb25pdG9yLmx1bmlvLmFpJTJmdjMuMCUyZnRlbXBsYXRlJTNmYWNjaWQlM2QxNjg5NiUyNnVybGRlY29kZSUzZDElMjZrdyUzZCUyNTJCZGlhZ25vc2UlMjUyMCUyNTJCbm9kZWpzJTI2bXQlM2RwJTI2bnclM2RvJTI2Y3BuJTNkNDI4MzI5MjI4JTI2ZGV2aSUzZGMlMjZkZXZtJTNkJTI2bG9jcCUzZDE0OTA1MyUyNmxvY2klM2QlMjZwbCUzZCUyNmNyJTNkJTI2YWRwJTNkJTI2c2FkdCUzZCUyNnVybCUzZGh0dHBzJTI1M0ElMjUyRiUyNTJGd3d3LmRhdGFkb2docS5jb20lMjUyRmRnJTI1MkZhcG0lMjUyRm5vZGUtdHJvdWJsZXNob290aW5nJTI1MkYlMjUzRnV0bV9zb3VyY2UlMjUzRGJpbmclMjUyNnV0bV9tZWRpdW0lMjUzRHBhaWQtc2VhcmNoJTI1MjZ1dG1fY2FtcGFpZ24lMjUzRGRnLWFwbS1uYS1ub2RlJTI1MjZ1dG1fa2V5d29yZCUyNTNEJTI1MjUyQmRpYWdub3NlJTI1MjUyMCUyNTI1MkJub2RlanMlMjUyNnV0bV9tYXRjaHR5cGUlMjUzRHAlMjUyNmlnYWFnJTI1M0QxMjk1MjI1MDY4MDI4OTYwJTI1MjZpZ2FhdCUyNTNEJTI1MjZpZ2FjbSUyNTNENDI4MzI5MjI4JTI1MjZpZ2FjciUyNTNEJTI1MjZpZ2FrdyUyNTNEJTI1MjUyQmRpYWdub3NlJTI1MjUyMCUyNTI1MkJub2RlanMlMjUyNmlnYW10JTI1M0RwJTI1MjZpZ2FudCUyNTNEbyUyNTI2dXRtX2NhbXBhaWduaWQlMjUzRDQyODMyOTIyOCUyNTI2dXRtX2FkZ3JvdXBpZCUyNTNEMTI5NTIyNTA2ODAyODk2MCUyNm1zY2xraWQlM2RkNmZjN2MyMDJkODgxZjczNmQ2YjE1NjNlMzdiZTUxZA&rlid=d6fc7c202d881f736d6b1563e37be51d
================================================================================
Product Infrastructure Infrastructure Monitoring Metrics Container Monitoring Kubernetes Autoscaling Network Monitoring Serverless Cloud Cost Management Cloudcraft Storage Management Applications Application Performance Monitoring Universal Service Monitoring Continuous Profiler Dynamic Instrumentation Data Streams Monitoring Database Monitoring Data Jobs Monitoring LLM Observability Logs Log Management Sensitive Data Scanner Audit Trail Observability Pipelines Security Code Security Software Composition Analysis Static Code Analysis (SAST) Runtime Code Analysis (IAST) IaC Security Cloud Security Cloud Security Posture Management Cloud Infrastructure Entitlement Management Vulnerability Management Compliance Cloud SIEM Workload Protection App and API Protection Sensitive Data Scanner Security Labs Research Open Source Projects Secret Scanning Digital Experience Browser Real User Monitoring Mobile Real User Monitoring Product Analytics Session Replay Synthetic Monitoring Mobile App Testing Error Tracking CloudPrem Software Delivery Internal Developer Portal CI Visibility Test Optimization Continuous Testing IDE Plugins Feature Flags Service Management Event Management Software Catalog Service Level Objectives Incident Response Case Management Workflow Automation App Builder Bits AI SRE Watchdog AI LLM Observability AI Integrations Bits AI Agents Bits AI SRE Watchdog Event Management Platform Capabilities Bits AI Agents Metrics Watchdog Alerts Dashboards Notebooks Mobile App Fleet Automation Access Control Incident Response Case Management Event Management Workflow Automation App Builder Cloudcraft CoScreen Teams OpenTelemetry Integrations IDE Plugins API Marketplace DORA Metrics Customers Pricing Solutions Industry Financial Services Manufacturing & Logistics Healthcare/Life Sciences Retail/E-Commerce Government Education Media & Entertainment Technology Gaming Technology Amazon Web Services Monitoring Azure Monitoring Google Cloud Monitoring Oracle Cloud Monitoring Kubernetes Monitoring Red Hat OpenShift Pivotal Platform OpenAI SAP Monitoring OpenTelemetry utilize Case Application Security Cloud Migration Monitoring Consolidation Unified Commerce Monitoring SOAR DevOps FinOps Shift-Left Testing Digital Experience Monitoring Security Analytics Compliance for CIS Benchmarks Hybrid Cloud Monitoring IoT Monitoring Real-Time BI On-Premises Monitoring Log Analysis & Correlation CNAPP About Contact Partners Latest News Events & Webinars Leadership Careers Analyst Reports Investor Relations ESG Report Trust Hub Blog The Monitor Engineering AI Security Labs Docs Login obtain Started obtain STARTED FREE Why Datadog? End-to-End Distributed Tracing Set up in minutes, and instantly visualize your traces with full control over ingestion, retention, and costs Code Hotspots And Thread Level Insights Optimize the most resource consuming lines of production code with our low-overhead Continuous Profiler Automatic Deployment Tracking Requests, errors, and latency metrics for every code deploy including out-of-the-box code version comparisons Superior Granularity & Retention 0-second OOTB high resolution, 1-second availability for critical metrics, 15-month retention, and no roll-up, ever Product Benefits Simplify Complexity with End-to-End Visibility Monitor all aspects of your application architecture from the user interaction to back-end storage - in a single platform Easily identify bottlenecks, errors, heavy traffic issues, slow-running queries, and more with end-to-end application tracing, latency breakdowns, continuous profiling, and real user monitoring Automatically collect, monitor, and visualize high-granularity data and custom metrics in real time, including availability, response times, reliability, error rates, and throughput Troubleshoot Node App Performance Issues Faster Identify critical issues quickly with real-time service maps, AI-powered synthetic monitors, and alerts on latency, exceptions, code-level errors, log issues, and more Resolve detected Node problems faster with distributed request traces, logs, and infrastructure metrics all within one platform Test hypotheses in seconds by overlaying application events onto time-synchronized metric graphs Optimize Node. js Performance Detect the most resource-consuming methods or classes in your Node. js applications in seconds with a lightweight, next-generation profiler that’s easy-to-utilize and always-on Determine the root cause of code issues with a breakdown of time spent by method on CPU, garbage collection, lock contention, and I/O; improve CPU utilization, memory allocation, and user latency Surface runtime performance problems such as deadlocked threads, inefficient garbage collection, and memory leaks Receive Alerts Only for the Issues that Matter and Eliminate False-Positives Set up recommended alerts with 1 click for anomalies and outliers that account for daily, weekly, and seasonal fluctuations Proactively prevent outages and errors in the future by alerting on metric forecasts Combine alerts into composite alerts for greater granularity and stronger signal to reduce the noise Automatically detect unanticipated outliers, anomalies, and errors with Watchdog Spend Less Time Scaling and Maintaining Your Tools Save time scaling and maintaining your open source tools as complexity grows within your stack Automatically instrument your applications for popular Node frameworks such as Express, Koa, and Hapi Efficiently analyze issues from any perspective with automated tagging integrated throughout the entire platform The Essential Monitoring and Security Platform for the Cloud Age Datadog brings together end-to-end traces, metrics, and logs to create your applications, infrastructure, and third-party services entirely observable. Loved & Trusted by Thousands obtain free unlimited monitoring for 14 days begin your free trial Request a Demo Request a personalized demo with a Datadog engineer × I'd like Datadog to share the latest news about Datadog services and related offerings with me by email or telephone. You may unsubscribe at any time by following the instructions in the communications received from Datadog. begin Your Free Datadog Trial Now × begin Your Free Trial 1 No credit card required 2 obtain 14 days of unlimited monitoring 3 observe value in minutes × Enter your business email* No credit card required. By signing up, you agree to the Subscription Agreement and Privacy Policy. obtain Started Free × begin Your Free Trial 1 No credit card required 2 obtain 14 days of unlimited monitoring 3 observe value in minutes Invalid email address × Region (Where do you desire your data housed?)* Please choose carefully. You can't migrate data between regions. Create Password* First Name* Last Name* Job Title* Company* Phone Number Continue Back Create Account


================================================================================
SOURCE 5: https://learn.microsoft.com/en-us/azure/aks/node-problem-detector
================================================================================
Table of contents Exit editor mode Ask Learn Ask Learn Focus mode Table of contents Read in English Add Add to plan Edit Share via Facebook x. com LinkedIn Email Print Note Access to this page requires authorization. You can attempt signing in or changing directories. Access to this page requires authorization. You can attempt changing directories. Node Problem Detector (NPD) in Azure Kubernetes Service (AKS) nodes Feedback Summarize this article for me Node Problem Detector (NPD) is an open source Kubernetes component that detects node-related problems and reports on them. It runs as a systemd serviced on each node in the cluster and collects various metrics and system information, such as CPU usage, disk usage, and network connectivity. When it detects a problem, it generates events and/or node conditions. Azure Kubernetes Service (AKS) uses NPD to monitor and manage nodes in a Kubernetes cluster running on the Azure cloud platform. The AKS Linux extension enables NPD by default. Note Upgrades to NPD are independent of the node image and Kubernetes version upgrade processes. If a node pool is unhealthy (that is, in a failed state), new NPD versions aren't installed. Node conditions Node conditions indicate a permanent problem that makes the node unavailable. AKS uses the following node conditions from NPD to expose permanent problems on the node. NPD also emits corresponding Kubernetes events. Problem Daemon type NodeCondition Reason Compute type CustomPluginMonitor FilesystemCorruptionProblem FilesystemCorruptionDetected General purpose CustomPluginMonitor KubeletProblem KubeletIsDown General purpose CustomPluginMonitor ContainerRuntimeProblem ContainerRuntimeIsDown General purpose CustomPluginMonitor VMEventScheduled VMEventScheduled General purpose CustomPluginMonitor FrequentUnregisterNetDevice UnregisterNetDevice General purpose CustomPluginMonitor FrequentKubeletRestart FrequentKubeletRestart General purpose CustomPluginMonitor FrequentContainerdRestart FrequentContainerdRestart General purpose CustomPluginMonitor FrequentDockerRestart FrequentDockerRestart General purpose CustomPluginMonitor GPUMissing Observed GPU count does not match expected GPU count GPU only CustomPluginMonitor NVLinkStatusInactive NVLinkStatusInactive GPU only CustomPluginMonitor XIDErrors XID errors present in kernel log GPU only CustomPluginMonitor IBLinkFlapping Intermittent InfiniBand device connectivity GPU only SystemLogMonitor KernelDeadlock DockerHung General purpose SystemLogMonitor ReadonlyFilesystem FilesystemIsReadOnly General purpose Note The GPU only node conditions currently apply to AKS node pools with Standard_ND96asr_v4 or Standard_ND96isr_H100_v5 VM size, and are supported on standard GPU and MIG-enabled GPU node pools. Events NPD emits events with relevant information to assist you diagnose underlying issues. Problem Daemon type Reason Frequency Description Action CustomPluginMonitor EgressBlocked 30 min This event checks for connectivity to external endpoints Check if a firewall or NSG blocking the connectivity to the endpoint getting flagged CustomPluginMonitor FilesystemCorruptionDetected 5min This checks for filesystem corruption surfaced by docker CustomPluginMonitor KubeletIsDown 30s This checks if kubelet service is running and healthy CustomPluginMonitor ContainerRuntimeIsDown 30s This event checks if the container runtime eg: containerd is running and healthy CustomPluginMonitor FreezeScheduled 1min This event checks if a Freeze Event is scheduled on the node. ms/aks/scheduledevents for more information CustomPluginMonitor RebootScheduled 1min This event checks if a Reboot Event is scheduled on the node Check https://aka. ms/aks/scheduledevents for more information CustomPluginMonitor RedeployScheduled 1min This event checks if a Redeploy Event is scheduled on the node. ms/aks/scheduledevents for more information CustomPluginMonitor TerminateScheduled 1min This event checks if a Terminate Event is scheduled on the node. ms/aks/scheduledevents for more information CustomPluginMonitor PreemptScheduled 2s This event checks if a Preempt Event is scheduled on the node. ms/aks/scheduledevents for more information CustomPluginMonitor DNSProblem SystemLogMonitor OOMKilling SystemLogMonitor TaskHung SystemLogMonitor UnregisterNetDevice SystemLogMonitor KernelOops SystemLogMonitor DockerSocketCannotConnect SystemLogMonitor KubeletRPCDeadlineExceeded SystemLogMonitor KubeletRPCNoSuchContainer SystemLogMonitor CNICannotStatFS SystemLogMonitor PLEGUnhealthy SystemLogMonitor KubeletStart SystemLogMonitor DockerStart SystemLogMonitor ContainerdStart In certain instances, AKS automatically cordons and drains the node to minimize disruption to workloads. For more information about the events and actions, observe Node autodrain. EgressBlocked The list of endpoints checked by the EgressBlocked are listed below Note The actual endpoints will depend on the type of the cluster and the location where it's hosted (Public cloud vs Airgapped clouds). Review the documentation for outbound access here. The documentation is for public clouds Type Example Note MCR https://mcr. com Microsoft Entra ID https://login. com" Resource Manager https://management. com Packages https://packages. com Kube Binary https://acs-mirror. net/acs-mirror/healthz,https://packages. com/acs-mirror/healthz Check the node conditions and events Check the node conditions and events using the kubectl describe node command. kubectl describe node my-aks-node Your output should look similar to the following example condensed output:. Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- VMEventScheduled False Thu, 01 Jun 2023 19:14:25 +0000 Thu, 01 Jun 2023 03:57:41 +0000 NoVMEventScheduled VM has no scheduled event FrequentContainerdRestart False Thu, 01 Jun 2023 19:14:25 +0000 Thu, 01 Jun 2023 03:57:41 +0000 NoFrequentContainerdRestart containerd is functioning properly FrequentDockerRestart False Thu, 01 Jun 2023 19:14:25 +0000 Thu, 01 Jun 2023 03:57:41 +0000 NoFrequentDockerRestart docker is functioning properly FilesystemCorruptionProblem False Thu, 01 Jun 2023 19:14:25 +0000 Thu, 01 Jun 2023 03:57:41 +0000 FilesystemIsOK Filesystem is healthy FrequentUnregisterNetDevice False Thu, 01 Jun 2023 19:14:25 +0000 Thu, 01 Jun 2023 03:57:41 +0000 NoFrequentUnregisterNetDevice node is functioning properly ContainerRuntimeProblem False Thu, 01 Jun 2023 19:14:25 +0000 Thu, 01 Jun 2023 03:57:40 +0000 ContainerRuntimeIsUp container runtime service is up KernelDeadlock False Thu, 01 Jun 2023 19:14:25 +0000 Thu, 01 Jun 2023 03:57:41 +0000 KernelHasNoDeadlock kernel has no deadlock FrequentKubeletRestart False Thu, 01 Jun 2023 19:14:25 +0000 Thu, 01 Jun 2023 03:57:41 +0000 NoFrequentKubeletRestart kubelet is functioning properly KubeletProblem False Thu, 01 Jun 2023 19:14:25 +0000 Thu, 01 Jun 2023 03:57:41 +0000 KubeletIsUp kubelet service is up ReadonlyFilesystem False Thu, 01 Jun 2023 19:14:25 +0000 Thu, 01 Jun 2023 03:57:41 +0000 FilesystemIsNotReadOnly Filesystem is not read-only NetworkUnavailable False Thu, 01 Jun 2023 03:58:39 +0000 Thu, 01 Jun 2023 03:58:39 +0000 RouteCreated RouteController created a route MemoryPressure True Thu, 01 Jun 2023 19:16:50 +0000 Thu, 01 Jun 2023 19:16:50 +0000 KubeletHasInsufficientMemory kubelet has insufficient memory available DiskPressure False Thu, 01 Jun 2023 19:16:50 +0000 Thu, 01 Jun 2023 03:57:22 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Thu, 01 Jun 2023 19:16:50 +0000 Thu, 01 Jun 2023 03:57:22 +0000 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Thu, 01 Jun 2023 19:16:50 +0000 Thu, 01 Jun 2023 03:57:23 +0000 KubeletReady kubelet is posting ready status. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal NodeHasSufficientMemory 94s (x176 over 15h) kubelet Node aks-agentpool-40622340-vmss000009 status is now: NodeHasSufficientMemory These events are also available in Container Insights through KubeEvents. Metrics NPD also exposes Prometheus metrics based on the node problems, which you can utilize for monitoring and alerting. These metrics are exposed on port 20257 of the Node IP and Prometheus can scrape them. The following example YAML shows a scrape config you can utilize with the Azure Managed Prometheus add on as a DaemonSet : kind: ConfigMap apiVersion: v1 metadata: name: ama-metrics-prometheus-config-node namespace: kube-system data: prometheus-config: |- global: scrape_interval: 1m scrape_configs: - job_name: node-problem-detector scrape_interval: 1m scheme: http metrics_path: /metrics relabel_configs: - source_labels: [__metrics_path__] regex: (. *) target_label: metrics_path - source_labels: [__address__] replacement: '$NODE_NAME' target_label: instance static_configs: - targets: ['$NODE_IP:20257'] The following example shows the scraped metrics: problem_gauge{reason="UnregisterNetDevice",type="FrequentUnregisterNetDevice"} 0 problem_gauge{reason="VMEventScheduled",type="VMEventScheduled"} 0 Next steps For more information on NPD, observe kubernetes/node-problem-detector. Feedback Was this page helpful? Yes No No require assist with this topic? desire to attempt using Ask Learn to clarify or guide you through this topic? Ask Learn Ask Learn Suggest a fix? Additional resources Last updated on 2025-09-25


================================================================================
SOURCE 6: https://kubernetes.io/docs/tasks/debug/debug-cluster/monitor-node-health/
================================================================================
Monitor Node Health Node Problem Detector represents daemon for monitoring and reporting about a node's health. You can run Node Problem Detector as a DaemonSet or as a standalone daemon. Node Problem Detector collects information about node problems from various daemons and reports these conditions to the API server as Node Condition s or as Event s. To learn how to install and utilize Node Problem Detector, observe Node Problem Detector project documentation. Before you begin You require to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can utilize one of these Kubernetes playgrounds: iximiuz Labs Killercoda KodeKloud Play with Kubernetes Limitations Node Problem Detector uses the kernel log format for reporting kernel issues. To learn how to extend the kernel log format, observe Add support for another log format. Enabling Node Problem Detector Some cloud providers enable Node Problem Detector as an Addon. You can also enable Node Problem Detector with kubectl or by creating an Addon DaemonSet. Using kubectl to enable Node Problem Detector kubectl provides the most flexible management of Node Problem Detector. You can overwrite the default configuration to fit it into your environment or to detect customized node problems. For example: Create a Node Problem Detector configuration similar to node-problem-detector. yaml : debug/node-problem-detector. yaml apiVersion : apps/v1 kind : DaemonSet metadata : name : node-problem-detector-v0. 1 namespace : kube-system labels : k8s-app : node-problem-detector version : v0. io/cluster-service : "true" spec : selector : matchLabels : k8s-app : node-problem-detector version : v0. io/cluster-service : "true" template : metadata : labels : k8s-app : node-problem-detector version : v0. io/cluster-service : "true" spec : hostNetwork : true containers : - name : node-problem-detector image : registry. io/node-problem-detector:v0. 1 securityContext : privileged : true resources : limits : cpu : "200m" memory : "100Mi" requests : cpu : "20m" memory : "20Mi" volumeMounts : - name : log mountPath : /log readOnly : true volumes : - name : log hostPath : path : /var/log/ Note: You should verify that the system log directory is right for your operating system distribution. begin node problem detector with kubectl : kubectl apply -f https://k8s. io/examples/debug/node-problem-detector. yaml Using an Addon pod to enable Node Problem Detector If you are using a custom cluster bootstrap solution and don't require to overwrite the default configuration, you can leverage the Addon pod to further automate the deployment. Create node-problem-detector. yaml , and save the configuration in the Addon pod's directory /etc/kubernetes/addons/node-problem-detector on a control plane node. Overwrite the configuration The default configuration is embedded when building the Docker image of Node Problem Detector. However, you can utilize a ConfigMap to overwrite the configuration: Change the configuration files in config/ Create the ConfigMap node-problem-detector-config : kubectl create configmap node-problem-detector-config --from-file = config/ Change the node-problem-detector. yaml to utilize the ConfigMap : debug/node-problem-detector-configmap. yaml apiVersion : apps/v1 kind : DaemonSet metadata : name : node-problem-detector-v0. 1 namespace : kube-system labels : k8s-app : node-problem-detector version : v0. io/cluster-service : "true" spec : selector : matchLabels : k8s-app : node-problem-detector version : v0. io/cluster-service : "true" template : metadata : labels : k8s-app : node-problem-detector version : v0. io/cluster-service : "true" spec : hostNetwork : true containers : - name : node-problem-detector image : registry. io/node-problem-detector:v0. 1 securityContext : privileged : true resources : limits : cpu : "200m" memory : "100Mi" requests : cpu : "20m" memory : "20Mi" volumeMounts : - name : log mountPath : /log readOnly : true - name : config # Overwrite the config/ directory with ConfigMap volume mountPath : /config readOnly : true volumes : - name : log hostPath : path : /var/log/ - name : config # Define ConfigMap volume configMap : name : node-problem-detector-config Recreate the Node Problem Detector with the new configuration file: # If you have a node-problem-detector running, delete before recreating kubectl delete -f https://k8s. io/examples/debug/node-problem-detector. yaml kubectl apply -f https://k8s. io/examples/debug/node-problem-detector-configmap. yaml Note: This approach only applies to a Node Problem Detector started with kubectl. Overwriting a configuration is not supported if a Node Problem Detector runs as a cluster Addon. The Addon manager does not support ConfigMap. Problem Daemons A problem daemon represents sub-daemon of the Node Problem Detector. It monitors specific kinds of node problems and reports them to the Node Problem Detector. There are several types of supported problem daemons. A SystemLogMonitor type of daemon monitors the system logs and reports problems and metrics according to predefined rules. You can customize the configurations for different log sources such as filelog , kmsg , kernel , abrt , and systemd. A SystemStatsMonitor type of daemon collects various health-related system stats as metrics. You can customize its behavior by updating its configuration file. A CustomPluginMonitor type of daemon invokes and checks various node problems by running user-defined scripts. You can utilize different custom plugin monitors to monitor different problems and customize the daemon behavior by updating the configuration file. A HealthChecker type of daemon checks the health of the kubelet and container runtime on a node. Adding support for other log format The system log monitor currently supports file-based logs, journald, and kmsg. Additional sources may be added by implementing a new log watcher. Adding custom plugin monitors You can extend the Node Problem Detector to execute any monitor scripts written in any language by developing a custom plugin. The monitor scripts must conform to the plugin protocol in exit code and standard output. For more information, please refer to the plugin interface proposal. Exporter An exporter reports the node problems and/or metrics to certain backends. The following exporters are supported: Kubernetes exporter : this exporter reports node problems to the Kubernetes API server. Temporary problems are reported as Events and permanent problems are reported as Node Conditions. Prometheus exporter : this exporter reports node problems and metrics locally as Prometheus (or OpenMetrics) metrics. You can specify the IP address and port for the exporter using command line arguments. Stackdriver exporter : this exporter reports node problems and metrics to the Stackdriver Monitoring API. The exporting behavior may be customized using a configuration file. Recommendations and restrictions It is recommended to run the Node Problem Detector in your cluster to monitor node health. When running the Node Problem Detector, you can expect extra resource overhead on each node. Usually this is fine, because: The kernel log grows relatively slowly. A resource limit is set for the Node Problem Detector. Even under high load, the resource usage is acceptable. For more information, observe the Node Problem Detector benchmark result. Feedback Was this page helpful? Yes No Thanks for the feedback. If you have a specific, answerable question about how to utilize Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub Repository if you desire to report a problem or suggest an improvement. Last modified August 24, 2023 at 6:38 PM PST: utilize code_sample shortcode instead of code shortcode (e8b136c3b3)


================================================================================
SOURCE 7: https://dev.to/nurudeen_kamilu/understanding-node-problem-detector-in-kubernetes-beyond-default-node-conditions-1d1h
================================================================================
Introducton In Kubernetes, monitoring node health is crucial for maintaining a reliable cluster. While Kubernetes provides built-in node conditions, these basic health checks might not be sufficient for production environments. This is where Node Problem Detector (NPD) comes in, extending the default monitoring capabilities with rich system-level problem detection. This article delves into the features and benefits of NPD, showing how it extends beyond the default Kubernetes node healthy monitoring to proactively detect and address potential node issues. Default Kubernetes Node Conditions By default, Kubernetes nodes come with several built-in conditions that provide basic health information about the nodes in the cluster. These conditions are: Ready : Is the node healthy and able to schedule pods? MemoryPressure : Is the node running low on memory? DiskPressure : Are disk space or I/O operations causing problems? PIDPressure : Is the node overloaded with too many processes? NetworkUnavailable : Are network configurations causing connectivity issues? Each condition is represented by status indicators that describe the current health or operational state of a node. There are three possible statuses: True : The condition is currently happening. For instance, if MemoryPressure is True , it means the node is experiencing memory pressure at the moment. False : The condition is not happening. For example, if DiskPressure is False , the node has sufficient disk space and no I/O issues. Unknown : The system cannot determine the status of the condition, often due to a lack of communication or incomplete data from the node. These conditions may be viewed using the command: kubectl describe node <node-name> Enter fullscreen mode Exit fullscreen mode This command will return each node condition along with its respective status. Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Mon, 13 Jan 2025 21:19:43 +0100 Sun, 01 Dec 2024 01:03:13 +0100 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Mon, 13 Jan 2025 21:19:43 +0100 Sun, 01 Dec 2024 01:03:13 +0100 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Mon, 13 Jan 2025 21:19:43 +0100 Sun, 01 Dec 2024 01:03:13 +0100 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Mon, 13 Jan 2025 21:19:43 +0100 Sun, 01 Dec 2024 01:03:33 +0100 KubeletReady kubelet is posting ready status Enter fullscreen mode Exit fullscreen mode Based on these statuses, Kubernetes adds the necessary taints that match the condition affecting the node. While these default conditions offer a quick glimpse into a node’s health, they may miss deeper, system-level issues. This is where Node Problem Detector steps in to fill the gap. Node Problem Detector: Enhanced Node Monitoring Node Problem Detector extends Kubernetes' native node monitoring capabilities by detecting and reporting various system-level issues. It runs as a daemon on the node, detects node problems, and reports them to the apiserver. How Node Problem Detector Works The problem daemon is the core component that monitors and detects node problems. Its function is to identify and report specific node problems to the node problem detector. NPD supports several types of problem daemons: SystemLogMonitor : Watches system logs (journald, syslog, etc) for predefined patterns and reports problems and metrics accordingly. The types of node conditions reported by this daemon are: KernelDeadlock ReadonlyFilesystem FrequentDockerRestart FrequentKubeletRestart FrequentContainerdRestart CustomPluginMonitor : Executes custom scripts for specific problem detection. HealthChecker : Performs periodic health checks. The types of node conditions reported by this daemon are KubeletUnhealthy and ContainerRuntimeUnhealthy. Upon detection of problems, NPD makes the problem visible to the Kubernetes management stack through the apiserver. Problems are reported as NodeCondition (if it represents permanent problem that will create the node unavailable for pod scheduling) or Event (if it represents temporary problem that has limited impact). Deploying Node Problem Detector Method 1: Using Helm NPD may be deployed using the official Node Problem Detector Helm chart : helm repo add deliveryhero https://charts. io/ helm install node-problem-detector deliveryhero/node-problem-detector \ --namespace kube-system Enter fullscreen mode Exit fullscreen mode Method 2: As a System Service For environments without DaemonSet support, NPD can run as a system service. To achieve this: Download the Node Problem Detector binaries. Create a systemd service file. Enable the service using systemd commands. begin the Node Problem Detector service. Customizing Node Problem Detector One of NPD’s standout features is its ability to adapt to your specific needs. By leveraging the CustomPluginMonitor problem daemon , you can define custom node conditions and rules to monitor exactly what matters most to your workloads. Adding Custom Conditions and Detection Rules This example demonstrates a custom-plugin JSON file. This file defines custom condition and rules that enable NPD to identify problems based on specific patterns. { "plugin" : "custom" , "pluginConfig" : { "invoke_interval" : "30s" , "timeout" : "5s" , "max_output_length" : 80 , "concurrency" : 3 , "enable_message_change_based_condition_update" : false }, "source" : "ntp-custom-plugin-monitor" , "metricsReporting" : true , "conditions" : [ { "type" : "NTPProblem" , "reason" : "NTPIsUp" , "message" : "ntp service is up" } ], "rules" : [ { "type" : "temporary" , "reason" : "NTPIsDown" , "path" : ". /config/plugin/check_ntp. sh" , "timeout" : "3s" }, { "type" : "permanent" , "condition" : "NTPProblem" , "reason" : "NTPIsDown" , "path" : ". /config/plugin/check_ntp. sh" , "timeout" : "3s" } ] } Enter fullscreen mode Exit fullscreen mode 2. Writing Custom Plugin Script The custom plugin script is the executable that performs the actual health checks. The output of the script must align with the patterns defined in the JSON file to trigger corresponding node conditions. #!/bin/bash readonly OK = 0 readonly NONOK = 1 readonly UNKNOWN = 2 readonly SERVICE = 'ntp. service' # Check systemd cmd present if ! command -v systemctl > /dev/null ; then echo "Could not discover 'systemctl' - require systemd" exit $UNKNOWN fi # Return success if service active (i. running) if systemctl -q is-active " $SERVICE " ; then echo " $SERVICE is running" exit $OK else # Does not differentiate stopped/failed service from non-existent echo " $SERVICE is not running" exit $NONOK fi Enter fullscreen mode Exit fullscreen mode Conclusion Node Problem Detector is more than just a monitoring tool — it’s a safety net for your Kubernetes clusters. By expanding beyond default node conditions and offering unparalleled customization, NPD equips you to tackle challenges head-on, ensuring high availability and smooth operations. Embrace NPD and take a proactive approach to node monitoring in your Kubernetes journey! References and Further Reading Node Problem Detector GitHub Repository Helm Chart for NPD Kubernetes Documentation: Nodes Kubernetes Documentation: Taints and Tolerations Create template Templates let you quickly answer FAQs or store snippets for re-utilize. Submit Preview Dismiss Collapse Expand Idowu Abdulazeez Idowu Abdulazeez Idowu Abdulazeez Follow Email idowuabdulazeez1@gmail. com Joined Jan 14, 2025 • Jan 14 Dropdown menu Copy link Hide Fantastic Collapse Expand Shittu Sulaimon (Barry) Shittu Sulaimon (Barry) Shittu Sulaimon (Barry) Follow Joined Mar 8, 2024 • Jan 27 Dropdown menu Copy link Hide Thanks for sharing. Are you sure you desire to hide this comment? It will become hidden in your post, but will still be visible via the comment's permalink. Hide child comments as well Confirm For further actions, you may consider blocking this person and/or reporting abuse


================================================================================
SOURCE 8: https://deepwiki.com/kubernetes/node-problem-detector
================================================================================
Index your code with Devin DeepWiki DeepWiki kubernetes/node-problem-detector Index your code with Devin Edit Wiki Share Loading. Last indexed: 23 April 2025 ( 308b7c ) Overview Architecture utilize Cases Installation and Configuration Deployment Options Configuration Options RBAC and Security Problem Detection System Log Monitor Custom Plugin Monitor System Stats Monitor Health Checker Problem Reporting Kubernetes Exporter Prometheus Exporter Stackdriver Exporter Build and Development Building From Source Testing CI/CD Pipeline Release Process Advanced Topics Custom Plugins Development Integration with Kubernetes Performance Considerations Menu Overview Relevant source files README. md cmd/nodeproblemdetector/node_problem_detector. go pkg/exporters/prometheusexporter/prometheus_exporter. go pkg/problemdaemon/problem_daemon. go pkg/problemdaemon/problem_daemon_test. go pkg/problemdetector/problem_detector. go pkg/version/version. go Node Problem Detector (NPD) represents daemon that runs on each node in a Kubernetes cluster to detect and report node problems. It serves as a critical monitoring component that makes various node-level issues visible to the upstream layers in the cluster management stack. This visibility enables appropriate remediation actions to assist maintain cluster health. Purpose and Scope NPD monitors node health by detecting various problems that could affect pods running on the node, such as: Infrastructure daemon issues (e. , NTP service failures) Hardware problems (e. , bad CPU, memory, or disk) Kernel issues (e. , kernel deadlocks, corrupted file systems) Container runtime issues (e. , unresponsive runtime daemon) Without NPD, these problems would remain invisible to Kubernetes, resulting in the scheduler continuing to place pods on problematic nodes. md 5-31 Core Architecture NPD follows a modular architecture with clear separation between problem detection and problem reporting: The NPD architecture consists of three main components: Problem Detector : The core component that coordinates problem daemons and exporters Problem Daemons (Monitors) : Specialized components that detect specific types of problems Exporters : Components that report detected problems to various backends Sources: README. md 42-53 pkg/problemdetector/problem_detector. go 28-45 pkg/types/types. go 104-118 Problem Daemons Problem daemons are specialized monitors that detect specific categories of problems: Problem Daemon Type NodeConditions Reported Description SystemLogMonitor KernelDeadlock, ReadonlyFilesystem, FrequentKubeletRestart, FrequentDockerRestart, FrequentContainerdRestart Monitors system logs and reports problems based on predefined rules SystemStatsMonitor None (metrics only) Collects various health-related system stats CustomPluginMonitor User-defined (e. , NTPProblem) Invokes user-defined check scripts to detect custom problems HealthChecker KubeletUnhealthy, ContainerRuntimeUnhealthy Checks the health of kubelet and container runtime Sources: README. md 60-67 pkg/problemdaemon/problem_daemon. go 16-73 Problem Detection Flow The core data structures that facilitate this flow: Sources: pkg/types/types. go 32-118 pkg/problemdetector/problem_detector. go 46-101 Problem Reporting NPD uses two mechanisms to report problems: NodeConditions : For permanent problems that create the node unavailable for pods Events : For temporary problems with limited impact but provide useful information These problems may be reported through various exporters: Exporter Description Kubernetes Exporter Reports problems to the Kubernetes API server Prometheus Exporter Exposes metrics for Prometheus scraping Stackdriver Exporter Reports problems and metrics to Stackdriver Monitoring API Sources: README. md 69-78 pkg/exporters/prometheusexporter/prometheus_exporter. go 34-60 Initialization and Runtime Process The following diagram illustrates the initialization and runtime process of NPD: At runtime, NPD: Initializes all configured problem daemons based on configuration files Initializes exporters based on command-line options Starts a monitoring loop that receives status updates from problem daemons Forwards received status updates to all configured exporters Sources: cmd/nodeproblemdetector/node_problem_detector. go 36-76 pkg/problemdetector/problem_detector. go 46-89 Deployment Options NPD may be deployed in two primary ways: As a DaemonSet : This is the most common deployment method, where NPD runs as a pod on every node in the cluster. Standalone : NPD can also run directly on the node, outside of Kubernetes control. For detailed installation instructions, observe Installation and Configuration. md 173-206 Compatibility and Ecosystem Integration NPD is designed to be compatible with all supported Kubernetes versions (v0. It integrates with various remediation systems: Descheduler : Can evict pods from nodes with reported issues mediK8S : An umbrella project for automatic remediation Cluster API's MachineHealthCheck : Remediates unhealthy machines based on NPD-reported conditions For more information about integration with Kubernetes and other systems, observe Integration with Kubernetes. md 314-316 Next Steps For more detailed information about specific components and functionality: To comprehend NPD's internal architecture in detail, observe Architecture To learn about real-world utilize cases for NPD, observe utilize Cases For installation and configuration details, observe Installation and Configuration For detailed explanation of problem detection mechanisms, observe Problem Detection For information about how problems are reported, observe Problem Reporting Dismiss Refresh this wiki Enter email to refresh On this page Overview Purpose and Scope Core Architecture Problem Daemons Problem Detection Flow Problem Reporting Initialization and Runtime Process Deployment Options Compatibility and Ecosystem Integration Next Steps


================================================================================
SOURCE 9: https://github.com/kubernetes/node-problem-detector/issues/698
================================================================================
kubernetes / node-problem-detector Public Notifications You must be signed in to change notification settings Fork 683 Star 3. 3k node-problem-detector cannot run in non-privileged mode #698 New issue Copy link New issue Copy link Closed as not planned Closed as not planned node-problem-detector cannot run in non-privileged mode #698 Copy link Labels lifecycle/rotten Denotes an issue or PR that has aged beyond stale and shall be auto-closed. Denotes an issue or PR that has aged beyond stale and shall be auto-closed. needs-kind Indicates a PR lacks a `kind/foo` label and requires one. Indicates a PR lacks a `kind/foo` label and requires one. Description ialidzhikov opened on Sep 1, 2022 Issue body actions /kind bug What happened? Running containers in privileged mode is not recommended as privileged containers run with all linux capabilities enabled and can access the host's resources. Running containers in privileged mode opens number of security threads such as breakout to underlying host OS. Currently the node-problem-detector DaemonSet runs in privileged mode. node-problem-detector/deployment/node-problem-detector. yaml Lines 41 to 42 in d8b2940 securityContext : privileged : true Trying to run node-problem-detector in non-privileged mode (even with all capabilities added) one of its monitors fails with: E0808 06:25:33. 740326 1 problem_detector. go:55] Failed to begin problem daemon &{/config/kernel-monitor. json 0xc00035b7a0 0xc000443100 {{kmsg map[] /dev/kmsg 5m } 10 kernel-monitor [{KernelDeadlock {0 0 <nil>} KernelHasNoDeadlock kernel has no deadlock} {ReadonlyFilesystem {0 0 <nil>} FilesystemIsNotReadOnly Filesystem is not read-only}] [{temporary OOMKilling Killed process \d+ (. +) total-vm:\d+kB, anon-rss:\d+kB, file-rss:\d+kB. *} {temporary TaskHung task [\S ]+:\w+ blocked for more than \w+ seconds\. } {temporary UnregisterNetDevice unregister_netdevice: waiting for \w+ to become free. Usage count = \d+} {temporary KernelOops BUG: unable to handle kernel NULL pointer dereference at. *} {temporary KernelOops divide error: 0000 \[#\d+\] SMP} {temporary Ext4Error EXT4-fs error. *} {temporary Ext4Warning EXT4-fs warning. *} {temporary IOError Buffer I/O error. *} {temporary MemoryReadError CE memory read error. *} {permanent KernelDeadlock DockerHung task docker:\w+ blocked for more than \w+ seconds\. } {permanent ReadonlyFilesystem FilesystemIsReadOnly Remounting filesystem read-only}] 0xc00043d21e} [] <nil> 0xc00045aea0 0xc00044bb80}: failed to create kmsg parser: open /dev/kmsg: operation not permitted I don't fully comprehend what it requires to read kernel logs from /dev/kmsg. What did you expect to happen? I would expect to be able to run node-problem-detector in non-privileged mode. 👍 React with 👍 7 AndrzejWisniewski, btiernay, alazyer, AlexzSouz, agravgaard and 2 more Metadata Metadata Assignees No one assigned Labels lifecycle/rotten Denotes an issue or PR that has aged beyond stale and shall be auto-closed. Denotes an issue or PR that has aged beyond stale and shall be auto-closed. needs-kind Indicates a PR lacks a `kind/foo` label and requires one. Indicates a PR lacks a `kind/foo` label and requires one. Type No type Projects No projects Milestone No milestone Relationships None yet Development No branches or pull requests Issue actions


================================================================================
SOURCE 10: https://superorbital.io/blog/node-problem-detector-custom-plugins-primer/
================================================================================
Node Problem Detector Custom Plugins A primer on configuring custom plugins for Node Problem Detector David Sharp Engineer Recovering kernel hacker. Containers and controllers are my jam. Published on January 24, 2025 I was recently asked to prototype a custom plugin for node-problem-detector. I found that the documentation for the plugin interface is pretty inscrutable. The official documentation links to the plugin interface proposal document on Google Docs which doesn’t create a good guide to writing your own plugin. So here’s a primer I developed based on my brief experience. Quick Introduction to Node Problem Detector Node Problem Detector (NPD) runs as a DaemonSet, and sets conditions in the status field of the node that it is running on. It can also output an Event when a problem occurs. The condition and events would be visible when running kubectl describe node <node-name>. NPD has several built-in problems it will detect. But it also has a way to take advantage of its infrastructure to add your own conditions and events. Writing Custom Plugins The “script interface” is based on exit status and a message on stdout. Exit status of 0 means no problem detected; 1 means the problem was detected; any other non-zero status means the status could not be determined. The path to this script is added to a JSON configuration file, and the path to that configuration file is passed to the node-problem-detector binary via a command-line argument. The helm chart abstracts some of this plumbing away so that you only require to author the script, the configuration, and probably modify the node-problem-detector image so that it contains the tools necessary for your script. You might desire to begin by adding a configuration like this to your helm values: { "plugin": "custom", "pluginConfig": { "invoke_interval": "30s", "timeout": "5s", "max_output_length": 80, "concurrency": 3 }, "source": "my-custom-plugin-monitor", "metricsReporting": true, "conditions": [ { "type": "MyProblemCondition", "reason": "NoProblem", "message": "Everything is normal" } ], "rules": [ { "type": "permanent", "condition": "MyProblemCondition", "reason": "ProblemCause", "path": ". /custom-config/plugin-my_problem. sh" } ] } But what do these configuration keys even mean? What are the “conditions” and “rules”? The best explanation I’ve seen for custom plugin configuration is in the source for node-problem-detector at docs/custom_plugin_monitor. The struct definitions for Condition and CustomRule serve as additional references. It’s not the most obvious configuration surface. Instead of starting from configuration, it’s simpler to think about this from the bottom-up, starting from the script, how it gets executed, and how the result is turned into a status condition or an event. How a plugin script invocation is turned into a Condition or Event I write a script that can return an exit status of 0 or 1. For example, below is an abridged version of a sample script from the NPD repository that checks if a systemd service (NTP) is running. # Return success if service active (i. running) if systemctl -q is-active ntp. service; then echo "NTP is running" exit 0 else echo "NTP is not running" exit 1 fi I put this script as the "path" of a "rules" entry with "type": "temporary". Since this represents "temporary" rule, NPD may output an Event. If the script returns 0 , then do nothing. If the script returns 1 , then output an Event with "reason" from this "rule" , and "message" from stdout. Alternatively, I could add this script to the configuration as the "path" of a "rules" entry with "type": "permanent". Since this represents "permanent" rule, NPD will update an entry in status. conditions of the node. Which Condition entry? The one with "type" equal to this rule’s "condition" field. (Or if none exists, then it will create it, of course. ) If the script returns 0 , then NPD will set the Condition to its default state. The default state comes from the entry in the "conditions" section whose "type" matches this rule’s "condition". NPD will update the Condition using both the "reason" and "message" from the "conditions" entry. If it returns 1 , then NPD will set the Condition to the "reason" from this rule and the "message" to the stdout produced by the script. Recipes We can distill this further into three recipes. I desire an Event to be emitted when my script returns non-zero status. Don’t add anything to "conditions". Add an entry in "rules" with "type": "temporary" , and "path" with the path to your script. Set the rule’s "reason" to what you desire emitted in the event. I desire a Condition to be set according to how my script returns Add an entry in "conditions" with "type" , and an entry in "rules" with "condition" , set to the same value: The name of the Condition you desire to output. The rule should have "type": "permanent" , and "path" with the path to your script. The condition should have a "reason" and "message" for the passing case. The rule should have a “reason” for the failing case. The detailed “message” will come from the script’s stdout. I desire a Condition and an Event when my script returns non-zero. Combine the above… A "conditions" entry for the default state of the Condition. A "permanent" rule for the erroring state of the Condition. A "temporary" rule to emit an event. That is, one condition and two rules with the same "path". Deployment You now have a script and custom plugin configuration to tell node-problem-detector to run the script and update a Condition or output an Event. A little more work is needed to bundle it all together to deploy. Using the helm chart , the following values should be set to enable the custom plugin: image : If a custom image was needed to extend the node-problem-detector image with additional binaries, then specify it here. custom_plugin_monitors : This represents list of file paths within the container to the JSON configuration file for custom plugins. If using the chart’s custom_monitor_definitions to populate a ConfigMap, then these paths should begin with /custom-config/. custom_monitor_definitions define the contents of a ConfigMap mounted at /custom-config/. Add a key under here with a filename like "my-custom-monitor. Your script can also go in a key here. That’s it! Deploy the helm chart with these values, and your custom conditions and events should begin appearing on the node objects when you run kubectl obtain nodes -o yaml or kubectl describe a node. Demo As part of my exploration, I made a demo repository. The exploration was to observe if NPD could be used to detect that the node had a network connection issue. The conclusion of that exploration was that NPD was not a good candidate for detecting network connectivity problems, since it would be unable to write the status back to the api-server over the highly network it was detecting a problem within. Nevertheless, the repository shows a complete custom plugin deployment that may be deployed in a kind cluster. David Sharp Engineer Recovering kernel hacker. Containers and controllers are my jam. Like what you observe? Reach out to talk about our stellar Kubernetes engineering and training Engineering Training Case Studies Partners Blog Contact hello@superorbital. io Github © 2018-2024 SuperOrbital, LLC All rights reserved | Privacy Policy

This information is tailored for the alertmend.io platform, providing comprehensive insights and solutions.
