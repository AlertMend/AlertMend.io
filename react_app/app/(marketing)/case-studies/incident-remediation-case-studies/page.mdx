import { CaseStudyLayout } from "@/components/case-study-layout";

export const caseStudy = {
  author: { name: "Yasser Ansari", src: "/avatar.jpeg", designation: "CEO" },
  date: "2024-07-14",
  title: "AlertMend in Action: Real Results.\nReal Fast.",
  description: "AlertMend streamlined our incident management. Faster resolutions, less manual work, and a more focused team. Peace of mind is invaluable",
  image:
    "/img/case-study1.webp",
  links: [
  { title: "Use Cases", href: "#use-cases" },
  { title: "Results from 1st Month", href: "#results-from-1st-month" },
  { title: "Customer Overview", href: "#customer-overview" },
  { title: "Challenge", href: "#challenge" },
  { title: "Solution with AlertMend", href: "#solution-with-alertmend" },
  { title: "Key Features Used", href: "#key-features-used" },
  { title: "Results", href: "#results" },
  { title: "Customer Quote (Optional)", href: "#customer-quote-optional" },
  { title: "Summary", href: "#summary" }
]
};
export const metadata = {
  title: caseStudy.title,
  description: caseStudy.description,
  openGraph: {
    images: [caseStudy.image],
  },
};

export default (props) => <CaseStudyLayout caseStudy={caseStudy} {...props} />;

# AlertMend in Action: Real Results.
# Real Fast.

> "AlertMend streamlined our incident management. Faster resolutions, less manual work, and a more focused team. Peace of mind is invaluable"

**Yasser Ansari**
CEO

![Polymer Logo](/img/polymer-logo.png)

---

### <span id="use-cases" />Use Cases:

* Automatically detect when a service is stuck on ECS.
* Proactively using SQS metrics and Datadog logs to confirm the issue.
* Trigger an automated restart to restore service without manual intervention.
* Optimize MongoDB performance and reduce infrastructure costs.
* Monitor self-hosted Redis and RabbitMQ clusters to ensure 100% uptime.

---

[See complete case study](https://docs.google.com/document/d/192rfaffZgEg2XNSAyL66wuWJ7_0NyXvfbpQKM-0a3tU/edit?usp=sharing)

---

## Results from 1st Month

### 21x
Downtime avoided

### 25h
Engineering hours saved

### USD 2000
Engineer Cost Saving

### USD 3000
Revenue Saved

### 99%
Application Uptime

---

## <span id="customer-overview" /> Customer Overview

**Company:** Polymer Search  
**Company Size:** 11–50  
**Industry:** AI / SaaS / Data Analytics  
**Region:** United States

Polymer Search is an AI-powered platform that enables users to explore and analyze data without requiring SQL or dashboards. Their product is built on a microservice-based architecture deployed via Kubernetes (GKE), supporting real-time data ingestion, transformation, and visualization.

---

## <span id="challenge" /> Challenge

Polymer Search operates a real-time AI-powered data analytics platform on a modern, serverless, container-based stack, including ECS, SQS, Lambda, MongoDB, Redis, EC2, and Datadog for monitoring.

Their most pressing reliability challenge stemmed from ECS-based AI worker tasks that:
- Silently stopped processing SQS messages due to internal errors or deadlocks
- Remained "healthy" per ECS health checks, as containers didn’t crash
- Went undetected for hours during nights/weekends due to lack of 24/7 SRE coverage

**Compounding issues:**
- Datadog alerts fired but lacked actionable guidance
- SQS queues backed up silently, delaying downstream workflows
- Manual restarts became routine and burdensome

This disconnect between "infrastructure looks fine" and "app is broken" consumed engineering time and undermined confidence in automation.

---

## <span id="solution-with-alertmend" /> Solution with AlertMend

Polymer integrated **AlertMend** into their observability stack, leveraging Datadog metrics/logs to implement intelligent auto-remediation.

### <span id="alert-trigger" /> Alert Trigger
- Alert fired when:
  - SQS message backlog exceeded threshold
  - Message age remained high (processing stalled)

### <span id="validation-via-logs" /> Validation via Logs
- AlertMend verified alerts before acting by:
  - Querying Datadog logs for the ECS task
  - Checking for missing activity logs ("processing started", "job completed") in a configurable window

This filtered out false positives during high-traffic events.

### <span id="automated-remediation" /> Automated Remediation
- If inactivity confirmed via logs:
  - AlertMend automatically restarted the ECS task
  - Action was **auto-approved** in production for speed
  - A detailed Slack summary was sent for review

### <span id="post-restart-health-check" /> Post-Restart Health Check
- AlertMend:
  - Re-monitored SQS/logs
  - Verified message count was decreasing
  - If still broken, escalated to `#critical` Slack channel

---

## <span id="key-features-used" /> Key Features Used

| Feature                     | Purpose / Impact                                                                 |
|----------------------------|----------------------------------------------------------------------------------|
| Custom Workflow Builder    | Combined SQS checks, log scans, ECS restarts, Slack reports                      |
| Auto Remediation           | Auto-restarted stuck ECS tasks with built-in validation                          |
| Slack Notifications        | Shared full incident summaries in Slack                                          |
| Critical Escalation Channel| Triggered manual alerts when automation failed                                   |
| Datadog Integration        | Seamless log + alert ingestion, no config changes needed                         |
| Production-Ready Workflows | Enabled auto-approved flows with audit trails and rollback visibility            |

---

## <span id="results" /> Results

- **90% reduction in MTTR** (from 45 minutes to under 5 minutes)
- **100% of known SQS stalls auto-resolved** without human touch
- **Alert fatigue cut drastically** — no more noise during nights
- **Zero overnight escalations** for repeat ECS issues post-deployment
- **Slack summaries ensured transparency and post-mortems**
- **Critical issues surfaced more reliably**, thanks to escalation fallback

By merging observability, logic, and automation, AlertMend empowered Polymer Search to achieve reliable, self-healing infrastructure — fixing problems before customers even noticed.

![Slack alert with auto-restart confirmation](/img/slack-auto-restart-alert.png)

![AlertMend remediation flow visualization](/img/alertmend-flow-ui.png)

---

## <span id="customer-quote-optional" /> Customer Quote (Optional)

> "We were drowning in alerts and incident noise. AlertMend helped us go from reactive firefighting to proactive resolution — it’s been a game-changer for our ops."  
> — [Name], [Role, Polymer Search]

---

## <span id="summary" /> Summary

By integrating AlertMend into their ECS infrastructure, **Polymer Search**:
- Eliminated silent SQS processing failures
- Reduced engineer toil
- Enabled instant, log-aware remediation of recurring issues

With ECS task restarts, smart validation, and Slack-based visibility, the team turned noisy problems into automated workflows. Critical issues are escalated only if automation fails, ensuring fast recovery **without human interruption**.
