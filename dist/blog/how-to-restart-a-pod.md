---
title: "How to Restart a Pod"
excerpt: "REFERENCE CONTENT FROM TOP 8 GOOGLE SEARCH RESULTS This content is gathered from the top-ranking pages for comprehensive reference"
date: "2025-12-18"
category: "Kubernetes"
author: "AlertMend Team"
keywords: "restart, Kubernetes, AlertMend AI, AIOps, container orchestration, DevOps"
---

# how to restart a pod

REFERENCE CONTENT FROM TOP 8 GOOGLE SEARCH RESULTS

This content is gathered from the top-ranking pages for comprehensive reference.
Sources:
  1. https://spacelift.io/blog/restart-kubernetes-pods-with-kubectl
  2. https://kodekloud.com/blog/kubernetes-pod-restart/
  3. https://www.vcluster.com/blog/how-to-restart-pods-in-kubectl-a-tutorial-with-examples
  4. https://www.cherryservers.com/blog/kubernetes-restart-pod
  5. https://www.suse.com/c/observability-restarting-kubernetes-pods-a-detailed-guide/
  6. https://www.strongdm.com/blog/restart-kubernetes-pods-with-kubectl
  7. https://middleware.io/blog/kubectl-restart-pod/
  8. https://www.warp.dev/terminus/kubectl-restart-pod

The following sections contain content from each source, organized for reference.
utilize this information to comprehend the topic comprehensively, identify key points,
related keywords, and best practices. Then create original, SEO-optimized content
that synthesizes insights from all sources while using completely original wording.






Kubernetes How to Restart Kubernetes Pods With Kubectl Jack Roper Updated 05 Dec 2025 ¬∑ 12 min read A pod is the smallest unit in Kubernetes (K8S). It should run until it is replaced by a new deployment. This means there is no way to restart a pod; it should be replaced instead. There is no kubectl restart [podname] command for utilize with K8S (with Docker you can utilize docker restart [container_id] ), but there are several ways to achieve a pod ‚Äòrestart‚Äô with kubectl. What we will cover in this article: Why might you desire to restart a Kubernetes pod? Available pod statuses How to restart a Kubernetes pod Why might you desire to restart a Kubernetes pod? Here are some situations in which you may require to restart a pod: Applying configuration changes ‚Üí Updates to the pod‚Äôs configuration (configmaps, secrets, environment variables) may require the pod to be restarted for the changes to take effect manually. Debugging applications ‚Üí Sometimes, if your application is not running correctly or you are experiencing issues with it, restarting the underlying pods to reset their state and create troubleshooting easier represents good practice. Pod stuck in a terminating state ‚Üí In this case, a delete and a recreation would usually do the trick. However, there are some cases where a node is taken out of service, and the pods cannot be evicted from it, so a restart will assist address the issue. Addressing Out Of Memory (OOM) errors ‚Üí If a pod is terminated with an Out Of Memory Error (OOM), you will require to restart the pod after making changes to the resource specifications. This may be solved automatically if the pod‚Äôs restart policy allows it. Forcing a new image pull ‚Üí If you are using the latest tag (which is not a best practice), you require to manually restart the pod to force a new image pull to ensure a pod is using the latest version of an image. Of course, if you are making changes to the image parameter in the configuration because you‚Äôve released a new image and desire to take advantage of that, a restart will still be required. Resource contention ‚Üí If a pod is consuming excessive resources, causing performance issues, or affecting other workflows, restarting the pod may release those resources and mitigate the problem. This usually occurs when you are not using memory and CPU restrictions. Note: Manually deleting and restarting pods in Kubernetes can introduce risks. If not managed carefully, this action may disrupt running applications, especially if the pod is handling live traffic or has not been configured with appropriate replication or readiness probes. Without proper checks, manual restarts can lead to temporary downtime, data loss, or state inconsistencies. It may also bypass automated orchestration logic, preventing Kubernetes from managing the pod lifecycle as intended. For safer operations, it‚Äôs recommended to utilize rolling updates or Kubernetes-native tools that respect deployment strategies and maintain service availability. Available pod statuses A Kubernetes pod has five possible statuses: pending, running, succeeded, failed, and unknown. Pending : This state shows that at least one container within the pod has not yet been created. Running : All containers were created, and the pod was bound to a Node. At this point, the containers are running or are being started or restarted. Succeeded : All containers in the pod were successfully terminated and will not be restarted. Failed: All containers were terminated, and at least one container has failed. The failed container exists in a non-zero state. Unknown: The status of the pod cannot be obtained. If you notice a pod in an undesirable state, with the status showing error , you might attempt a ‚Äòrestart‚Äô as part of your troubleshooting to obtain things back to normal operations. You may also observe the status CrashLoopBackOff , which is the default when an error is encountered, and K8S tries to restart the pod automatically. How to restart a Kubernetes pod Kubernetes does not provide a direct kubectl restart pod command. However, you can achieve similar functionality using several methods with kubectl. Below are some common ways to restart a Kubernetes pod: Rolling restart the deployment ( kubectl rollout restart ) Scale deployment replicas ( kubectl scale ) Delete an individual pod ( kubectl delete ) Force replace a pod ( kubectl replace ) Update environment variables ( kubectl set env ) Once new pods are re-created, they will have a different name from the old ones. utilize the kubectl obtain pods command to obtain a list of all your pods. Method 1: Rolling restart the deployment Quick command reference: kubectl rollout restart This method is recommended for triggering a restart when you haven‚Äôt made changes to the deployment manifest but desire the pods to refresh (e. , to pick up new secrets, reinitialize a process, etc. A rollout restart will kill one pod at a time, and then new pods shall be scaled up. This method works on Kubernetes and kubectl 1. 15 and newer , which includes all currently supported Kubernetes releases. kubectl rollout restart deployment < deployment_name > -n < namespace > This command tells Kubernetes to restart the Deployment, which causes all the associated pods to be replaced one by one. Troubleshooting: error: unknown command ‚Äúrestart‚Äù If you obtain this error when running kubectl rollout restart , your kubectl binary is too old ‚Äì restart was added in 1. Upgrade kubectl (and ideally your cluster) to a supported version, then attempt again. To check versions: kubectl version # or: kubectl version --output=yaml What represents rolling restart in Kubernetes? A rolling restart in Kubernetes represents process where pods in a deployment are gradually terminated and replaced with new ones, ensuring that the application remains available throughout the update. This is done incrementally, typically one pod at a time, so that there is no downtime during the rollout. Rolling restarts are commonly used for applying configuration changes or updating container images without disrupting end users. Method 2: Scale deployment replicas Quick command reference: kubectl scale Scaling a Deployment down to 0 replicas and then back up forces Kubernetes to terminate all existing pods and create fresh ones. This is essentially a ‚Äúrestart‚Äù because the new pods are instantiated from the Deployment‚Äôs pod template. However, this method will introduce an outage and is not recommended. If downtime is not an issue, it may be used as a quicker alternative to the kubectl rollout restart method (your pod may have to run through a lengthy continuous integration/deployment process before it is redeployed). If there is no YAML file associated with the deployment, you can set the number of replicas to 0. kubectl scale deployment < deployment name > -n < namespace > --replicas = 0 This terminates the pods. Once scaling is complete, the replicas may be scaled back up as needed (to at least 1): kubectl scale deployment < deployment name > -n < namespace > --replicas = 3 Pod status may be checked during the scaling using: kubectl obtain pods -n < namespace > üí° You might also like: How to Maintain Operations Around Kubernetes Cluster 15 Kubernetes Best Practices to Follow The Role of Kubernetes in DevOps ‚Äì utilize Cases & Other Tools Method 3: Delete an individual pod Quick command reference: kubectl delete pod and kubectl delete replicaset If the pod is managed by a Deployment, ReplicaSet, or StatefulSet, you can safely delete the pod with kubectl delete pod since Kubernetes will automatically recreate it. Each pod may be deleted individually if required: kubectl delete pod < pod_name > -n < namespace > Doing this will cause the pod to be recreated because K8S is declarative; it will create a new pod based on the specified configuration. However, when many pods are running, this is not truly a practical approach. Where many pods have the same label, you could utilize that label to select multiple pods at once: kubectl delete pod -l ‚Äúapp:myapp‚Äù -n < namespace > ReplicaSet may be deleted instead if there are many pods: kubectl delete replicaset < name > -n < namespace > Method 4: Force replace a Pod Quick command reference: kubectl obtain pod | kubectl replace The pod you desire to replace may be retrieved using the kubectl obtain pod to obtain the YAML statement of the currently running pod and passed it to the kubectl replace command with the --force flag specified in order to achieve a restart. This is useful if no YAML file is available and the pod was started. kubectl obtain pod < pod_name > -n < namespace > -o yaml | kubectl replace --force -f - This method only works for manually created pods or those not controlled by higher-level objects like Deployments, StatefulSets, etc. If you attempt this on a pod that‚Äôs part of a Deployment, Kubernetes might immediately recreate a second pod (since the Deployment notices one went missing), leading to duplicates or conflicts. Method 5: Update environment variables Quick command reference: kubectl set env A simple and effective way to restart pods in Kubernetes is by using the kubectl set env command to update an environment variable in a Deployment. Kubernetes triggers a rolling restart whenever the pod template changes, and changing or adding an environment variable is enough to create that happen. The example below sets the environment variable DEPLOY_DATE to the date specified, causing the pod to restart. kubectl set env deployment < deployment name > -n < namespace > DEPLOY_DATE = " $( date ) " This method is safe, causes no downtime (thanks to the rolling update), and is perfect for triggering restarts after updating ConfigMaps and Secrets or refreshing the application state without changing the app code or deployment image. It‚Äôs also a popular approach in automation scripts and CI/CD pipelines. Best practices for restarting pods in Kubernetes Best practices for restarting pods in Kubernetes involve using declarative and automated approaches to ensure reliability and minimal disruption: utilize readiness and liveness probes : These health checks assist Kubernetes detect when a pod is unhealthy and should be restarted, ensuring that only healthy containers receive traffic. Avoid manual deletion : Instead of manually deleting pods, update deployments or utilize rolling restarts ( kubectl rollout restart deployment/<name> ) to let Kubernetes handle restarts gracefully. Implement rolling updates : utilize rolling updates for deployments to ensure zero downtime by gradually replacing old pods with new ones. Configure resource requests and limits : Proper resource allocation helps prevent pods from being killed unexpectedly by the scheduler or node-level OOM (Out Of Memory) conditions. utilize CrashLoopBackOff as a signal : If a pod enters this state, investigate logs and errors before forcing restarts. Configuration or code may be causing the issue. Avoid frequent or unnecessary restarts: Unplanned or frequent restarts can lead to performance issues, cascading failures, or configuration drift. Always aim to identify and fix root causes rather than relying on restarts as a workaround. Log and monitor restarts: Track pod restarts through logs and metrics to comprehend restart reasons (e. , crashes, probes failing, OOM errors). Restarting shouldn‚Äôt be the first solution to runtime problems; observability helps prevent recurring issues. Tune your rolling-update strategy: For Deployments, fields like maxUnavailable and maxSurge control how many pods may be down and how aggressively you roll out changes. utilize them to strike the right balance between safety and speed. Managing Kubernetes with Spacelift If you require assistance managing your Kubernetes projects, look at Spacelift. It brings a GitOps workflow, so your Kubernetes deployments stay in sync with your Kubernetes stacks, and pull requests demonstrate you a preview of the changes they‚Äôre going to apply. Spacelift has a native Kubernetes integration that applies changes via kubectl. For each Kubernetes stack, you can choose which kubectl version to utilize, so you can align with your cluster version and utilize features such as kubectl rollout restart , provided your cluster supports them. You can also utilize Spacelift to mix and match Terraform, OpenTofu, Pulumi, AWS CloudFormation, and Kubernetes stacks and have them talk to one another. To take this one step further, you could add custom policies to reinforce the security and reliability of your configurations and deployments. Spacelift provides different types of policies and workflows that are easily customizable to fit every utilize case. For instance, you could add plan policies to restrict or warn about security or compliance violations or approval policies to add an approval step during deployments. You can attempt Spacelift for free by creating a trial account or booking a demo with one of our engineers. Supply chain management platform Logixboard has found Spacelift easy to install, configure, and maintain. Some of Logixboard‚Äôs stacks manage Kubernetes resources, such as CRDs, deployments, and ConfigMaps, via some Terraform providers. The company shall be creating a lot more stacks as they increasingly utilize Kubernetes, and Spacelift will assist them do that with confidence and ease. Spacelift customer case study Read the full story Key points Having a range of commands to utilize when you encounter issues with pods in K8S will enable you to restart them appropriately, depending on how you have deployed the pods, the necessity for application uptime, and the urgency of the restart. In general, the best approach is to utilize the kubectl rollout restart method described above, as it will avoid application downtime. A restart will not resolve the problem that caused the pods to have issues in the first place, so further investigation into the root cause shall be required. Manage Kubernetes easier and faster Spacelift allows you to automate, audit, secure, and continuously deliver your infrastructure. It helps overcome common state management issues and adds several must-have features for infrastructure management. Learn more Frequently asked questions How to restart a Kubernetes service? You cannot directly restart a Kubernetes Service using kubectl , because a Service represents stable network abstraction without runtime state. If your goal is to restart the underlying application (like a Deployment or Pod), you require to target the controller or resource managing the Pods. How to restart a Kubernetes deployment? Kubernetes does not offer a native restart command like some other systems. Instead, restarting is simulated by changing the deployment‚Äôs pod template metadata, prompting the system to replace the existing pods. The rollout restart command updates the spec. annotations with a timestamp, which changes the hash and initiates a new rollout. Is it safe to just delete pods? Deleting pods managed by a controller (Deployment/ReplicaSet/StatefulSet/DaemonSet) is usually safe ‚Äì the controller will recreate them. But you may cause a brief capacity dip, and for large fleets it‚Äôs harder to control blast radius than using a rolling restart. For manually-created pods without a controller, deleting them will remove them permanently




Join 1M+ Learners Learn & Practice DevOps, Cloud, AI, and Much More ‚Äî All Through Hands-On, Interactive Labs! Create Your Free Account How to Restart a Pod in Kubernetes Restarting a Pod represents common operation in Kubernetes. There are several reasons why you might desire to restart a Pod. For example, you might desire to update the container image to a newer version or modify the environment variables to adjust how your application works. In this blog post, we‚Äôll explore three different methods to restart a Pod in Kubernetes. It‚Äôs crucial to note that in Kubernetes, "restarting a pod" doesn't happen in the traditional sense, like restarting a service or a server. When we say a Pod is "restarted," it usually means a Pod is deleted, and a new one is created to replace it. The new Pod runs the same container(s) as the one that was deleted. Prerequisites To follow along with the examples in this post, you‚Äôll require a code editor. In addition, you‚Äôll require access to a running Kubernetes cluster. If you don‚Äôt have access to one, you can utilize a tool such as minikube to set up a Kubernetes cluster. Also, you‚Äôll require to have kubectl installed on your local machine to interact with the Kubernetes cluster. attempt the Kubernetes Pods Lab for free Kubernetes Pods Lab Understanding Kubernetes Pod Restart Policy In Kubernetes, a Deployment manages the lifecycle of one or more Pods. When we define a Deployment using a YAML file, the spec field of the Pod template contains the configuration for the containers running inside the Pod. The restartPolicy field is one of the configuration options available in the spec field. It allows you to control how the Pods hosting the containers are restarted in case of failure. Here's an example of a Deployment configuration file with a restartPolicy field added to the Pod spec: apiVersion: apps/v1 kind: Deployment metadata: name: demo-deployment spec: replicas: 1 selector: matchLabels: app: alpine-demo template: metadata: labels: app: alpine-demo spec: restartPolicy: Always containers: - name: alpine-container image: alpine:3. 15 command: ["/bin/sh","-c"] args: ["echo Hello World! && sleep infinity"] You can set the restartPolicy field to one of the following three values: Always : Always restart the Pod when it terminates. OnFailure : Restart the Pod only when it terminates with failure. Never : Never restart the Pod after it terminates. Note that if you don‚Äôt explicitly specify the restartPolicy field in a Deployment configuration file, Kubernetes sets the restartPolicy to Always by default. Creating a Pod To restart a Pod, we must create one first. Follow these steps to create a Pod: Open your code editor and create a new folder. Within this folder, create a new file with a. Copy and paste the following content into the new. apiVersion: apps/v1 kind: Deployment metadata: name: demo-deployment spec: replicas: 1 selector: matchLabels: app: alpine-demo template: metadata: labels: app: alpine-demo spec: restartPolicy: Always containers: - name: alpine-container image: alpine:3. 15 command: ["/bin/sh","-c"] args: ["echo Hello World! && sleep infinity"] In this file, we have defined a Deployment named demo-deployment that manages a single Pod. The Pod has one container running the alpine:3. When the container starts, it prints Hello World! to the standard output (stdout) stream and then sleeps indefinitely. Now run the following command to create the Deployment: kubectl apply -f <FILE-NAME>. yaml Replace <FILE-NAME> with the name of your file. Running the command above will produce an output similar to the following: As you can observe, the Deployment named demo-deployment was created. Next, check the Pod status by running the following command: kubectl obtain pods This command will display a list of all the Pods running in your Kubernetes cluster, as shown below: Look for the Pod with a name starting with demo-deployment and ensure that it's in the Running state. Note that Kubernetes creates unique Pod names by adding unique characters to the Deployment name. Hence, your Pod name shall be different from mine. Next, let‚Äôs view the message Hello World! printed by the container by running the following command: kubectl logs <POD-NAME> Replace <POD-NAME> with the name of your Pod. After running the command above, you should observe the output displaying Hello World! as shown below: 3 Methods to Restart Kubernetes Pod In this section, we‚Äôll explore three methods you can utilize to restart a Kubernetes Pod. Method 1: Deleting the Pod One of the easiest methods to restart a running Pod is to simply delete it. Run the following command to observe the Pod restart in action: kubectl delete pod <POD-NAME> After running the command above, you will receive a confirmation that the Pod was deleted, as shown in the output below: The job of a Deployment is to ensure that the specified number of Pod replicas (in our case, 1) is running at all times. Therefore, after deleting the Pod, Kubernetes will automatically create a new Pod to replace the deleted one. To check if Kubernetes has created a new Pod, run the following command: kubectl obtain pods This command will list all the Pods in your system: As you can observe, Kubernetes has created a new Pod. It‚Äôs worth noting that although it‚Äôs a new Pod, it still runs the same container as the deleted Pod. How can we confirm this? One way would be to check the container logs. If it‚Äôs indeed the same container running, we should observe the output Hello World!. To fetch the container logs, run the following command: kubectl logs <POD-NAME> The output displays Hello World! , which confirms that the new Pod runs the same container as the deleted Pod. Method 2: Modifying the fields in spec. containers In a Deployment configuration file, the spec. containers field describes the container(s) that should exist in a Pod overseen by the Deployment. This field holds various specifications about the container, such as its image, any necessary environment variables, and any required volumes, among others. Any modification to the value of these fields effectively changes the Pods' definition, triggering a Pod restart. To observe the Pod restart in action, let‚Äôs change the alpine image version from 3. Run the following command: kubectl set image deployment/demo-deployment alpine-container=alpine:3. 16 After running the command, you‚Äôll receive the following output: As you can observe, the container image was updated. Next, immediately run the following command: kubectl obtain pods You should observe an output, as shown above, where the previous Pod is in a Terminating state while the new Pod is already up and running. If you run the kubectl obtain pods command again after a minute or so, you will only observe the new Pod in the running state, as shown below: The reason I wanted you to run the kubectl obtain pods immediately after updating the container image, I wanted you to observe the Pod rollout process in action. During this process, Kubernetes creates Pods with the updated image and gradually phases out the old Pods. Next, as we did earlier, let‚Äôs ensure that the new Pod is indeed running the same container as the terminated Pod. Run the following command to fetch the container logs: kubectl logs <POD-NAME> As you can observe, Hello World! is displayed in the terminal, confirming that the new Pod is indeed running the same container as the deleted Pod. Note that we only changed the alpine image version to trigger a Pod restart in this example. Depending on your configuration file, you could modify other fields such as the env entries, volumeMounts , and resources fields to trigger Pod restart. Method 3: Using the "kubectl rollout restart" command You can restart a Pod using the kubectl rollout restart command without making any modifications to the Deployment configuration. To observe the Pod restart in action, run the following command: kubectl rollout restart deployment/demo-deployment After running the command, you‚Äôll receive an output similar to the following: As you can observe, the Deployment was restarted. Next, let‚Äôs list the Pods in our system by running the following command: kubectl obtain pods As you can observe in the output above, the Pod rollout process is in progress. If you run the kubectl obtain pods command again, you‚Äôll observe only the new Pod in a Running state, as shown below: To verify that the new Pod is running the same container as the deleted Pod, retrieve the container logs by running the following command: kubectl logs <POD-NAME> The terminal will display, Hello World! , confirming that the new Pod is indeed running the same container as the terminated Pod. How Do I Restart Kubernetes Pod Without Downtime? The Deployment resource in Kubernetes has a default rolling update strategy, which allows for restarting Pods without causing downtime. Here's how it works: Kubernetes gradually replaces the old Pods with the new version, minimizing the impact on users and ensuring the system remains available throughout the update process. To restart a Pod without downtime, you can choose between two methods: using a Deployment (Method 2) or using the kubectl rollout restart command (Method 3). Note that manually deleting a Pod (Method 1) to restart it won't work effectively because there might be a brief period of downtime. When you manually delete a Pod in a Deployment, the old Pod is immediately removed, but the new Pod takes some time to begin up. Note : You can confirm that Kubernetes uses a rolling update strategy by fetching the Deployment details using the following command: kubectl describe deployment/demo-deployment After running the command above, you‚Äôll observe the following output: Notice the highlighted section in the output above. The RollingUpdateStrategy field has a default value of 25% max unavailable, 25% max surge. 25% max unavailable means that during a rolling update, 25% of the total number of Pods may be unavailable. And 25% max surge means that the total number of Pods can temporarily exceed the desired count by up to 25% to ensure that the application is available as old Pods are brought down. Master container orchestration with our Kubernetes Learning path : Kubernetes Learning Path | Kodekloud Embark on the Kubernetes learning path. Hone your Kubernetes skills with our study roadmap. begin your Kubernetes journey today. Kodekloud Conclusion Knowing how to restart a Pod is an crucial skill when managing containerized applications on a Kubernetes cluster. In this blog post, you learned about three different methods you can utilize to restart a Kubernetes Pod. You also learned about how Kubernetes uses the rolling update strategy to restart Pods without causing any interruptions, keeping the application up and running for your users even during the update process. More on Kubernetes: How to Fix ImagePullBackOff & ErrImagePull in Kubernetes How to Manage Your K8s with K9s Kubernetes CLI How to utilize Kubectl Scale on Deployment How to utilize Kubectl Config Set-Context How to obtain Pod Logs in Kubernetes (With Examples) How to Copy File From Pod to Local (With Examples) How to Execute Shell Commands Into a Container (With Examples) Optimizing Kubernetes Clusters for Cost & Performance: Part 1 - Resource Requests Deploying, Maintaining, and Scaling Kubernetes Clusters Join 1M+ Learners Learn & Practice DevOps, Cloud, AI, and Much More ‚Äî All Through Hands-On, Interactive Labs! Create Your Free Account Hemanta Sundaray 29 min read Certified Kubernetes Security Specialist (CKS) Exam Verification Guide Pramodh Kumar M Pramodh Kumar M LinkedIn Dec 17, 2025 ‚Ä¢ K8s ‚Ä¢ CKS ‚Ä¢ CKS Kubernetes 22 min read How to Build a CI/CD Pipeline on AWS in 2026 (Step-by-Step Guide) Nimesha Jinarajadasa Nimesha Jinarajadasa Nimesha Jianrajadasa represents DevOps & Cloud Consultant, K8s expert, and instructional content strategist-crafting hands-on learning experiences in DevOps, Kubernetes, and platform engineering. LinkedIn Dec 15, 2025 ‚Ä¢ AWS ‚Ä¢ CI/CD Pipeline ‚Ä¢ CI/CD 15 min read Multi-Cloud DevAIOps: Engineering High-Performing Cloud & AI-native Dev Teams in the GenAI Era Dheeraj Nayal Dheeraj Nayal Dec 12, 2025 ‚Ä¢ DevAIOps ‚Ä¢ DevOps ‚Ä¢ Multi-Cloud 11 min read 1 Million Hands-On Learners: How KodeKloud Labs Built a Global Movement - One Command at a Time Nimesha Jinarajadasa Nimesha Jinarajadasa Nimesha Jianrajadasa represents DevOps & Cloud Consultant, K8s expert, and instructional content strategist-crafting hands-on learning experiences in DevOps, Kubernetes, and platform engineering. LinkedIn Dec 8, 2025 ‚Ä¢ DevOps hands-on labs ‚Ä¢ hands-on-labs-free ‚Ä¢ AI 25 min read Istio Certified Associate (ICA) Study Guide Pramodh Kumar M Pramodh Kumar M LinkedIn Dec 3, 2025 ‚Ä¢ Istio ‚Ä¢ service mesh ‚Ä¢ Istio Certified Associate 7 min read From AI Literacy to AI Readiness: The 2026 Playbook for Universities Shashank Karpal Shashank Karpal Shashank leads Education & Government Programs at KodeKloud and writes about AI-native education and workforce transformation, covering hands-on labs, agentic workflows, and outcome analytics for academia, enterprises, and governments. LinkedIn Dec 2, 2025 ‚Ä¢ AI ‚Ä¢ AI Literacy 8 min read desire a Competitive Edge in 2026? Follow This AI-Powered Roadmap for DevOps & Cloud Engineers Nimesha Jinarajadasa Nimesha Jinarajadasa Nimesha Jianrajadasa represents DevOps & Cloud Consultant, K8s expert, and instructional content strategist-crafting hands-on learning experiences in DevOps, Kubernetes, and platform engineering. LinkedIn Dec 1, 2025 ‚Ä¢ AI ‚Ä¢ AI Agents ‚Ä¢ Cloud 19 min read Top AWS Certifications in 2026: Which Are Worth Your Investment? Nimesha Jinarajadasa Nimesha Jinarajadasa Nimesha Jianrajadasa represents DevOps & Cloud Consultant, K8s expert, and instructional content strategist-crafting hands-on learning experiences in DevOps, Kubernetes, and platform engineering. LinkedIn Nov 27, 2025 ‚Ä¢ AWS ‚Ä¢ AWS Certification ‚Ä¢ Cloud 34 min read CKAD Exam Verification Guide Pramodh Kumar M Pramodh Kumar M LinkedIn Nimesha Jinarajadasa Nimesha Jianrajadasa represents DevOps & Cloud Consultant, K8s expert, and instructional content strategist-crafting hands-on learning experiences in DevOps, Kubernetes, and platform engineering. LinkedIn +1 Nimesha Jinarajadasa Nov 25, 2025 ‚Ä¢ CKAD ‚Ä¢ CKAD Tip ‚Ä¢ Kubernetes 3 min read The Hackathon Habit: Turning Everyday Problems Into Creative Sprints Phuong Vu Phuong Vu Nov 24, 2025 Subscribe to Newsletter Join me on this exciting journey as we explore the boundless world of web design together. Subscribe Sending Sent Please check your inbox and click the confirmation link




Related blog posts View all posts December 1, 2025 ‚Ä¢ 9 Minute Read Architecting a Private Cloud for AI Workloads Public clouds are convenient for AI experimentation, but production workloads often hit walls. For enterprises running continuous training and inference, a private cloud can deliver better ROI, data sovereignty, and performance. This comprehensive guide walks through architecting a private cloud for AI workloads from the ground up. November 26, 2025 ‚Ä¢ 6 Minute Read Demystifying Karpenter on GCP: The Complete Setup Guide Karpenter has become the gold standard for Kubernetes autoscaling on AWS, but what about GCP? This guide shows you how to set it up and introduces a better way. November 21, 2025 ‚Ä¢ 5 Minute Read GPU Multitenancy in Kubernetes: Strategies, Challenges, and Best Practices GPUs don't support native sharing between isolated processes. Learn four approaches for running multitenant GPU workloads at scale without performance hits. Sign up for our newsletter Be the first to understand about new features, announcements and industry insights. eu1 26230392 15314855-8a4c-4751-ad29-605050b96984 loft-sh true Subscribe Join our newsletter to stay up to date on new releases. eu1 26230392 15314855-8a4c-4751-ad29-605050b96984 loft-sh true By subscribing you agree to our Privacy Policy and provide email consent Product GitHub Getting Started Documentation Slack Changelog Company About Blog Careers Events ¬© 2025 vCluster, All rights reserved. Privacy Policy Terms of Service Cookie Policy




Home Blog Kubernetes How to Restart a Pod in Kubernetes? Methods and Best Practices How to Restart a Pod in Kubernetes? Methods and Best Practices Boemo Wame Mmopelwa #Kubernetes Published on Nov 22, 2024 Updated on Nov 7, 2025 Pods are essential for deploying and scaling applications in Kubernetes. When a Pod starts having issues such as high resource consumption or misconfigurations, updating and restarting the Pod may be the solution. Restarting a Pod fixes many issues and enables you to add new Pod configuration changes. In this article, you will learn when it is crucial to restart a Pod and how to restart the Pod in order to fix issues and optimize the cluster. # Prerequisites You require a running Kubernetes cluster and have at least basic knowledge on how to create a Pod you will utilize in this tutorial. Build and scale your self-managed Kubernetes clusters effortlessly with powerful Dedicated Servers ‚Äî ideal for containerized workloads. obtain started now # Why should you restart a Kubernetes Pod? Restarting a Pod represents remedy for many issues that Pods face. However, not every error or problem a Pod experiences may be solved with a restart. Below are the valid reasons for restarting a Pod. Fixing resource issues: Pods can experience resource-related issues such as CPU throttling and memory leaks. If a Pod uses all the resources it was allocated to within a short time, the containers and applications hosted by the Pod shall be starved of resources. Uncontrolled resource issues degrade application performance or can even crash the application. Restarting the Pod minimizes the resource issue by resetting CPU usage and clears temporary memory issues. Updating Pod configuration: When you create Pod configuration changes such as updating the Pod‚Äôs environment variables or updating the Secret and ConfigMaps, it is crucial to restart the Pod so that the changes may be applied. Debugging and troubleshooting Pods: Restarting a misbehaving Pod can assist clear temporary issues or glitches. Also, restarting a Pod can trigger the generation of diagnostic logs or metrics that are useful for troubleshooting. Monitoring the Pod during and after the restart can provide insights into the root cause of issues. The status of the Pod determines whether the Pod needs a Restart or not. A Pod that is consistently failing and in a pending status needs a troubleshooting procedure. A Pod goes through 5 stages in its lifecycle. Below are the 5 stages of the Pod lifecycle: Pending: A Pod gets in the pending phase when it is being scheduled and bound to a node and necessary resources. The Pod was created but isn't yet running on a node. It is called the pending phase because the Kubernetes scheduler is still looking for a node that will run the Pod. Running: After a Pod was assigned to a node it moves to the running phase where it will begin to execute its tasks such as starting networks. This is the stage where the Pod starts pulling the container‚Äôs image from the registry and setting up storage volumes and networks. Also, Kubernetes prepares the environment for the containers. This is the functional stage of the Pod. Kubernetes will monitor the resource usage and the health of the Pod using readiness and liveness probes. Success or Failure: The success and failure of the Pod is determined by the status of the containers. If containers fail because of persistent errors the Pod will fail too. If the containers execute their tasks successfully the Pod shall be successful. Termination: If a node running the Pod fails, the Pod will automatically obtain terminated. Deployment scalability and API server issues can also lead to a Pod being terminated. The Pod‚Äôs resources shall be released after it gets terminated. A Pod can also obtain into a phase called ‚ÄúUnknown‚Äù if the Kubernetes control plane cannot determine the status of the Pod. # Different methods for restarting a Kubernetes Pod In this section, you will learn various ways that may be used to restart a Pod. Every method has its own pros and cons. Below are different techniques you can implement to restart Pods. # Restarting a Pod by Scaling the number of Pods When traffic increases or decreases it is crucial to scale the deployments to create sure that resources are used efficiently and no downtime occurs. Whenever you scale the number of Pods, Kubernetes will automatically restart the Pods whether you are scaling up or down. utilize the following command to increase or decrease the number of Pod replicas. kubectl scale deployment <deployment name> -n <namespace> --replicas=3 You will obtain the following output that shows the number of new replicas created. # Restarting a Pod by updating configuration specifications When you change the Pod‚Äôs configuration, Kubernetes shall be forced to restart the Pod in order to apply and enable the new changes added to the Pod‚Äôs configuration file. Updating the Pod configuration with new variables and values that are incorrect will cause more errors. This method of restarting the Pod by updating its specifications is complex. So, it is crucial to ensure that the configuration updates are made accurate. utilize the following command to restart a Pod by replacing its configuration with new configuration details using a YAML file. kubectl obtain pod <pod_name> -n <namespace> -o yaml | kubectl replace --force -f - # Restarting a Pod by Deleting it One of the most straightforward ways to restart a Pod is by deleting it. Kubernetes controllers like Deployments and ReplicaSets ensure that the desired number of Pods are running. When you delete a Pod that's managed by a controller, Kubernetes automatically creates a new one to replace it using the specified configuration. utilize the following command to delete a Pod. kubectl delete pod <pod_name> -n <namespace> # Using the rollout restart command The kubectl rollout restart command allows you to restart Pods for a specific deployment gradually without experiencing downtime. utilize the following command to restart a deployment rollout. kubectl rollout restart deployment <deployment_name> -n <namespace> Also read: How to deploy Kubernetes on Bare Metal # Best practices for restarting Kubernetes Pods Abruptly restarting Pods can have serious consequences on the containers. That is why it is crucial to follow best practices when restarting Pods. This section will teach you different ways you should follow in order to safely restart Pods. utilize liveness and readiness probes: The liveness probe is used to check if an application in a Pod is failing or crashing. The status of the application is used to determine the health of the Pod. The liveness probe is crucial because it may be instructed to restart the application if it crashes after a certain specified period of time. On the other hand, the readiness probe is used to check if an application is ready to begin receiving incoming traffic. Before you begin sending traffic to the newly deployed application it is crucial to utilize the readiness probe to understand when an application is ready to begin receiving incoming traffic. Doing this will assist save resources and prevent traffic from being sent to unhealthy and unready applications. Monitor Pod restarts: After restarting the Pod plans might not go well as expected. So, it is crucial to monitor the restarted Pods to ensure that you are alert when they fail. You can monitor them by analyzing logs or setting up metrics collection tools such as Prometheus to collect metrics and notify you when the Pod exceeds set thresholds. Avoid frequent restarts: Frequent Pod restarts can lead to instability and performance degradation. It's crucial to minimize unnecessary restarts to maintain a healthy Kubernetes environment. Test in non-production environment: Before restarting Pods that are in the production environment it is crucial to test the new changes in the development environment. Testing a Pod helps you to identify issues early and validate changes before deploying the new version to the live environment. # Conclusion In this guide, you have learned various reasons why it is necessary to restart a Kubernetes Pod and how to restart the Pod in different ways. In addition, you have learned best practices to follow when restarting Pods. Consistently experiencing the require to restart Pods might be a sign of a bigger underlying problem. Restarting a Pod solves surface problems. Persistent resource and network issues require full troubleshooting and solutions. Deeper resource issues may be solved by expanding memory and CPU. Fortunately, Cherry Servers offers elastic cloud storage solutions that include security such as RBAC. Fix your Kubernetes resource issues by using a premium storage bandwidth at a lower cost. Cloud VPS Hosting Starting at just $ 3. 24 / month, obtain virtual servers with top-tier performance. Check Available Servers Share this article Related Articles #Kubernetes Published on Oct 10, 2025 Updated on Nov 7, 2025 OpenShift vs Kubernetes on Bare Metal: Which One to Choose Compare Kubernetes vs OpenShift on bare metal. Learn how each platform performs, their setup, features, costs, and which suits your infrastructure best. Read More #Kubernetes Published on Sep 26, 2025 Updated on Nov 7, 2025 How to Create a Kubernetes Cluster with Minikube and Kubeadm Learn how to create a Kubernetes cluster using Minikube for local development and Kubeadm for production-ready setups with step-by-step guidance. Read More #Kubernetes Published on Sep 24, 2025 Updated on Nov 7, 2025 How to Install Calico on Kubernetes: Step-by-Step Tutorial Install Calico on Kubernetes with Helm or YAML for secure, scalable networking. Learn setup steps, benefits, and performance boosts for your cluster




< Back to Blog Restarting Kubernetes Pods: A Detailed Guide May 28, 2025 | By: Mark Bakker Share Share This blog will assist you learn all about restarting Kubernetes pods and give you some tips on troubleshooting issues you may encounter. Kubernetes pods are one of the most commonly used Kubernetes resources. Since all of your applications running on your cluster live in a pod, the sooner you learn all about pods, the better. One of the things you‚Äôll probably require to do from time to time is to restart your Kubernetes pods, either to reload their configuration or to recover from a crashed application situation. We‚Äôll guide you through exactly how to do that in this post, but before we dive into restarting pods, let‚Äôs do a quick recap. What are Kubernetes Pods and Why You Would desire to Restart Them? When you desire to deploy your application on Kubernetes, you have a few options for doing that depending on your needs. Kubernetes pods are the simplest and smallest things you can deploy on Kubernetes. You can‚Äôt deploy a container on its own in Kubernetes. Instead, you require to encapsulate it in a pod. Therefore, when you desire to effectively restart your application, you basically require to restart the pod that it‚Äôs running in. But restarting a pod is not actually as simple and straightforward as one would think. It‚Äôs not difficult either, but there are some things you require to understand about restarting a pod in order to avoid some unexpected issues. For example, depending on the restart method you choose, you may or may not experience downtime of your application. You also absolutely require to understand that it‚Äôs possible that your pod won‚Äôt begin correctly after a restart. This can happen, for example, if a Kubernetes secret that the pod was using was deleted in the meantime. In rare cases, it can also happen that your pod shall be stuck in the ‚Äúwaiting for resources‚Äù state for quite a while if your cluster operates at full capacity. Let‚Äôs Restart Some Pods Enough theory‚Äîlet‚Äôs dive into practice. If we desire to restart some pods, we require to deploy some first. Let‚Äôs create a simple deployment with the following YAML definition: So, in order to create a pod, save the above code snippet into a YAML file and apply it with kubectl apply-f [filename. yaml]: Now, our deployment was created, and the deployment resource should create one pod for us. Before we can restart our pod, we require to verify a few things. First of all, we require to check if our pod is in fact running. We can do that with the following command: OK, it looks like our pod does indeed exist. The second thing we require to check is the state of the pod. If your pod is in a terminal state (succeeded or completed), then restarting won‚Äôt truly work. You‚Äôll just delete the pod, but a new one won‚Äôt be created. Technically, you can restart a pod that‚Äôs in a pending state and you could do that, for example, when you decided to create a last-minute configuration change. But normally you would restart a pod that‚Äôs in either a running or failed state. You can check the state of a pod with the same command that we used above. There, in the same output, we can observe that our pod is in a ‚Äúrunning‚Äù state. We are all set then, so it‚Äôs time to restart our innocent pod. One could expect that it‚Äôs as easy as executing a command such as kubectl restart pod followed by a pod name. It may come as a surprise to you, but there‚Äôs no kubectl restart pod command. In fact, there isn‚Äôt any command to restart the pod. Wait, what? Well, in the Kubernetes world, you actually can‚Äôt restart a pod in a typical sense. What you can do, however, is delete the pod. And once you do that, Kubernetes will realize that the number of pods running doesn‚Äôt match its desired state and, therefore, will quickly create a new one. The outcome basically shall be as if you‚Äôd restarted the pod. So, long story short, the easiest way to ‚Äúrestart‚Äù a pod in Kubernetes is to execute kubectl delete pod [pod_name]: Kubernetes says that our pod was deleted. Let‚Äôs list the pods again then: It looks like Kubernetes lied to us. It seems like the pod hasn‚Äôt been deleted. But if you look closely, you‚Äôll observe that the pod ID (the random numbers and letters after the pod name) are different. Also, if you check the age of the pod, you‚Äôll observe it‚Äôs only four seconds old. All of that tells us that the pod we observe now is actually not the same pod that we originally deployed. Kubernetes did its job. As soon as we deleted one pod, Kubernetes created a new one to bring the cluster state back to its desired state. In other words, we effectively restarted our pod. Great! Now you understand how to restart a pod. But there‚Äôs something else you should understand. First of all, that‚Äôs not the only method of restarting a pod. And second, this method will introduce a downtime to your application since Kubernetes will only create a new pod after the old one was deleted. Let‚Äôs take a look at other methods to restart a pod. A bit more of a sophisticated method of restarting a pod is to perform a rollout restart of a deployment. Our pod is managed by a Kubernetes deployment resource, and with Kubernetes deployments, it‚Äôs possible to do the opposite of what we just did. Instead of deleting a pod and then creating a new one, rollout restart will first create a new pod and, once that new pod is ready, only then will it terminate the old one. Let‚Äôs observe that in action. To perform rollout restart, you require to execute kubectl rollout restart deployment [deployment_name]: And if you‚Äôre quick enough in executing kubectl obtain pods, you may be able to observe the whole process: As you can observe, with this method we always have at least one pod running. Therefore, there‚Äôs no downtime like there was with the previous method. You can also observe that with this method, it may happen that for a short period of time both old and new pods shall be running at the same time. Depending on how your application works, this may be an issue, so it‚Äôs something to be aware of. How to cease and begin a Pod Both methods shown above have two things in common‚Äîthe pod restart is truly quick, and you don‚Äôt have control over it. But there are times when you may desire to restart a pod ‚Äúslower‚Äù with a few seconds or even minutes to do something in between. In such cases, you can utilize the kubectl scale command to cease the pod first, then do whatever you require to do before bringing the pod up again. Normally, the kubectl scale command is used to increase or decrease the number of pods to deal with the load‚Äîfor example, from 2 to 5. But you can also scale the number of pods to 0 and then scale it back to at least 1. This will effectively restart your pods. Let‚Äôs observe an example: If you‚Äôve worked with Kubernetes for a bit already, you may be wondering why you can‚Äôt just delete the deployment and recreate it later instead. Sure, you could do that as well with a similar result, but there‚Äôs a key difference. You should utilize the scaling method when you don‚Äôt desire to lose the history of the deployment. You‚Äôll still be able to observe when it was originally created and when it was scaled. Sometimes this information may be useful. If you delete the deployment and recreate it later, from Kubernetes‚Äô perspective, these shall be just two completely separate deployments that won‚Äôt have anything in common. This method may be useful, for example, when you are debugging an issue and desire to be 100% sure that new pods are starting from a ‚Äúclean state. ‚Äù What If Something Goes Wrong? Now you understand a few ways to restart a pod in Kubernetes. But restarting a pod may sometimes cause a pod to fail‚Äîfor example, if you desire to restart a pod in order to change its configuration or add a new configmap or volume. Any of these changes could introduce some bugs that prevent the pod from starting again properly. Let‚Äôs observe how we can troubleshoot issues in such cases. Imagine that your pod uses some Kubernetes secrets. You wanted to update the secret, so you ran your CI/CD process for that, and then you restarted the pod using the kubectl delete pod method to test the change quickly. Oh, no! Instead of a shiny new pod, you observe some errors. What now? The first and easiest thing to do in such cases is to execute the kubectl describe pod command followed by a pod name. There, in the Events section of the output, you should observe some indication of what‚Äôs going on: Pretty straightforward, isn‚Äôt it? It seems like there was a problem with the secret update, and it was deleted instead. So, to fix our failed pod restart in this case, we require to restore the secret that the pod wants to load. This is just one example, but as a general rule, kubectl describe pod can usually tell you what the problem with your pod is. However, sometimes it won‚Äôt. Imagine that you restored the secret on the cluster and restarted your pod again but it still fails. This time, however, kubectl describe pod doesn‚Äôt say anything about the missing secret. This could indicate that now we have an issue not with the pod configuration but with the application running in the pod. If that happens to you, the next debugging step is to check the logs of your application. You can do that by executing kubectl logs [pod_name]: Aha! It seems like the secret is not missing anymore, but it‚Äôs simply the wrong one now. Updating the secret to the correct one and restarting the pod one more time will fix your problem. Summary As you can observe, something as simple as restarting a pod is worth a whole blog post. Not only can you actually directly restart a pod in Kubernetes, but you have a few ways of achieving a pod restart. If you desire some more tips on troubleshooting Kubernetes, check out this guide ‚Äú Top 4 Kubernetes Troubleshooting Issues Identified and Fixed in Minutes. ‚Äù And if you desire to avoid failures in your cluster caused by issues with configurations, typos or any other reasons, sign up for a 30-day free trial of SUSE Cloud Observability on the AWS Marketplace. Share (Visited 82 times, 1 visits today) Related Articles Apr 09th, 2025 Driving Kubernetes Modernization Together: SUSE and CloudCasa heidi. com Jul 22nd, 2025 SIGKILL vs SIGTERM: A Developer‚Äôs Guide to Process Termination Genevieve Cross Jan 06th, 2025 5 Reasons Why Enterprise Leaders Must Prioritize Owning Their AI Platforms in 2025 Jen Canfor Mar 18th, 2025 Zero Trust Infrastructure with SUSE Linux & Confidential Computing Sebastian Martinez 2,179 views Mark Bakker Co-founder of StackState and now serves as a Product Owner at SUSE, leveraging his extensive experience as an IT architect. Mark plays a key role in shaping the SUSE Observability solution and is dedicated to creating solutions that drive efficiency and innovation




Blog / DevOps How to Restart Kubernetes Pods with Kubectl: 5 Methods observe StrongDM in action ‚Üí Written by StrongDM Team Universal Privileged Access Authorization (UPAA) StrongDM Last updated on: May 15, 2024 Reading time: 5 minutes Contents Secure Access Made Simple Built for Security. Free Trial ‚Äî No Credit Card Needed Full Access to All Features Trusted by the Fortune 100, early startups, and everyone in between Free Trial Kubernetes pod restarts are crucial for efficiently managing containerized applications in a dynamic microservices architecture. Understanding how to effectively restart pods using kubectl will assist you streamline operations and minimize downtime. This article describes five methods to restart Kubernetes pods empowering you to maintain application health and performance confidently. Key Stages in the Lifecycle of a Kubernetes Pod Understanding the Kubernetes pod lifecycle is crucial for efficient management and troubleshooting within a containerized environment. Pods progress through various states from creation to termination, each of which plays a critical role in deploying and maintaining your application. Exploring these states will assist you comprehend the require to restart your pod at different points. Creation: Pods are created manually or through controllers like Deployments and StatefulSets. During this state, the container initializes with the defined configuration and resources, ready to perform the desired task. Pending: At this state, the pod was created, but one or more of its containers are not yet running. This could be due to resource limitations or waiting for dependencies to become available. Running: A pod is running when all containers within it are operational. Applications inside containers actively process requests and perform their intended functions. Success/Failure/Completed: Pods enter these states based on the outcome of their tasks. A pod is in a successful state when all containers within it have successfully terminated their tasks. Conversely, if a container within a pod fails, the pod enters a failed state. Additionally, when all containers in a pod complete their tasks, the pod enters a completed state. Termination: A pod is terminated either spontaneously or due to external factors like node failure or scaling operations. During termination, resources associated with the pod are released, and the container stops gracefully. These states may necessitate restarting the pod to address various scenarios such as updating configurations, troubleshooting performance issues, and recovering from errors. Reasons for Restarting a Kubernetes Pod Restarting a Kubernetes pod is common when managing containerized applications for a variety of reasons: Configuration changes: Restarting a pod changes the environment variables. New settings like volume mounts are effectively applied. Application updates: Ensure that the latest changes, such as new images for deployment or updates to code, are included upon pod restart. Troubleshooting: It helps resolve issues like crashes or resource conflicts to restore normal operation. Resource management: By restarting pods, resource limits may be adjusted and usage optimized to ensure efficient resource utilization within the Kubernetes cluster. Network/Service Discovery Adjustments: Network configuration changes may be adopted upon pod restart, ensuring proper communication between application components. State Cleanup: It clears accumulated state to improve application performance and stability. Performance optimization: It updates resources, reduces memory leaks, and improves overall performance. Health Indicators Monitoring pod status and interpreting health indicators is an essential way to maintain the stability and reliability of Kubernetes deployment. By understanding how to assess pod health and interpret health metrics, operators can proactively identify issues and decide when to restart pods to ensure optimal performance and availability. Health indicators: Kubernetes provides several health indicators to assess the health of pods. These are: Readiness Probe: This determines if the container is ready to handle traffic. If the readiness check fails, the container is removed from service until it succeeds. Liveness Probe: This verifies that the container is responsive and operational. If the liveness check fails, Kubernetes restarts the container. Resource usage metrics: Monitoring resource usage metrics such as CPU and memory usage will indicate performance issues or resource contention. Logs and events: Reviewing pod logs and Kubernetes events provides insight into application behavior, errors, and potential issues. Prerequisites for Restarting Kubernetes Pods Before getting into how to restart a Kubernetes pod using Kubectl, certain prerequisites must be met to ensure a smooth and efficient process. These prerequisites are primarily about having the necessary access and tools to interact with the Kubernetes cluster. Access to Kubernetes Cluster: create sure you have the credentials (such as a kubeconfig file or API token) and permissions to authenticate to the Kubernetes cluster where kubectl is deployed. Without appropriate permissions, you cannot run commands or create changes to the cluster. Setting up kubectl: kubectl is the command used to interact with Kubernetes. It facilitates various operations including managing pods, deployments, services, etc. Before restarting the pod, ensure that kubectl is properly installed and configured on your local computer or the system where you are running the command. Installation: Install kubectl on your operating system by following the official Kubernetes documentation or distribution-specific instructions. Configuration: Configure kubectl to connect to the target Kubernetes cluster by setting the appropriate kubeconfig file or environment variables. This ensures that Kubectl communicates with the correct cluster and authenticates itself using the provided credentials. Once you have access to the Kubernetes cluster and Kubectl is properly configured, you can proceed with restarting pods. Methods to Restart Kubernetes Pods Using Kubectl Restarting a Kubernetes pod using Kubectl provides flexibility and control in managing container applications in a Kubernetes cluster. There are five ways to restart pods with Kubectl, each addressing different scenarios and needs. Method 1: kubectl Delete Pod Command This method involves deleting the pod directly. It's crucial to ensure that the pod is terminated before executing this command, as Kubernetes will create a new pod instance to replace it. `kubectl delete pod <pod_name>` Method 2: kubectl Rollout Restart Command If the pod is managed by a deployment, a rollout restart policy is used to properly restart pods, ensuring zero downtime. `kubectl rollout restart deployment/<deployment_name>` Method 3: kubectl Scale Replicas command Scaling a deployment's replica down to zero and backing it up initiates a restart of all pods controlled by that deployment. ```kubectl scale deployment <deployment_name> --replicas=0 kubectl scale deployment <deployment_name> --replicas=<desired_replica_count>``` Method 4: Updating Environment Variables (kubectl Set Env) Updating environment variables associated with a pod can trigger a restart to apply the new configurations. `kubectl set env pod <pod_name> <key>=<value>` Method 5: Using a restart policy Specifying a restart policy in the pod manifest allows automatic restarts based on defined conditions, such as OnFailure or Always. Define the restart policy in the pod spec: ```yaml spec: restart policy: Always``` These methods offer versatility in restarting Kubernetes pods to accommodate various utilize cases, from manual intervention to automated restarts based on defined policies. Troubleshooting common pod restart issues Although restarting a Kubernetes pod using Kubectl is generally straightforward, there are certain common issues that you require to troubleshoot to ensure a smooth restart process. Here are steps to identify and resolve common pod restart issues. Removing stuck pods: Issue: Pods stuck in a terminal state may prevent the creation of new pods. Solution: utilize the `--force` flag with `kubectl delete pod` to force delete stuck pods and ensure there are no underlying node issues. Liveness check failure: Issue: Liveness check failure causes pods to continually restart. Solution: Check the liveness probe configuration, check logs/events, and resolve the root cause, like application code or environment issues. Resource contention: Issue: Restarting pods at the same time causes resource contention. Solution: Implement a pod suspension budget to limit concurrent restarts and accommodate off-peak schedules. Configuration errors: Issue: Configuration errors lead to application errors. Solution: Double-check that your changes are accurate, utilize version control, and roll back if necessary. Network connectivity issues: Issue: Network connectivity is temporarily lost during pod restart. Solution: Monitor network traffic, verify proper configuration, and adjust firewall rules as necessary. Integrating StrongDM for Easier Kubernetes Management StrongDM streamlines operational workflows and enhances the security of Kubernetes cluster management. Here's how it simplifies Kubernetes management, including pod restarts: 1. Centralized access control: StrongDM provides a centralized platform for managing access to Kubernetes clusters and managing individual credentials. Administrators can define access policies based on roles and permissions to allow authorized users to perform actions such as restarting a pod. Auditing and Compliance: StrongDM provides auditing capabilities to track user activity, including pod restarts, for compliance and security. A Kubectlgs provides visibility into user actions and supports compliance with legal requirements and internal policies. Automation and Orchestration: StrongDM integrates with automation tools to enable streamlined Kubernetes management, such as restarting pods. Administrators can utilize StrongDM's API and CLI to automate tasks such as restarting pods based on triggers or schedules, reducing manual effort. Improved security: StrongDM enforces the least privilege access, restricting users to necessary resources and actions. Session recording allows you to monitor pod restarts in real-time, allowing you to detect suspicious activity and reduce security risks. Regarding the upcoming Kubernetes changes, our product and engineering teams shall be rolling out updates in the next few weeks or months, accompanied by new documentation. To sum things up, mastering pod restarts is critical to managing Kubernetes and ensuring application stability, reliability, and performance in container environments. We delved into the importance of pod restarts and the many methods available through Kubectl to efficiently manage Kubernetes, covering scenarios such as configuration changes, troubleshooting, and performance tuning. Understanding these methods allows operators to effectively manage the pod lifecycle and ensure smooth application operations while troubleshooting common restart issues and maintaining deployment stability. Integrating solutions like StrongDM streamlines Kubernetes management by centralizing access control, auditing, automation, and improved security, ultimately streamlining management operations and enhancing application stability and reliability. Overall, mastering pod restarts assist Kubernetes administrators navigate dynamic environments, effectively manage applications, reduce risk, and drive success in cloud-native initiatives. desire to simplify Kubernetes management? attempt StrongDM for free. Next Steps StrongDM unifies access management across databases, servers, clusters, and more‚Äîfor IT, security, and DevOps teams. Learn how StrongDM works Book a personalized demo begin your free StrongDM trial Share this: Share How to Restart Kubernetes Pods with Kubectl: 5 Methods on LinkedIn Share How to Restart Kubernetes Pods with Kubectl: 5 Methods on Reddit Share How to Restart Kubernetes Pods with Kubectl: 5 Methods on Facebook Share How to Restart Kubernetes Pods with Kubectl: 5 Methods on Hacker News Categories: DevOps Kubernetes About the Author StrongDM Team , Universal Privileged Access Authorization (UPAA) , the StrongDM team is building and delivering a Zero Trust Privileged Access Management (PAM), which delivers unparalleled precision in dynamic privileged action control for any type of infrastructure. The frustration-free access stops unsanctioned actions while ensuring continuous compliance. üíô this post? Then obtain all that StrongDM goodness, right in your inbox. You May Also Like Kubernetes Security: Guide to Securing Your Clusters Kubernetes security is the practice of protecting containerized workloads and cluster components from unauthorized access, misconfigurations, and vulnerabilities. It involves securing the infrastructure, clusters, containers, and application code through layered controls like RBAC, network policies, image scanning, and runtime protection. StrongDM Kubernetes: Zero Trust Access for Kubernetes Clusters StrongDM‚Äôs Next-Gen Kubernetes provides secure, seamless access to Kubernetes clusters at scale. By eliminating standing privileges and enforcing Zero Trust security principles, StrongDM helps security teams maintain tight access controls without slowing down DevOps workflows. What Are Microservices in Kubernetes? Architecture, Example & More Microservices create applications more scalable and resilient, and Kubernetes is the backbone that keeps them running smoothly. By orchestrating containers, handling service discovery, and automating scaling, Kubernetes simplifies microservices management‚Äîbut it also introduces complexity. This guide covers key principles, deployment strategies, and security best practices to assist you navigate microservices in Kubernetes. Plus, observe a modern way of simplifying access and security, so your teams can build faster‚Äîwithout compromising control. What Is Kubernetes Observability? Best Practices, Tools & More Kubernetes observability is the practice of monitoring and analyzing a Kubernetes environment through metrics, logs, and traces to gain visibility into system performance and health. It enables teams to detect and resolve issues proactively, optimize resource utilization, and maintain cluster reliability through real-time insights and automated monitoring tools. What Is Kubernetes Ingress? Guide to K8s Traffic Management This article breaks down Kubernetes Ingress, explaining how it manages external access to services, routing configurations, and best practices. You‚Äôll learn how Ingress differs from Load Balancers, how controllers enforce routing rules, and how to choose the right setup for your needs




Back to Blog Knowledge 9 mins read How to Restart Pods in Kubernetes Using kubectl By Keval Bhogayata Updated 29 Sep 2025 What's in this article Share Now: Learn how to restart Kubernetes pods using kubectl, why it matters, and the safest methods plus monitor pods easily with Middleware for smarter decisions. kubectl is the essential command-line tool for managing Kubernetes clusters. It allows you to manage pods, deployments, and other resources from the terminal, helping you troubleshoot Kubernetes issues , check pod health, and scale applications easily. Most kubectl commands follow a simple structure. For example, kubectl obtain pods lists running pods, and kubectl delete pod <pod-name> removes a pod. Many users wonder how to restart a Kubernetes pod using kubectl. Contrary to popular belief, there is no direct kubectl restart pod command. Instead, Kubernetes expects you to work with higher-level objects, such as Deployments. This guide covers the safest and most effective methods for restarting pods, including rollout restarts, deleting pods, scaling replicas, and updating environment variables, helping you maintain a healthy and stable cluster without guesswork. Table of Contents When Should You Restart a Kubernetes Pod? Knowing when to restart a Kubernetes pod is key to maintaining application stability and performance. Here are the most common scenarios that require a pod restart: 1. Configuration Changes When you update your application‚Äôs settings (such as environment variables or resource limits), the pod continues to utilize the old configurations. Restarting ensures the new settings take effect. Recover from Application Failure If your app crashes but the container stays in a ‚ÄúRunning‚Äù state, or the pod shows as running but isn‚Äôt functioning, a restart forces a clean begin to recover the service. üê≥For more in-depth analysis of pod failures, check out our guide on Exit Code 137 in Kubernetes: Causes, Diagnosis, and Fixes. Debugging Application Issues Restarting the pod helps resolve temporary issues or confirms persistent problems while troubleshooting why the application isn‚Äôt behaving as expected. Effective debugging often starts with understanding application logs. Learn how to tail kubectl logs in real-time. Pod Stuck or Not Responding A pod may cease responding to traffic while Kubernetes still reports it as healthy. Restarting resolves frozen states or resource leaks and restores responsiveness. What are the Different Pod States in Kubernetes? Understanding the different Kubernetes pod states enables you to monitor your application‚Äôs health and take the necessary actions when needed. Here are the key pod states you should understand: 1. Pending Kubernetes has approved the pod, but it is awaiting scheduling and launch. This occurs while Kubernetes is downloading container images or while it is still looking for a suitable node to run your pod. A prolonged pending pod typically indicates a configuration issue or insufficient resources. Running Your pod has at least one active container. The containers are working, but this doesn‚Äôt mean everything is functional. Your application may still have troubles despite the pod running. Succeeded You typically observe this state with jobs or one-time tasks that are designed to run once and finish. It means all containers in the pod have completed their tasks successfully and won‚Äôt restart. Failed The failed state means one or more containers in the pod have stopped running, maybe due to an error, or the system terminated the containers. It indicates something went wrong with your application, or the container couldn‚Äôt restart correctly. Failed pods often require a restart. Instead of reacting to a failing pod, ‚ö†Ô∏è learn how to obtain ahead of the problem. Our guide on how to catch deployment issues can assist you prevent many of these problems before they ever happen. Unknown This indicates that the node where your pod should be running has lost contact with Kubernetes. Node failures, network problems, or other infrastructure issues may be the cause of this. It‚Äôs actually hard to tell what‚Äôs going on with your pod when you observe this state. üîç Don‚Äôt Just React, Proactively Monitor Pods Instead of restarting pods to fix issues, utilize real-time monitoring to spot the root cause. Our dashboard provides comprehensive metrics and alerts to prevent issues before they happen. obtain a Demo begin Free Trial How to Restart Pods in Kubernetes using kubectl When you search for how to restart a Kubernetes pod using kubectl , the first thing that comes to mind is the command: kubectl restart pod However, that command does not exist. Instead, there are several reliable methods to restart Kubernetes pods using kubectl. Below are the most effective and commonly used approaches: 1. Restart Pods Using Kubectl Rollout Restart This is the safest and most recommended method for restarting pods managed by a deployment. It performs a controlled restart without downtime by creating new pods and removing old ones. For a deeper dive into managing application lifecycles and automated updates, check out our article on Kubernetes Operators. ü§ñ Commands to utilize: kubectl rollout restart deployment/my-app This command replaces existing pods with new ones. It will remove the old pods after starting and waiting for the new ones. This approach keeps your app up during the restart. To restart pods in a deployment within a specific namespace: kubectl rollout restart deployment/my-app -n your-namespace To check the status of your restart kubectl rollout status deployment/my-app Consider this strategy if you desire minimal downtime, the safest alternative, or have deployment-managed pods. Delete Individual Pods to Force Restart With this method, you must delete pods to force Kubernetes to recreate them. It‚Äôs simpler than rollout restart , but you must watch which pods you remove. If the pod is managed by a deployment, replica set, or equivalent controller, Kubernetes immediately creates a new one when you delete the existing one. However, this may temporarily disrupt service. Here‚Äôs how to go about it: # List all pods to observe what you're working with kubectl obtain pods # To delete a specific pod kubectl delete pod <pod-name> # To delete multiple pods at once kubectl delete pod <pod-1> <pod-2> # To delete and wait to remove fully kubectl delete pod <pod-name> --wait=true # To force delete a stuck pod (utilize with caution) kubectl delete pod <pod-name> --grace-period=0 --force Delete only controller-managed pods. A standalone pod that isn‚Äôt managed by anything will never be restored if it is deleted. Scale Deployment Replicas to Restart Pods This strategy works by scaling your deployment down to zero replicas for a short time, which stops all the pods. Then it scales back up to the number you started with. Kubernetes lets you turn your program off and back again in a controlled way. Check how many replicas you currently have kubectl obtain deployment my-app 2. Scale down to zero kubectl scale deployment my-app --replicas=0 3. Lastly, scale back up to your original number (creates new pods) kubectl scale deployment my-app --replicas=3 When you scale down to zero, Kubernetes deletes all the pods in that deployment. When you scale back up, it creates new pods from scratch. This approach is more aggressive than rollout restart, but sometimes necessary when you require a complete fresh begin. Update Environment Variables to Trigger a Restart This is yet another clever method for pod restarts. You will require to modify their configuration slightly. Kubernetes interprets changing environment variables in a deployment as a configuration change and restarts the pods automatically to implement the updated configuration. The key here is that you don‚Äôt even have to alter your environment variables significantly. To initiate the restart, update a timestamp or add a dummy variable. For instance: You can update an existing environment variable kubectl set env deployment/my-app RESTART_TRIGGER=$(date +%s) or You can also edit the deployment directly: kubectl edit deployment my-app Then add or modify any environment variable in the editor. The benefit of using this approach is that it follows the same safe strategy as the rollout restart , and there‚Äôs no downtime during the restart process. Replacing Pods Manually Using the same configuration or an updated version, this method requires deleting particular pods and then manually creating new ones to replace them. You have total control over the creation and deletion processes with this method. obtain the pod configuration and save it kubectl obtain pod -o yaml > pod-backup. Delete the existing pod kubectl delete pod 3. Create a new pod using the saved configuration kubectl apply -f pod-backup. yaml This method causes downtime because the old pod is removed before the new one starts. This method is only used with standalone pods; don‚Äôt do this with pods managed by deployments. Monitoring Pods with Middleware Monitoring Kubernetes pods keeps apps healthy. Without monitoring, you may overlook memory leaks , high CPU consumption, or network difficulties that could kill pods. The best way to understand when and how to perform a kubectl restart pod is with continuous monitoring. Our guide to Kubernetes Monitoring Tools can assist you choose the right solution for your needs. Middleware provides a simple and user-friendly platform that allows you to monitor pod metrics. Once you install and set up Middleware in your cluster, you‚Äôll be able to track pods like: CPU and memory usage Pod restart counts and reasons Network traffic and errors Container health status Resource limits and requests Middleware helps you create decisions by displaying the issue rather than relying on your intuition about when to restart pods. You shall be able to observe whether a pod is throwing errors, not responding to requests, or using excessive amounts of memory. How to Set Up Middleware for Kubernetes Pod Monitoring The installation involves setting up Middleware Agent inside your Kubernetes cluster to collect data and display it on a centralized dashboard for you to observe and analyze. This setup process is simplified in the official documentation , so you can check it out. Proper monitoring represents cornerstone of Kubernetes Observability , which encompasses more than just metrics; it also includes logs and traces. Getting started with Middleware is straightforward, and in this guide, I‚Äôm going to simplify it into two simple steps: Step 1. Install the Middleware Agent This may be done in three different ways. You can install the Middleware Agent using either helm, bash, or. Install Via Helm: helm repo add middleware-labs https://helm. io helm install mw-agent middleware-labs/mw-kube-agent-v3 --set mw. apiKey=<MW_API_KEY> --set mw. target=https://<MW_UID>. io:443 --set clusterMetadata. name=<your-cluster-name> -n mw-agent-ns --create-namespace 2. Install via Bash MW_API_KEY="" MW_TARGET=https://. io:443 bash -c "$(curl -L https://install. io/scripts/mw-kube-agent-install-v3. bat (Windows) set MW_API_KEY="<MW_API_KEY>" set MW_TARGET=https://<MW_UID>. io:443 Mw-kube-agent-install-windows. bat Replace <your-cluster-name> and <MW_API_KEY> with the right information. You can obtain these from your dashboard. For the cluster name, you‚Äôll have to set it up here: For the API KEY, you can obtain it from ‚Äú your profile > API Key ‚Äù Next, verify the installation by running the command: kubectl obtain daemonset/mw-kube-agent -n mw-agent-ns kubectl obtain deployment/mw-kube-agent -n mw-agent-ns 2. Access the dashboard Once installed, Middleware automatically collects data from your Kubernetes cluster and presents it in an easy-to-utilize dashboard. From there, you can: Monitor CPU and memory usage per pod Track pod restart counts and reasons View network traffic and error rates Analyze container health and resource limits Tracking pod restarts is crucial for understanding application health. For a more detailed look at the kubectl logs tail command and what to look for, observe our guide on Kubernetes Logging. Additionally, set up custom alerts to obtain instant notifications when a pod becomes unhealthy or consumes excessive resources, helping you act before issues impact your applications. Conclusion To wrap it up, restarting Kubernetes pods can assist you in so many ways, as discussed in this article, one of which is maintaining healthy applications. It doesn‚Äôt matter the method you choose to utilize; the key thing is to comprehend when and why you desire to restart. Monitoring pods also helps you in making the right restart decision. Instead of guessing what‚Äôs wrong, which you might be wrong about, proper observability shows you exactly when pods require attention. begin monitoring your pods with Middleware and catch issues before they become problems. üì¶Deploy in Minutes, Gain Insights Instantly Middleware agent installs in minutes, giving you instant visibility into your pod health, resource usage, and application logs. Install Agent Now obtain Started with Free Monitoring FAQs How can I restart a single pod? utilize kubectl delete pod <pod-name> to remove the pod. If it represents component of a deployment, Kubernetes will automatically generate a new one. Which Method of Testing Pods in a Deployment is the Safest? Restart the deployment of my-app using kubectl rollout. This eliminates downtime by restarting pods one at a time. What happens if I delete a Pod? If it is managed by a deployment, Kubernetes creates a new pod, but if it‚Äôs standalone, it‚Äôll be gone forever. When should I avoid restarting Pods manually? Don‚Äôt restart during high traffic, in production without monitoring, or when you haven‚Äôt fixed the root cause of the problem first. Related Posts Knowledge How to Monitor Mobile App Performance with Synthetic Testing Archish Thakkar 17 Dec 2025 Knowledge What is AWS Step Functions? How It Works & utilize Cases Neel Shah 10 Dec 2025 Knowledge Proactive Alerts for Slow APIs Kaushal Madani 2 Dec 2025 Optimize More, Worry Less With Middleware obtain Started View Pricing




Restart Kubernetes Pods with kubectl Sudha Bulusu Published: 1/31/2024 Kubernetes About Terminus A Kubernetes pod serves as an isolated environment for running a single process, representing the smallest operational unit within the Kubernetes ecosystem. Unlike other resources, such as Deployments, which are replaced during updates, pods have a more persistent nature. Consequently, Kubernetes does not provide a dedicated kubectl restart pod command. Instead, various methods, both automatic and manual, may be employed to restart pods effectively. The short answer To restart a Kubernetes pod, you can utilize the kubectl delete pod command as follows: $ kubectl delete pod <pod_name> Run in Warp Where: pod_name is the name of the pod to delete Kubernetes will delete the pod and automatically recreate a new one with the same configuration. Easily retrieve this command using Warp‚Äôs AI Command Suggestions If you‚Äôre using Warp as your terminal, you can easily retrieve this command using the Warp AI Command Suggestions feature : Entering kubectl delete pod in the AI Command Suggestions will prompt a kubectl command that can then quickly be inserted into your shell by doing CMD+ENTER. ‚Äç Deleting multiple pods To delete all the pods that have failed across all namespaces, you can utilize the following command: $ kubectl delete pods --field-selector status. phase=Failed -A Run in Warp Where: The --field-selector flag with status. phase=Failed specifies failed pods. The -A flag (short for --all-namespaces) specifies all the namespaces in the cluster. Deleting pods in a namespace To delete all the failed pods in a particular namespace, you can utilize the following command instead: $ kubectl delete pods --field-selector status. phase=Failed --namespace=<namespace> Run in Warp Where: namespace is the name of the namespace from which to delete the failed pods. Deleting pods by label If you‚Äôve used labels to organize your pods, you can delete pods with a specific label using the -l flag as follows: $ kubectl delete pods -l <label>=<value> Run in Warp For example: $ kubectl delete pods -l environment=test Run in Warp The above command will delete all the pods with the environment label set to test. Defining a restart policy Pod specifications include a restartPolicy field used to define the restart policy associated with a pod. The available values are Always, OnFailure, and Never. The Always restart policy Always is the default restart policy. It ensures that Kubernetes will automatically restart a container within the pod whenever it terminates, whether it exits successfully or fails. As in the example below, a web server represents common utilize case for a restart policy of Always. apiVersion: v1 kind: Pod metadata: name: nginx-webserver-pod spec: restartPolicy: Always containers: - name: my-container image: nginx:latest Run in Warp The OnFailure restart policy The OnFailure restart policy is used for pods whose containers execute tasks on a schedule or only once. This ensures that the containers in the pod are replaced automatically in the event of a failure and prevents them from restarting unnecessarily, thus consuming resources. The example below of a container that copies a file from one directory to another represents utilize case for setting the restart policy to OnFailure. apiVersion: v1 kind: Pod metadata: name: busybox-file-task spec: restartPolicy: OnFailure containers: - name: busybox-container image: busybox:latest command: ["cp", "/path/to/source/file", "/path/to/destination/file"] Run in Warp Restarting pods using rolling updates Aside from choosing the appropriate restart policy, there are many scenarios where it becomes necessary to restart a pod manually. The most recommended method for a manual restart is the rolling restart. A rollout represents process used to ensure that updates are rolled out with minimal disruption of the availability of the application by terminating pods gracefully and recreating them one at a time. This method is available in Kubernetes v1. Performing the rolling restart of a Deployment A Deployment controller may be defined to declare the desired application state, including details such as the container image, number of replicas, and update strategy. A common utilize case is managing the deployment and scaling of a web application. If a change is made to the Deployment manifest, all the pods in the Deployment may be restarted with the command below: $ kubectl rollout restart deployment/<deployment_name> Run in Warp Performing a rolling restart using namespaces Namespaces are logical, isolated environments within a Kubernetes cluster that allow you to group and manage Kubernetes resources separately. To restart all the Deployments located in a specified namespace, you can utilize the following kubectl rollout restart deployment command with the -n flag as follows: $ kubectl rollout restart deployment/<deployment_name>-n<namespace> Run in Warp Performing a rolling restart using labels Labels are key-value pairs attached to Kubernetes objects used for organizing resources based on specific criteria. To restart all the pods based on a specific label, you can utilize the kubectl rollout restart pod command with the -l flag as follows: $ kubectl rollout restart pod -l <label>=<value> Run in Warp For example: $ kubectl rollout restart pod -l environment=development Run in Warp The above command will restart all the pods identified by the environment=development label. Scaling down deployment replicas Scaling pods down is another method to restart pods in a controlled manner with graceful termination. To restart a pod by scaling deployment replicas to 0, you can utilize the following kubectl scale deployment command: $ kubectl scale --replicas=0 deployment <deployment_name> Run in Warp Which will gracefully terminate all the pods in the specified deployment. You can then recreate them by setting the number of replicas to 1 or more as follows: $ kubectl scale --replicas=1 deployment <deployment_name> Run in Warp Scaling down Deployments in a namespace To restart the pods in a particular namespace, you can utilize the kubectl scale deployment with the -n flag as follows: $ kubectl scale --replicas=0 deployment <deployment_name> -n <namespace> $ kubectl scale --replicas=1 deployment <deployment_name> -n <namespace> Run in Warp Scaling down Deployment using labels To restart pods with a specific label, you can utilize the kubectl scale command with the --selector flag as follows: $ kubectl scale --replicas=0 --selector=<label>=<value> deployment $ kubectl scale --replicas=1 --selector=<label>=<value> deployment Run in Warp Where: label is the name of the label selector. value is the value associated with this label. For example: $ kubectl scale --replicas=0 --selector=app=my-app deployment $ kubectl scale --replicas=1 --selector=app=my-app deployment Run in Warp Updating environment variables Updating the environment variables of a pod, or container in a pod, will gracefully terminate the pod, change the specified environment variables, and recreate the pod. Updating the environment variables of a pod To update the environment variables of a pod, you can utilize the kubectl set env command as follows: $ kubectl set env pod <pod_name> -n<namespace> <env_name> =<env_value> Run in Warp Where: pod_name is the name of the pod you desire to update. namespace is the name of the namespace the pod runs in. env_name is the name of the environment variable you desire to add or update. env_value is the value of the env\_name variable. For example: $ kubectl set env pod my-pod -n my-namespace USER_NAME=admin USER_PWD=admin Run in Warp Updating the environment variables of a container To update the environment variables of a specific container in a pod, you can utilize the kubectl set env command as follows: $ kubectl set env pod <pod_name> -n <namespace> -c <container_name> <env_name>=<env_value> Run in Warp Where: pod_name is the name of the pod you desire to update. namespace is the name of the namespace the pod runs in. container_name is the name of the container in the pod. env_name is the name of the environment variable you desire to add or update. env_value is the value of the env_name variable. For example: $ kubectl set env pod my-pod -n my-namespace -c database DB_NAME=admin DB_PWD=admin Run in Warp Written by Sudha Bulusu Filed Under Kubernetes Related Articles Copy Files From Pod in Kubernetes Learn how to copy files and directories from within a Kubernetes Pod into the local filesystem using the kubectl command. Kubernetes Scale Deployments in Kubernetes Learn how to manually and automatically scale a Deployment based on CPU usage in Kubernetes using the kubectl-scale and kubectl-autoscale commands. Kubernetes obtain Kubernetes Logs With kubectl Learn how to obtain the logs of pods, containers, deployments, and services in Kubernetes using the kubectl command. Troubleshoot a cluster stuck in CrashloopBackoff, ImagePullBackoff, or Pending error states. Kubernetes Forward Ports In Kubernetes Learn how to forward the ports of Kubernetes resources such as Pods and Services using the kubectl port-forward command. Kubernetes Tail Logs In Kubernetes Learn how to tail and monitor Kubernetes logs efficiently to debug, trace, and troubleshoot errors more easily using the kubectl command. Kubernetes obtain Context In Kubernetes Learn how to obtain information about one or more contexts in Kubernetes using the kubectl command. Kubernetes Delete Kubernetes Namespaces With kubectl Learn how to delete one or more namespaces and their related resources in a Kubernetes cluster using the kubectl command. Kubernetes obtain Kubernetes Secrets With kubectl Learn how to list, describe, customize, sort and filter secrets in a Kubernetes cluster by name, type, namespace, label and more using the kubectl command. Kubernetes List Kubernetes Namespaces With kubectl Learn how to list, describe, customize, sort and filter namespaces in a Kubernetes cluster by name, label, and more using the kubectl command. Kubernetes How To List Events With kubectl Learn how to list and filter events in Kubernetes cluster by namespace, pod name and more using the kubectl command. Kubernetes Kubernetes vs Docker: The Backbone of Modern Backend Technologies Lean the fundamentals of the Kubernetes and Docker technologies and how they interplay with each other. Kubernetes Docker Set Context With kubectl Learn how to create, modify, switch, and delete a context in Kubernetes using the kubectl config command

This information is tailored for the alertmend.io platform, providing comprehensive insights and solutions.
