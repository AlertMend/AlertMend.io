---
title: "exit code 1 kubernetes"
excerpt: "REFERENCE CONTENT FROM TOP 8 GOOGLE SEARCH RESULTS This content is gathered from the top-ranking pages for comprehensive reference"
date: "2025-12-18"
category: "Kubernetes"
author: "AlertMend Team"
keywords: "exit, code, kubernetes, Kubernetes, AlertMend AI, AIOps, container orchestration, DevOps"
---

# exit code 1 kubernetes

REFERENCE CONTENT FROM TOP 8 GOOGLE SEARCH RESULTS

This content is gathered from the top-ranking pages for comprehensive reference.
Sources:
  1. https://alertmend.io.com/learn/exit-codes-in-containers-and-kubernetes-the-complete-guide/
  2. https://github.com/nuclio/nuclio/issues/2029
  3. https://stackoverflow.com/questions/74713374/for-kubernetes-pods-how-to-discover-cause-of-exit-code-2
  4. https://www.linkedin.com/posts/donald-lutz-5a9b0b2_exit-codes-in-containers-kubernetes-complete-activity-6872604824941199360-TIMc
  5. https://qiita.com/naomichi-y/items/b18eb09d016a6bf33568
  6. https://hackernoon.com/top-5-kubernetes-coding-errors-and-how-to-solve-them
  7. https://mickael-baron.fr/blog/2024/07/19/guide-deploying-nvidiagpu-k8s
  8. https://cloud.tencent.com/developer/article/1644610

The following sections contain content from each source, organized for reference.
utilize this information to comprehend the topic comprehensively, identify key points,
related keywords, and best practices. Then create original, SEO-optimized content
that synthesizes insights from all sources while using completely original wording.






Home Learning Center Exit Codes in Docker and Kubernetes: The Complete Guide Exit Codes in Docker and Kubernetes: The Complete Guide Nir Shtein, Software Engineer 14 min read November 2nd, 2021 Kubernetes Troubleshooting What are Container Exit Codes Exit codes are used by container engines, when a container terminates, to report why it was terminated. If you represent Kubernetes user, container failures are one of the most common causes of pod exceptions, and understanding container exit codes can assist you obtain to the root cause of pod failures when troubleshooting. The most common exit codes used by containers are: Code # Name What it means Exit Code 0 Purposely stopped Used by developers to indicate that the container was automatically stopped Exit Code 1 Application error Container was stopped due to application error or incorrect reference in the image specification Exit Code 125 Container failed to run error The docker run command did not execute successfully Exit Code 126 Command invoke error A command specified in the image specification could not be invoked Exit Code 127 File or directory not found File or directory specified in the image specification was not found Exit Code 128 Invalid argument used on exit Exit was triggered with an invalid exit code (valid codes are integers between 0-255) Exit Code 134 Abnormal termination (SIGABRT) The container aborted itself using the abort() function. Exit Code 137 Immediate termination ( SIGKILL ) Container was immediately terminated by the operating system via SIGKILL signal Exit Code 139 Segmentation fault ( SIGSEGV ) Container attempted to access memory that was not assigned to it and was terminated Exit Code 143 Graceful termination ( SIGTERM ) Container received warning that it was about to be terminated, then terminated Exit Code 255 Exit Status Out Of Range Container exited, returning an exit code outside the acceptable range, meaning the cause of the error is not known Below weâ€™ll elaborate on how to troubleshoot failed containers on a self-managed host and in Kubernetes, and provide more details on all of the exit codes listed above. This is part of an extensive series of guides about Observability. The Container Lifecycle To better comprehend the causes of container failure, letâ€™s discuss the lifecycle of a container first. Taking Docker as an example â€“ at any given time, a Docker container may be in one of several states: Created â€“ the Docker container is created but not started yet (this is the status after running docker create, but before actually running the container) Up â€“ the Docker container is currently running. This means the operating system process managed by the container is running. This happens when you utilize the commands docker begin or docker run can happen using docker begin or docker run. Paused â€“ the container process was running, but Docker purposely paused the container. Typically this happens when you run the Docker pause command Exited â€“ the Docker container was terminated, usually because the containerâ€™s process was killed When a container reaches the Exited status, Docker will report an exit code in the logs, to inform you what happened to the container that caused it to shut down. Tips from the expert Itiel Shwartz Co-Founder & CTO Itiel is the CTO and co-founder of alertmend.io. Heâ€™s a big believer in dev empowerment and moving fast, has worked at eBay, Forter and Rookout (as the founding engineer). Itiel represents backend and infra developer turned â€œDevOpsâ€, an avid public speaker that loves talking about things such as cloud infrastructure, Kubernetes, Python, observability, and R&D culture. In my experience, here are tips that can assist you better handle exit codes in containers and Kubernetes: Monitor resource limits Set appropriate resource limits to prevent OOMKilled (exit code 137) and other resource-related issues. utilize liveness and readiness probes Implement these probes to detect and respond to container failures automatically. Leverage container restart policies Configure restart policies to handle transient issues without manual intervention. Enable detailed logging Collect and analyze logs for all containers to quickly identify the root cause of failures. utilize health checks Implement health checks in your containerized applications to detect and resolve issues proactively. Understanding Docker Container Exit Codes Below we cover each of the exit codes in more detail. Exit Code 0: Purposely Stopped Exit Code 0 is triggered by developers when they purposely cease their container after a task completes. Technically, Exit Code 0 means that the foreground process is not attached to a specific container. What to do if a container terminated with Exit Code 0? Check the container logs to identify which library caused the container to exit Review the code of the existing library and identify why it triggered Exit Code 0, and whether it is functioning correctly Exit Code 1: Application Error Exit Code 1 indicates that the container was stopped due to one of the following: An application error â€“ this could be a simple programming error in code run by the container, such as â€œdivide by zeroâ€, or advanced errors related to the runtime environment, such as Java, Python, etc An invalid reference â€“ this means the image specification refers to a file that does not exist in the container image What to do if a container terminated with Exit Code 1? Check the container log to observe if one of the files listed in the image specification could not be found. If this is the issue, correct the image specification to point to the correct path and filename. If you cannot discover an incorrect file reference, check the container logs for an application error, and debug the library that caused the error. Exit Code 125: Container Failed to Run Exit Code 125 means that the command is used to run the container. For example docker run was invoked in the system shell but did not execute successfully. Here are common reasons this might happen: An undefined flag was used in the command, for example docker run --abcd The user-defined in the image specification does not have sufficient permissions on the machine Incompatibility between the container engine and the host operating system or hardware What to do if a container terminated with Exit Code 125? Check if the command used to run the container uses the proper syntax Check if the user running the container, or the context in which the command is executed in the image specification, has sufficient permissions to create containers on the host If your container engine provides other options for running a container, attempt them. For example, in Docker, attempt docker begin instead of docker run Test if you are able to run other containers on the host using the same username or context. If not, reinstall the container engine, or resolve the underlying compatibility issue between the container engine and the host setup Exit Code 126: Command Invoke Error Exit Code 126 means that a command used in the container specification could not be invoked. This is often the cause of a missing dependency or an error in a continuous integration script used to run the container. What to do if a container terminated with Exit Code 126? Check the container logs to observe which command could not be invoked attempt running the container specification without the command to ensure you isolate the problem Troubleshoot the command to ensure you are using the correct syntax and all dependencies are available Correct the container specification and retry running the container Exit Code 127: File or Directory Not Found Exit Code 127 means a command specified in the container specification refers to a non-existent file or directory. What to do if a container terminated with Exit Code 127? Same as Exit Code 126, identify the failing command and create sure you reference a valid filename and file path available within the container image. Learn more in our detailed guide to exit code 127. Exit Code 128: Invalid Argument Used on Exit Exit Code 128 means that code within the container triggered an exit command, but did not provide a valid exit code. The Linux exit command only allows integers between 0-255, so if the process was exited with, for example, exit code 3. 5 , the logs will report Exit Code 128. What to do if a container terminated with Exit Code 128? Check the container logs to identify which library caused the container to exit. Identify where the offending library uses the exit command, and correct it to provide a valid exit code. Exit Code 134: Abnormal Termination (SIGABRT) Exit Code 134 means that the container abnormally terminated itself, closed the process and flushed open streams. This operation is irreversible, like SIGKILL (observe Exit Code 137 below). A process can trigger SIGABRT by doing one of the following: Calling the abort() function in the libc library Calling the assert() macro, used for debugging. The process is then aborted if the assertion is false. What to do if a container terminated with Exit Code 134? Check container logs to observe which library triggered the SIGABRT signal Check if process abortion was planned (for example because the library was in debug mode), and if not, troubleshoot the library and modify it to avoid aborting the container. Exit Code 137: Immediate Termination (SIGKILL) Exit Code 137 means that the container has received a SIGKILL signal from the host operating system. This signal instructs a process to terminate immediately, with no grace period. This may be either: Triggered when a container is killed via the container engine, for example when using the docker kill command Triggered by a Linux user sending a kill -9 command to the process Triggered by Kubernetes after attempting to terminate a container and waiting for a grace period of 30 seconds (by default) Triggered automatically by the host, usually due to running out of memory. In this case, the docker inspect command will indicate an OOMKilled error. Learn more in our detailed guide to exit code 137. What to do if a container terminated with Exit Code 137? Check logs on the host to observe what happened prior to the container terminating, and whether it previously received a SIGTERM signal (graceful termination) before receiving SIGKILL If there was a prior SIGTERM signal, check if your container process handles SIGTERM and is able to gracefully terminate If there was no SIGTERM and the container reported an OOMKilled error, troubleshoot memory issues on the host Learn more in our detailed guide to the SIGKILL signal >> Exit Code 139: Segmentation Fault (SIGSEGV) Exit Code 139 means that the container received a SIGSEGV signal from the operating system. This indicates a segmentation error â€“ a memory violation, caused by a container trying to access a memory location to which it does not have access. There are three common causes of SIGSEGV errors: Coding error â€”container process did not initialize properly, or it tried to access memory through a pointer to previously freed memory Incompatibility between binaries and libraries â€”container process runs a binary file that is not compatible with a shared library, and thus may attempt to access inappropriate memory addresses Hardware incompatibility or misconfiguration â€”if you observe multiple segmentation errors across multiple libraries, there may be a problem with memory subsystems on the host or a system configuration issue What to do if a container terminated with Exit Code 139? Check if the container process handles SIGSEGV. On both Linux and Windows, you can handle a containerâ€™s response to segmentation violations. For example, the container can collect and report a stack trace If you require to further troubleshoot SIGSEGV, you may require to set the operating system to allow programs to run even after a segmentation fault occurs, to allow for investigation and debugging. Then, attempt to intentionally cause a segmentation violation and debug the library causing the issue If you cannot replicate the issue, check memory subsystems on the host and troubleshoot memory configuration Learn more in our detailed guide to the SIGSEGV signal >> Exit Code 143: Graceful Termination (SIGTERM) Exit Code 143 means that the container received a SIGTERM signal from the operating system, which asks the container to gracefully terminate, and the container succeeded in gracefully terminating (otherwise you will observe Exit Code 137). This exit code may be: Triggered by the container engine stopping the container, for example when using the docker cease or docker-compose down commands Triggered by Kubernetes setting a pod to Terminating status, and giving containers a 30 second period to gracefully shut down What to do if a container terminated with Exit Code 143? Check host logs to observe the context in which the operating system sent the SIGTERM signal. If you are using Kubernetes, check the kubelet logs to observe if and when the pod was shut down. In general, Exit Code 143 does not require troubleshooting. It means the container was properly shut down after being instructed to do so by the host. Learn more in our detailed guide to the SIGTERM signal >> Exit Code 1: Application Error Exit Code 1 indicates that the container was stopped due to one of the following: An application error â€“ this could be a simple programming error in code run by the container, such as â€œdivide by zeroâ€, or advanced errors related to the runtime environment, such as Java, Python, etc An invalid reference â€“ this means the image specification refers to a file that does not exist in the container image What to do if a container terminated with Exit Code 1? Check the container log to observe if one of the files listed in the image specification could not be found. If this is the issue, correct the image specification to point to the correct path and filename. If you cannot discover an incorrect file reference, check the container logs for an application error, and debug the library that caused the error. Exit Code 125 Exit Code 125: Container Failed to Run Exit Code 125 means that the command is used to run the container. For example docker run was invoked in the system shell but did not execute successfully. Here are common reasons this might happen: An undefined flag was used in the command, for example docker run --abcd The user-defined in the image specification does not have sufficient permissions on the machine Incompatibility between the container engine and the host operating system or hardware What to do if a container terminated with Exit Code 125? Check if the command used to run the container uses the proper syntax Check if the user running the container, or the context in which the command is executed in the image specification, has sufficient permissions to create containers on the host If your container engine provides other options for running a container, attempt them. For example, in Docker, attempt docker begin instead of docker run Test if you are able to run other containers on the host using the same username or context. If not, reinstall the container engine, or resolve the underlying compatibility issue between the container engine and the host setup Exit Code 126: Command Invoke Error Exit Code 126 means that a command used in the container specification could not be invoked. This is often the cause of a missing dependency or an error in a continuous integration script used to run the container. What to do if a container terminated with Exit Code 126? Check the container logs to observe which command could not be invoked attempt running the container specification without the command to ensure you isolate the problem Troubleshoot the command to ensure you are using the correct syntax and all dependencies are available Correct the container specification and retry running the container Exit Code 127: File or Directory Not Found Exit Code 127 means a command specified in the container specification refers to a non-existent file or directory. What to do if a container terminated with Exit Code 127? Same as Exit Code 126 above, identify the failing command and create sure you reference a valid filename and file path available within the container image. Exit Code 128: Invalid Argument Used on Exit Exit Code 128 means that code within the container triggered an exit command, but did not provide a valid exit code. The Linux exit command only allows integers between 0-255, so if the process was exited with, for example, exit code 3. 5 , the logs will report Exit Code 128. What to do if a container terminated with Exit Code 128? Check the container logs to identify which library caused the container to exit. Identify where the offending library uses the exit command, and correct it to provide a valid exit code. Exit Code 134: Abnormal Termination (SIGABRT) Exit Code 134 means that the container abnormally terminated itself, closed the process and flushed open streams. This operation is irreversible, like SIGKILL (observe Exit Code 137 below). A process can trigger SIGABRT by doing one of the following: Calling the abort() function in the libc library Calling the assert() macro, used for debugging. The process is then aborted if the assertion is false. What to do if a container terminated with Exit Code 134? Check container logs to observe which library triggered the SIGABRT signal Check if process abortion was planned (for example because the library was in debug mode), and if not, troubleshoot the library and modify it to avoid aborting the container. Exit Code 137: Immediate Termination (SIGKILL) Exit Code 137 means that the container has received a SIGKILL signal from the host operating system. This signal instructs a process to terminate immediately, with no grace period. This may be either: Triggered when a container is killed via the container engine, for example when using the docker kill command Triggered by a Linux user sending a kill -9 command to the process Triggered by Kubernetes after attempting to terminate a container and waiting for a grace period of 30 seconds (by default) Triggered automatically by the host, usually due to running out of memory. In this case, the docker inspect command will indicate an OOMKilled error. What to do if a container terminated with Exit Code 137? Check logs on the host to observe what happened prior to the container terminating, and whether it previously received a SIGTERM signal (graceful termination) before receiving SIGKILL If there was a prior SIGTERM signal, check if your container process handles SIGTERM and is able to gracefully terminate If there was no SIGTERM and the container reported an OOMKilled error, troubleshoot memory issues on the host Learn more in our detailed guide to the SIGKILL signal >> Exit Code 139: Segmentation Fault (SIGSEGV) Exit Code 139 means that the container received a SIGSEGV signal from the operating system. This indicates a segmentation error â€“ a memory violation, caused by a container trying to access a memory location to which it does not have access. There are three common causes of SIGSEGV errors: Coding error â€”container process did not initialize properly, or it tried to access memory through a pointer to previously freed memory Incompatibility between binaries and libraries â€”container process runs a binary file that is not compatible with a shared library, and thus may attempt to access inappropriate memory addresses Hardware incompatibility or misconfiguration â€”if you observe multiple segmentation errors across multiple libraries, there may be a problem with memory subsystems on the host or a system configuration issue What to do if a container terminated with Exit Code 139? Check if the container process handles SIGSEGV. On both Linux and Windows, you can handle a containerâ€™s response to segmentation violations. For example, the container can collect and report a stack trace If you require to further troubleshoot SIGSEGV, you may require to set the operating system to allow programs to run even after a segmentation fault occurs, to allow for investigation and debugging. Then, attempt to intentionally cause a segmentation violation and debug the library causing the issue If you cannot replicate the issue, check memory subsystems on the host and troubleshoot memory configuration Learn more in our detailed guide to the SIGSEGV signal >> Exit Code 143: Graceful Termination (SIGTERM) Exit Code 143 means that the container received a SIGTERM signal from the operating system, which asks the container to gracefully terminate, and the container succeeded in gracefully terminating (otherwise you will observe Exit Code 137). This exit code may be: Triggered by the container engine stopping the container, for example when using the docker cease or docker-compose down commands Triggered by Kubernetes setting a pod to Terminating status, and giving containers a 30 second period to gracefully shut down What to do if a container terminated with Exit Code 143? Check host logs to observe the context in which the operating system sent the SIGTERM signal. If you are using Kubernetes, check the kubelet logs to observe if and when the pod was shut down. In general, Exit Code 143 does not require troubleshooting. It means the container was properly shut down after being instructed to do so by the host. Learn more in our detailed guide to the SIGTERM signal >> Exit Code 255: Exit Status Out Of Range When you observe exit code 255, it implies the main entrypoint of a container stopped with that status. It means that the container stopped, but it is not known for what reason. What to do if a container terminated with Exit Code 255? If the container is running in a virtual machine, first attempt removing overlay networks configured on the virtual machine and recreating them. If this does not solve the problem, attempt deleting and recreating the virtual machine, then rerunning the container on it. Failing the above, bash into the container and examine logs or other clues about the entrypoint process and why it is failing. Which Kubernetes Errors are Related to Container Exit Codes? Whenever containers fail within a pod, or Kubernetes instructs a pod to terminate for any reason, containers will shut down with exit codes. Identifying the exit code can assist you comprehend the underlying cause of a pod exception. You can utilize the following command to view pod errors: kubectl describe pod [name] The result will look something like this: Containers: kubedns: Container ID:. State: Running Started: Fri, 15 Oct 2021 12:06:01 +0800 Last State: Terminated Reason: Error Exit Code: 255 Started: Fri, 15 Oct 2021 11:43:42 +0800 Finished: Fri, 15 Oct 2021 12:05:17 +0800 Ready: True Restart Count: 1 utilize the Exit Code provided by kubectl to troubleshoot the issue: If the Exit Code is 0 â€“ the container exited normally, no troubleshooting is required If the Exit Code is between1-128 â€“ the container terminated due to an internal error, such as a missing or invalid command in the image specification If the Exit Code is between 129-255 â€“ the container was stopped as the result of an operating signal, such as SIGKILL or SIGINT If the Exit Code was exit(-1) or another value outside the 0-255 range, kubectl translates it to a value within the 0-255 range. Refer to the relevant section above to observe how to troubleshoot the container for each exit code. Troubleshooting Kubernetes Pod Termination with alertmend.io As a Kubernetes administrator or user, pods or containers terminating unexpectedly may be a pain and can result in severe production issues. The troubleshooting process in Kubernetes is complex and, without the right tools, may be stressful, ineffective, and time-consuming. Some best practices can assist minimize the chances of container failure affecting your applications, but eventually, something will go wrongâ€”simply because it can. This is the reason why we created alertmend.io, a tool that helps dev and ops teams cease wasting their precious time looking for needles in (hay)stacks every time things go wrong. Acting as a single source of truth (SSOT) for all of your k8s troubleshooting needs, alertmend.io offers: Change intelligence: Every issue represents result of a change. Within seconds we can assist you comprehend exactly who did what and when. In-depth visibility: A complete activity timeline, showing all code and config changes, deployments, alerts, code diffs, pod logs and etc. All within one pane of glass with easy drill-down options. Insights into service dependencies: An easy way to comprehend cross-service changes and visualize their ripple effects across your entire system. Seamless notifications: Direct integration with your existing communication channels (e. , Slack) so youâ€™ll have all the information you require, when you require it. observe Additional Guides on Key Observability Topics Together with our content partners, we have authored in-depth guides on several other topics that can also be useful as you explore the world of observability. eBPF Authored by Tigera [Guide] eBPF Explained: utilize Cases, Concepts, and Architecture [Guide] eBPF XDP: The Basics and a Quick Tutorial [Blog] Introducing the Calico eBPF Dataplane [Product] Tigera | Security and Observability for Containers and Kubernetes Cloud Security Authored by Tigera [Guide] Cloud Security: Challenges and 5 Technologies That Can assist [Guide] Micro-segmentation in the Cloud Native World [eBook] Oâ€™Reilly eBook: Kubernetes Security and Observability [Product] Tigera | Security and Observability for Containers and Kubernetes Git Errors Authored by alertmend.io [Guide] Common Git Errors, How to Fix, and 5 Ways to Avoid Them [Guide] Git Error: â€˜failed to push some refs toâ€™: Steps to Fix [Blog] Kubernetes health checks [Product] alertmend.io | Kubernetes Management and Troubleshooting Share: Latest Articles Kubernetes Certificates: A Practical Guide K8sGPT: Improving K8s Cluster Management with LLMs Top 7 Kubernetes GUI Tools in 2024




nuclio / nuclio Public Notifications You must be signed in to change notification settings Fork 557 Star 5. 6k Dashboard error on Kubernetes 1. x (AKS) #2029 New issue Copy link New issue Copy link Closed Closed Dashboard error on Kubernetes 1. x (AKS) #2029 Copy link Description turowicz opened on Jan 3, 2021 Issue body actions I obtain an error after creating a namespace, a secret and deploying the helm chart according to this guide: https://github. com/nuclio/nuclio/tree/development/hack/k8s/helm/nuclio The dashboard pod logs: Running in parallel Starting dashboard Starting nginx 21. 508 ï¿½[0;37m dashboard. platformï¿½[0m ï¿½[0;32m(D)ï¿½[0m Using kubeconfig {"kubeconfigPath": ""} 21. runnerï¿½[0m ï¿½[0;32m(D)ï¿½[0m Executing {"command": "docker version"} 21. runnerï¿½[0m ï¿½[0;32m(D)ï¿½[0m Failed to execute command {"output": "Client: Docker Engine - Community\n Version: 19. 10\n Git commit: 48a66213fe\n Built: Mon Jun 22 15:42:53 2020\n OS/Arch: linux/amd64\n Experimental: false\nCannot connect to the Docker daemon at unix:///var/run/docker. Is the docker daemon running?\n", "stderr": "", "exitCode": 1, "err": "exit status 1"} Error - exit status 1 /nuclio/pkg/cmdrunner/shellrunner. go:95 Call stack: stdout: Client: Docker Engine - Community Version: 19. 10 Git commit: 48a66213fe Built: Mon Jun 22 15:42:53 2020 OS/Arch: linux/amd64 Experimental: false Cannot connect to the Docker daemon at unix:///var/run/docker. Is the docker daemon running? stderr: /nuclio/pkg/cmdrunner/shellrunner. go:95 No docker client found /nuclio/pkg/dockerclient/shell. go:65 Failed to create docker client. /pkg/containerimagebuilderpusher/docker. go:31 Failed to create a Docker builder /nuclio/pkg/platform/kube/platform. go:117 Failed to create kube platform /nuclio/pkg/platform/factory/factory. go:69 parallel: This job failed: /runners/dashboard. sh Exiting Metadata Metadata Assignees No one assigned Labels No labels No labels Type No type Projects No projects Milestone No milestone Relationships None yet Development No branches or pull requests Issue actions




4 I have pods that are of kind Cronjob running in parallel. They complete their task and run again after a fixed interval of 20 minutes as per the cron expression. I noticed that some pods are restarting 2-3 times before completing their task. I checked for details with the kubectl describe pod command and found that the pods terminate with exit code 2 when it restart due to some error: Last State: Terminated Reason: Error Exit Code: 2 I searched about exit code 2 and found that it is misuse of a shell builtin commands. How I can discover which shell builtin is misused? How to debug the cause of exit code 2? docker kubernetes Share Improve this question Follow edited Feb 11, 2024 at 14:46 Alexis Wilke 21. 2k 11 11 gold badges 111 111 silver badges 183 183 bronze badges asked Dec 7, 2022 at 7:55 anujprashar 6,359 8 8 gold badges 55 55 silver badges 92 92 bronze badges 2 1 This is incredibly dependent on the code running inside the pod. There's nothing that special about the number 2 that would point at a single definite cause. Can you edit the question to include a minimal reproducible example ? David Maze â€“ David Maze 2022-12-07 14:10:45 +00:00 Commented Dec 7, 2022 at 14:10 I found issue was with code after carefully going though code and application logs. There was null check that was causing this issue. Thanks for providing direction. anujprashar â€“ anujprashar 2022-12-08 09:03:30 +00:00 Commented Dec 8, 2022 at 9:03 Add a comment | 2 Answers 2 Sorted by: Reset to default Highest score (default) Trending (recent votes count more) Date modified (newest first) Date created (oldest first) 3 An exit code of 2 indicates either that the application chose to return that error code, or (by convention) there was a misuse of a shell built-in. Check your podâ€™s command specification to ensure that the command is correct. If you think it is correct, attempt running the image locally with a shell and run the command directly. Refer to this link for more information. Share Improve this answer Follow edited Dec 7, 2022 at 10:23 Harsh Manvar 30. 6k 8 8 gold badges 64 64 silver badges 125 125 bronze badges answered Dec 7, 2022 at 10:18 Fariya Rahmat 3,330 7 7 silver badges 17 17 bronze badges Sign up to request clarification or add additional context in comments. Comments Add a comment 0 You can obtain logs with kubectl logs my-pod Post output here if you can't fix it. Share Improve this answer Follow answered Dec 7, 2022 at 8:14 Christoph Fischer 115 10 10 bronze badges 3 Comments Add a comment anujprashar anujprashar Over a year ago Thanks for reply. I have already check logs from kubectl logs. It only demonstrate application logs and nothing unusal in logs. 113Z+00:00 0 Reply Copy link Christoph Fischer Christoph Fischer Over a year ago What's your output for kubectl obtain pods ? 2022-12-07T09:15:50. 677Z+00:00 0 Reply Copy link Christoph Fischer Christoph Fischer Over a year ago Also, did you check this one? containersolutions. io/runbooks/posts/kubernetes/â€¦ 2022-12-07T09:16:31. 49Z+00:00 0 Reply Copy link Add a comment Your Answer Draft saved Draft discarded Sign up or log in Sign up using Google Sign up using Email and Password Submit Post as a guest Name Email Required, but never shown Post Your Answer Discard By clicking â€œPost Your Answerâ€, you agree to our terms of service and acknowledge you have read our privacy policy. begin asking to obtain answers discover the answer to your question by asking. Ask question Explore related questions docker kubernetes observe similar questions with these tags




Donald Lutz 4y Report this post Exit Codes in Containers and Kubernetes: The Complete Guide #containers #kubernetes #exitcodes #completeguide https://lnkd. in/ev7_bExd 3 Like Comment Share Copy LinkedIn Facebook X To view or add a comment, sign in




info More than 5 years have passed since last update. @ naomichi-y ( Naomichi Yamakita ) Dockerç’°å¢ƒã§Railsåœæ­¢æ™‚ã«Exit code 1ãŒç™ºç”Ÿã™ã‚‹ Rails puma Docker kubernetes ECS Last updated at 2020-09-13 Posted at 2020-09-13 å•é¡Œ Dockerä¸Šã§Rails (Puma) ã‚’å®Ÿè¡Œä¸­ã« docker cease ã§ã‚³ãƒ³ãƒ†ãƒŠã‚’åœæ­¢ã•ã›ã‚ˆã†ã¨ã™ã‚‹ã¨Exit 1 (SIGHUP) ãŒç™ºç”Ÿã—ã¾ã™ã€‚ ECSã‚„Kubernetesã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹å ´åˆã€ã‚³ãƒ³ãƒ†ãƒŠãŒæ­£ã—ãçµ‚äº†ã›ãšã€äºˆæœŸã—ãªã„å•é¡Œã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ % docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES xxx rails "bundle exec rails sâ€¦" 44 seconds ago Exited (1) 3 seconds ago api xxx åŸå›  ä¿—ã«è¨€ã†ã€ŒPID 1å•é¡Œ 1 ã€ãŒåŸå› ã€‚PID 1ã¯initãƒ—ãƒ­ã‚»ã‚¹ã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã§ã€ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•æ™‚ã«ã‚«ãƒ¼ãƒãƒ«ã«ã‚ˆã£ã¦å‘¼ã³å‡ºã•ã‚Œã‚‹ç‰¹åˆ¥ãªãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚initãƒ—ãƒ­ã‚»ã‚¹ã¯ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚„å­ãƒ—ãƒ­ã‚»ã‚¹ã®ç”Ÿæˆã€ã‚¾ãƒ³ãƒ“ãƒ—ãƒ­ã‚»ã‚¹ã®å‰Šé™¤ãªã©ã‚’è¡Œã„ã¾ã™ã€‚ ä»Šå›ã¯Rails ( 2 )ã‚’èµ·å‹•ã—ãŸéš›ã«PID 1ãŒä½¿ã‚ã‚Œã¦ã—ã¾ã„ã€ã‚·ã‚°ãƒŠãƒ«ã‚’æ­£ã—ããƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã§ããªã„ã¨ã„ã£ãŸå•é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã¾ã—ãŸã€‚ # ã‚³ãƒ³ãƒ†ãƒŠä¸Šã§topã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ãŸçµæœ PID USER PR NI VIRT RES %CPU %MEM TIME+ S COMMAND 1 root 20 0 2. 02 S /bin/bash /bin/docker-entrypoint. sh bundle exec rails s -b 0. ãƒ—ãƒ­ã‚»ã‚¹ã‚’è¦‹ã‚‹ã¨ã€PID 1ã§RailsãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚ å¯¾ç­– tini ã‚„ dumb-init ã¨ã„ã£ãŸãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä½¿ã†ã“ã¨ã§ã€PID 1ã®å­ãƒ—ãƒ­ã‚»ã‚¹ã¨ã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã¨ãªã‚Šã¾ã™ã€‚ docker-compose 3. 7ä»¥é™ãŒåˆ©ç”¨å¯èƒ½ã§ã‚ã‚Œã°ã€ docker-compose. yml ã« init ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä»˜ã‘ã‚‹ã“ã¨ã§å•é¡Œã‚’å›é¿ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ ( 3 )ã€‚ # RailsãŒinitãƒ—ãƒ­ã‚»ã‚¹ã®å­ãƒ—ãƒ­ã‚»ã‚¹ã¨ã—ã¦èµ·å‹•ã™ã‚‹ PID USER PR NI VIRT RES %CPU %MEM TIME+ S COMMAND 1 root 20 0 1. 03 S /sbin/docker-init -- /bin/docker-entrypoint. sh bundle exec rails s -b 0. 00 S `- /bin/bash /bin/docker-entrypoint. sh bundle exec rails s -b 0. 0 docker-compose down å®Ÿè¡Œå¾Œã«ãƒ—ãƒ­ã‚»ã‚¹ã‚’è¦‹ã‚‹ã¨ã€143 (SIGTERM) ã§ã‚³ãƒ³ãƒ†ãƒŠãŒåœæ­¢ã—ã¦ã„ã¾ã™ã€‚ % docker-compose ps Name Command State Ports --- api /bin/docker-entrypoint. Exit 143 ECSã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹å ´åˆ ã‚¿ã‚¹ã‚¯å®šç¾©ã« initProcessEnabled: true ( 4 ) ã‚’è¿½åŠ ã—ã¾ã™ã€‚ initProcessEnabled ã¯ docker run ã® --init ã«ç›¸å½“ã—ã¾ã™ã€‚ Kubernetesã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹å ´åˆ tiniãªã©ã®è»½é‡initã‚’ä½¿ã†æ–¹æ³•ã‚‚ã‚ã‚Šã¾ã™ãŒã€Kubernetes 1. 17ã‹ã‚‰ã¯ Share Process Namespace ( 5 ) ã‚’ä½¿ã†ã“ã¨ã§å•é¡Œã‚’å›é¿ã§ãã‚‹ã‚ˆã†ã§ã™ã€‚ Docker and the PID 1 zombie reaping problem â†© Puma returns exit code 1 in container when received SIGTERM â†© Compose ãƒ•ã‚¡ã‚¤ãƒ« ãƒãƒ¼ã‚¸ãƒ§ãƒ³ 3 ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ - init â†© AWS::ECS::TaskDefinition LinuxParameters â†© Share Process Namespace between Containers in a Pod â†© 25 Go to list of users who liked 12 comment 0 Go to list of comments Register as a new user and utilize Qiita more conveniently You obtain articles that match your needs You can efficiently read back useful information You can utilize dark theme What you can do with signing up Sign up Login




# MACHINE-LEARNING 10 Best Python Machine Learning Tutorials Gilad David Maayan Feb 10, 2023




A step-by-step pratical guide for deploying NVIDIA GPUs on Kubernetes 19 July 2024 - 27 mins read time Tags: kubernetes Prerequisites Kubernetes nodes configuration Client machine configuration Create a Kubernetes cluster Add GPU support to the Kubernetes cluster Deploying an Application Requiring a GPU Conclusion Resources This blog post guides you through the creation of a Kubernetes cluster with NVIDIA GPU resources. We will utilize the kubeadm deployment tool to setup the Kubernetes cluster. For the discovery and configuration of nodes with GPU cards, we will integrate GPU Operator. Finally, we will deploy the Ollama application on the cluster and verify that it correctly uses the GPU resources. This experiment was validated by the intern FrÃ©dÃ©ric Alleron, student in the Network and Telecom department at the IUT ChÃ¢tellerault, who completed his internship from June to July 2024 at the LIAS laboratory. Prerequisites The hardware prerequisites to reproduce this experiment are: two Linux machines (my setup: Ubuntu 22. 04 LTS), one of which has at least one NVIDIA GPU (my setup: NVIDIA P400 with 2GB). The disk size and memory are not crucial, a client machine (my setup: macOS Sonoma) for accessing the Kubernetes cluster. Since the component GPU Operator , which allows the installation of NVIDIA GPU support on Kubernetes, does not support version 24. 04 LTS ( platform-support. html ), we will limit ourselves to Ubuntu 22. The three machines are identified in the network as follows: client machine: 192. 102, master/worker node (one with GPU): 192. 103 named k8s-gpu-node1 , worker node: 192. 104 named k8s-gpu-node2. Versions of the used components/tools: containerd: 1. 13, kubeadm, kubelet, kubectl: 1. For learning about Kubernetes, you can consult my training courses: course material: Mise en Å“uvre dâ€™architectures microservices avec Kubernetes ğŸ‡«ğŸ‡· ; hands on lab: Tutoriel Microservices avec Kubernetes - Les bases de K8s ğŸ‡«ğŸ‡·. Kubernetes nodes configuration This section details the configuration of both nodes prior to setting up the Kubernetes cluster (installation of components and operating system configuration). All operations must be performed identically on all nodes. Update the repositories and install the latest versions of packages already present on the operating system. 1 2 $ sudo apt-obtain update $ sudo apt-obtain upgrade -y A Kubernetes cluster requires a container runtime manager on each cluster node. The container runtime manager is responsible for managing the entire lifecycle of containers, including image management, container startup and shutdown. We will utilize Containerd , which appears to be the most widely used. An incomplete list of various container runtimes is available at https://kubernetes. io/docs/setup/production-environment/container-runtimes. Download the latest current version of Containerd from GitHub and extract the contents into the directory /usr/local/. 1 2 $ wget https://github. com/containerd/containerd/releases/download/v1. gz $ sudo tar -C /usr/local -xzvf containerd-1. gz Download the containerd service description file. 1 2 $ wget https://raw. com/containerd/containerd/main/containerd. service $ sudo mv containerd. service /lib/systemd/system/containerd. service begin the containerd service using the command below. 1 2 $ sudo systemctl daemon-reload $ sudo systemctl enable --now containerd Verify that the containerd service is started. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ sudo systemctl status containerd â— containerd. service - containerd container runtime Loaded: loaded (/lib/systemd/system/containerd. service; enabled; vendor preset: enabled) Active: active (running) since Mon 2024-07-15 14:11:19 UTC; 1min 1s ago Docs: https://containerd. io Process: 2449 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 2450 (containerd) Tasks: 9 Memory: 13. 7M CPU: 148ms CGroup: /system. service â””â”€2450 /usr/local/bin/containerd Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time="2024-07-15T14:11:19. 305055646Z" level=info msg="begin subscribing containerd event" Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time="2024-07-15T14:11:19. 305117731Z" level=info msg="begin recovering state" Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time="2024-07-15T14:11:19. 305182076Z" level=info msg="begin event monitor" Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time="2024-07-15T14:11:19. 305200106Z" level=info msg="begin snapshots syncer" Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time="2024-07-15T14:11:19. 305216073Z" level=info msg="begin cni network conf syncer for default" Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time="2024-07-15T14:11:19. 305229234Z" level=info msg="begin streaming server" Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time="2024-07-15T14:11:19. 305092597Z" level=info msg=serving. address=/run/containerd/containerd. ttrpc Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time="2024-07-15T14:11:19. 305368243Z" level=info msg=serving. address=/run/containerd/containerd. sock Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time="2024-07-15T14:11:19. 305468660Z" level=info msg="containerd successfully booted in 0. 033130s" Jul 15 14:11:19 k8s-gpu-node1 systemd[1]: Started containerd container runtime. Containerd is associated with a container runtime that interacts directly with the Linux kernel to configure and run containers. We will utilize runC , which also appears to be widely used. Download the latest current version of runC from GitHub and install it in the directory /usr/local/sbin. 1 2 $ wget https://github. com/opencontainers/runc/releases/download/v1. amd64 $ sudo install -m 755 runc. amd64 /usr/local/sbin/runc From the configuration of Containerd , the CGroup driver for runC must be configured. 1 2 3 $ sudo mkdir -p /etc/containerd/ $ containerd config default | sudo tee /etc/containerd/config. toml $ sudo sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config. toml Restart Containerd to apply the previous modifications. 1 $ sudo systemctl restart containerd Ensure that the Containerd service is still running. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ systemctl status containerd â— containerd. service - containerd container runtime Loaded: loaded (/lib/systemd/system/containerd. service; enabled; vendor preset: enabled) Active: active (running) since Mon 2024-07-15 14:51:27 UTC; 17min ago Docs: https://containerd. io Process: 2907 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 2908 (containerd) Tasks: 9 Memory: 14. service â””â”€2908 /usr/local/bin/containerd Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time="2024-07-15T14:51:27. 678801098Z" level=info msg="begin subscribing containerd event" Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time="2024-07-15T14:51:27. 678888229Z" level=info msg="begin recovering state" Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time="2024-07-15T14:51:27. 678895101Z" level=info msg=serving. address=/run/containerd/containerd. ttrpc Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time="2024-07-15T14:51:27. 678980481Z" level=info msg=serving. address=/run/containerd/containerd. sock Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time="2024-07-15T14:51:27. 678988030Z" level=info msg="begin event monitor" Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time="2024-07-15T14:51:27. 679069975Z" level=info msg="begin snapshots syncer" Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time="2024-07-15T14:51:27. 679091006Z" level=info msg="begin cni network conf syncer for default" Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time="2024-07-15T14:51:27. 679129596Z" level=info msg="begin streaming server" Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time="2024-07-15T14:51:27. 679219448Z" level=info msg="containerd successfully booted in 0. 029455s" Jul 15 14:51:27 k8s-gpu-node1 systemd[1]: Started containerd container runtime. Some components ( kubelet , for example) of Kubernetes do not work well with Linux SWAP. Therefore, Linux SWAP must be disabled. Disable Linux SWAP without rebooting and permanently way from by editing the etc/fstab file. 1 2 $ sudo swapoff -a $ sudo sed -i '/swap/s/^/#/' /etc/fstab The Linux kernel does not allow IPv4 packet routing between interfaces by default. 1 2 3 4 5 cat <<EOF | sudo tee /etc/sysctl. ip_forward = 1 EOF sudo sysctl --system Four tools shall be installed: kubelet , kubeadm , kubectl and helm. The first tool kubelet is responsible for the runtime state on each node, ensuring all containers run within a Pod. The second tool kubeadm handles cluster creation. The third kubectl represents command-line utility for administering the Kubernetes cluster. Finally, helm represents tool used to define, install, and upgrade applications using charts for Kubernetes. Note that kubectl and helm are client tools and are not necessarily required on cluster nodes. However, they are required on the client machine. Install kubelet , kubeadm and kubectl. 1 2 3 4 5 6 7 8 9 $ sudo apt-obtain update $ sudo apt-obtain install -y apt-transport-https ca-certificates curl gpg $ curl -fsSL https://pkgs. key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring. gpg $ echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring. 30/deb/ /' | sudo tee /etc/apt/sources. list $ sudo apt-obtain update $ sudo apt-obtain install -y kubelet kubeadm kubectl $ sudo apt-mark hold kubelet kubeadm kubectl Install helm. 1 2 3 $ curl -fsSL -o get_helm. com/helm/helm/main/scripts/obtain-helm-3 $ chmod 700 get_helm. sh Enable and begin the kubelet service. 1 2 3 $ sudo systemctl daemon-reload $ sudo systemctl enable --now kubelet $ sudo systemctl begin kubelet obtain the status of the kubelet component. 1 2 3 4 5 6 7 8 9 10 $ systemctl status kubelet â— kubelet. service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/lib/systemd/system/kubelet. service; enabled; preset: enabled) Drop-In: /usr/lib/systemd/system/kubelet. conf Active: activating (auto-restart) (Result: exit-code) since Mon 2024-07-15 15:31:41 UTC; 3s ago Docs: https://kubernetes. io/docs/ Process: 11727 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=1/FAILURE) Main PID: 11727 (code=exited, status=1/FAILURE) CPU: 47ms You can observe that the kubelet component is enabled but not started; it will become active once the cluster is set up. Client machine configuration On the client machine, the kubectl and helm tools shall be necessary. Below, we detail the installation of these tools on macOS and Linux. macOS : to install kubectl and helm via Homebrew. 1 $ brew install kubectl helm Linux : to install kubectl and helm on any Linux distribution. 1 2 3 4 5 6 7 8 9 # kubectl $ curl -LO https://storage. com/kubernetes-release/release/$(curl -s https://storage. com/kubernetes-release/release/stable. txt)/bin/linux/amd64/kubectl $ chmod +x. /kubectl /usr/local/bin/kubectl # helm $ curl -fsSL -o get_helm. com/helm/helm/main/scripts/obtain-helm-3 $ chmod 700 get_helm. sh Create a Kubernetes cluster This section shows how to create a Kubernetes cluster using the kubeadm tool. kubeadm represents command-line tool for managing a Kubernetes cluster by installing various components. Only kubelet needs to be installed before, as described in the previous section. From the master node ( k8s-gpu-node1 ), initialize the Kubernetes cluster. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ sudo kubeadm init --node-name node-master` --cri-socket /run/containerd/containerd. Your Kubernetes control-plane has initialized successfully! To begin using your cluster, you require to run the following as a regular user: mkdir -p $HOME/. kube sudo cp -i /etc/kubernetes/admin. kube/config sudo chown $(id -u):$(id -g) $HOME/. kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin. conf You should now deploy a pod network to the cluster. Run "kubectl apply -f [podnetwork]. yaml" with one of the options listed at: https://kubernetes. io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192. 103:6443 --token YOUR_TOKEN \ --discovery-token-ca-cert-hash sha256:YOUR_CA_CERT_HASH At the end of the installation, you shall be prompted to perform some operations to access the Kubernetes cluster. The first one involves storing the Kubernetes cluster access information in $HOME/. This file may be used by kubectl to interact with the cluster. The second operation is to add a node to the Kubernetes cluster. 1 2 3 $ mkdir -p $HOME/. kube $ sudo cp -i /etc/kubernetes/admin. kube/config $ sudo chown $(id -u):$(id -g) $HOME/. kube/config To allow the client machine to connect to the Kubernetes cluster, copy the config file from master node ( k8s-gpu-node1 ) to the client machine. kube $ scp k8suser@192. kube Still from the client machine, test the communication with the Kubernetes cluster. 1 2 3 $ kubectl obtain nodes NAME STATUS ROLES AGE VERSION node-master NotReady control-plane 25h v1. 3 The master node is currently the only node in the Kubernetes cluster. Additionally, our cluster cannot schedule Pods on this master node for security reasons. Since our cluster may not have many nodes, the security feature shall be disabled. 1 2 $ kubectl taint nodes node-master node-role. io/control-plane- $ kubectl label nodes node-master node. io/exclude-from-external-load-balancers- To re-enable this security feature. 1 2 $ kubectl taint nodes node-master node-role. io/control-plane:NoSchedule $ kubectl label nodes node-master node. io/exclude-from-external-load-balancers=true Letâ€™s add a second node to the Kubernetes cluster. Connect to the worker node k8s-gpu-node2 and execute the following command-line instructions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ sudo kubeadm join 192. 103:6443 --token gf8ui6. ulo4gcme68k7j1zv \ --discovery-token-ca-cert-hash sha256:2563ef8edc1fb9e4bdfdde6c0e723b9812647405be819eff95596eeae0ac254e [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster. [preflight] FYI: You can look at this config file with 'kubectl -n kube-system obtain cm kubeadm-config -o yaml' [kubelet-begin] Writing kubelet configuration to file "/var/lib/kubelet/config. yaml" [kubelet-begin] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags. env" [kubelet-begin] Starting the kubelet [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s [kubelet-check] The kubelet is healthy after 507. 199197ms [kubelet-begin] Waiting for the kubelet to perform the TLS Bootstrap This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl obtain nodes' on the control-plane to observe this node join the cluster. The values for the --token and --discovery-token-ca-cert-hash parameters are provided during the creation of the Kubernetes cluster. However, the token value is valid for only 24 hours and you may not have had time to save this information from the console. Donâ€™t worry, both pieces of information may be retrieved from the master node. To retrieve the token value ( --token ) if the 24-hour deadline has not been reached. 1 2 3 $ kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS gf8ui6. ulo4gcme68k7j1zv 23h 2024-07-17T16:58:57Z authentication,signing <none> system:bootstrappers:kubeadm:default-node-token To generate a new value of the token. 1 2 3 4 5 6 $ kubeadm token create zuku5f. gjtnq2bcupmg0902 $ kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS gf8ui6. ulo4gcme68k7j1zv 23h 2024-07-17T16:58:57Z authentication,signing <none> system:bootstrappers:kubeadm:default-node-token zuku5f. gjtnq2bcupmg0902 23h 2024-07-17T17:24:39Z authentication,signing <none> system:bootstrappers:kubeadm:default-node-token To retrieve the value of the certificate authority hash ( --discovery-token-ca-cert-hash ). 1 2 $ openssl x509 -in /etc/kubernetes/pki/ca. crt -pubkey -noout | openssl pkey -pubin -outform DER | openssl dgst -sha256 SHA2-256(stdin)= 2563ef8edc1fb9e4bdfdde6c0e723b9812647405be819eff95596eeae0ac254e From the client machine, check that both nodes are available. 1 2 3 4 $ kubectl obtain nodes NAME STATUS ROLES AGE VERSION k8s-gpu-node2 NotReady <none> 15m v1. 3 node-master NotReady control-plane 25h v1. 3 Also, ensure that the internal Kubernetes components installed by kubeadm are deployed. 1 2 3 4 5 6 7 8 9 10 $ kubectl obtain pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-7db6d8ff4d-ht8wc 0/1 Pending 0 25h coredns-7db6d8ff4d-rlzh5 0/1 Pending 0 25h etcd-node-master 1/1 Running 0 25h kube-apiserver-node-master 1/1 Running 0 25h kube-controller-manager-node-master 1/1 Running 0 25h kube-proxy-f66kq 1/1 Running 0 20m kube-proxy-rvcxh 1/1 Running 0 25h kube-scheduler-node-master 1/1 Running 0 25h In the previous outputs, both nodes are in NotReady status and the Pods coredns-7db6d8ff4d-ht8wc and coredns-7db6d8ff4d-rlzh5 are not deployed. To resolve this issue, a network plugin compliant with the CNI project must be installed. This will enable Pods to communicate within the Kubernetes cluster. There are numerous network plugins available, and the choice was made to utilize Cilium. Cilium offers the significant advantage of leveraging eBPF (extended Berkeley Packet Filter) technology, which has recently been integrated into Linux kernels. With eBPF, there is no require to load modules into the Linux kernel as was necessary with IPTables. From the master node, download the latest version of Cilium. 1 2 3 4 $ CILIUM_CLI_VERSION=$(curl -s https://raw. com/cilium/cilium-cli/main/stable. txt) $ curl -L --remote-name-all https://github. com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-amd64. gz $ sudo tar xzvfC cilium-linux-amd64. gz /usr/local/bin $ rm cilium-linux-amd64. gz To configure the installation of Cilium , you can utilize a configuration file. For instance, in the values. yaml file shown below, the CIDR (Classless Inter-Domain Routing) used to assign IPs to the Pods is modified. 1 2 3 4 ipam: mode: "cluster-pool" operator: clusterPoolIPv4PodCIDRList: ["172. 0/16"] Install Cilium. The installed version shall be shown. 1 2 3 4 $ cilium install --helm-values values. yaml â„¹ï¸ Using Cilium version 1. 6 ğŸ”® Auto-detected cluster name: kubernetes ğŸ”® Auto-detected kube-proxy was installed Wait a few seconds for the images to be downloaded and the pods to be deployed, and so check that the network plugin was successfully installed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ cilium status /Â¯Â¯\ /Â¯Â¯\__/Â¯Â¯\ Cilium: OK \__/Â¯Â¯\__/ Operator: OK /Â¯Â¯\__/Â¯Â¯\ Envoy DaemonSet: disabled (using embedded mode) \__/Â¯Â¯\__/ Hubble Relay: disabled \__/ ClusterMesh: disabled Deployment cilium-operator Desired: 1, Ready: 1/1, Available: 1/1 DaemonSet cilium Desired: 2, Ready: 2/2, Available: 2/2 Containers: cilium Running: 2 cilium-operator Running: 1 Cluster Pods: 2/2 managed by Cilium Helm chart version: Image versions cilium quay. 6@sha256:6aa840986a3a9722cd967ef63248d675a87add7e1704740902d5d3162f0c0def: 2 cilium-operator quay. io/cilium/operator-generic:v1. 6@sha256:5789f0935eef96ad571e4f5565a8800d3a8fbb05265cf6909300cd82fd513c3d: 1 From the client machine, verify that both nodes are available and operational. 1 2 3 4 kubectl obtain nodes NAME STATUS ROLES AGE VERSION k8s-gpu-node2 Ready <none> 20h v1. 3 node-master Ready control-plane 45h v1. 3 The status of both nodes must be Ready. Add GPU support to the Kubernetes cluster At this stage, a Kubernetes cluster with two nodes is configured. One of the nodes has a GPU card, but the Kubernetes cluster does not understand that this node has a GPU. The goal of this section is to declare the GPU in the cluster and identify it as a resource, similar to a CPU or memory resource. However, configuring a GPU in a Kubernetes cluster is not trivial since it requires installing the GPU driver, identifying it with the container manager Containerd , detecting and labeling the nodes with GPUs, and installing specific libraries (such as CUDA). NVIDIA has provided an operator called GPU Operator that simplifies all these tasks. This section aims to detail the installation of this operator. Create a namespace called gpu-operator. The GPU Operator shall be deployed in this namespace. 1 2 $ kubectl create ns gpu-operator namespace/gpu-operator created Add the NVIDIA helm repository. 1 $ helm repo add nvidia https://helm. com/nvidia Install GPU Operator. 1 2 3 4 5 6 7 $ helm install --wait --generate-name -n gpu-operator --create-namespace nvidia/gpu-operator NAME: gpu-operator-1721224440 LAST DEPLOYED: Wed Jul 17 15:54:02 2024 NAMESPACE: gpu-operator STATUS: deployed REVISION: 1 TEST SUITE: None The operator will perform several tasks to discover that an NVIDIA GPU is available on the master node. New labels were added to the description of the master node. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 $ kubectl describe nodes node-master | grep nvidia. timestamp=1721229911 nvidia. com/gpu-driver-upgrade-state=upgrade-done nvidia. container-toolkit=true nvidia. dcgm-exporter=true nvidia. device-plugin=true nvidia. gpu-feature-discovery=true nvidia. node-status-exporter=true nvidia. operator-validator=true nvidia. machine=Precision-3450 nvidia. product=Quadro-P400 nvidia. sharing-strategy=none nvidia. strategy=single nvidia. com/gpu-driver-upgrade-enabled: true The previous description shows that a GPU is available: nvidia. count=1 , and the detected card represents Quadro-P400 with 2048 MB of memory. A Pod called cuda-vectoradd based on the image nvcr. io/nvidia/k8s/cuda-sample:vectoradd-cuda12. 04 is deployed to verify that the GPU may be used by a program for GPU computations. Once the computations are completed, the Pod stops. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 cat <<EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: cuda-vectoradd spec: restartPolicy: OnFailure containers: - name: cuda-vectoradd image: "nvcr. io/nvidia/k8s/cuda-sample:vectoradd-cuda12. 04" resources: limits: nvidia. com/gpu: 1 EOF Display the logs of the cuda-vectoradd pod. 1 2 3 4 5 6 7 $ kubectl logs pod/cuda-vectoradd [Vector addition of 50000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done The GPU usage by the cuda-vectoradd Pod works perfectly. Letâ€™s now focus on an application that continuously uses the GPU and will demonstrate the utilization of a constrained GPU resource. Deploying an Application Requiring a GPU The experimental application used shall be Ollama. It is an application that exposes generative AI models, such as LLMs, via a REST API. It is possible to download LLM models and to run them either using only the CPU or by combining the CPU with a GPU to reduce execution time. The outcome of this experiment should demonstrate that if Ollama utilizes a GPU resource, it is preempted and not available for other applications until Ollama releases it. The Ollama application is available through helm. All the following operations shall be performed from the client machine. Before deploying the Ollama application, check that the GPU resource is available by querying the description of the master node. 1 2 3 4 5 6 7 8 9 10 11 12 $ kubectl describe nodes node-master. Allocated resources: (Total limits may be over 100 percent, i. ) Resource Requests Limits --- --- --- cpu 1150m (9%) 500m (4%) memory 350Mi (1%) 690Mi (2%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) nvidia. com/gpu 0 0 The resource identified by nvidia. com/gpu shows that it is not in utilize. Add the Ollama helm repository. 1 2 $ helm repo add ollama-helm https://otwld. io/ollama-helm/ "ollama-helm" was added to your repositories Create a ollama namespace to group all resources related to the Ollama application. 1 2 $ kubectl create ns ollama namespace/ollama created Deploy Ollama application into the Kubernetes cluster. 1 2 3 4 5 6 7 8 9 10 11 12 $ helm install appli-ollama ollama-helm/ollama --namespace ollama --set ollama. enabled=true --set ollama. number=1 --set ollama. type=nvidia NAME: appli-ollama LAST DEPLOYED: Thu Jul 18 09:43:56 2024 NAMESPACE: ollama STATUS: deployed REVISION: 1 NOTES: 1. obtain the application URL by running these commands: export POD_NAME=$(kubectl obtain pods --namespace ollama -l "app. io/instance=appli-ollama" -o jsonpath="{. name}") export CONTAINER_PORT=$(kubectl obtain pod --namespace ollama $POD_NAME -o jsonpath="{. containerPort}") echo "Visit http://127. 1:8080 to utilize your application" kubectl --namespace ollama port-forward $POD_NAME 8080:$CONTAINER_PORT It specifies that GPU support must be enabled ( --set ollama. enabled=true ), the required number of GPUs is one ( --set ollama. number=1 ) and the GPU type must be NVIDIA ( --set ollama. Check that the Ollama application was deployed (packaged into a Pod) on the master node that has the GPU. 1 2 3 $ kubectl obtain pods -n ollama -o wide NAME READY STATUS RESTARTS AGE IP NODE appli-ollama-8665457c88-gz8ch 1/1 Running 0 2m59s 10. 242 node-master The Pod (related to the Ollama application) is correctly located on the master node. In the deployment output for the Ollama application, it is also explained how to utilize the deployed application via a port-forward. However, this is not the deployment method we will utilize; instead, we will utilize a classic NodePort service. Apply the following service configuration to expose Ollama at the addresses 192. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ cat <<EOF | kubectl create -n ollama -f - kind: Service apiVersion: v1 metadata: name: ollamanodeportservice spec: selector: app. io/name: ollama type: NodePort ports: - protocol: TCP targetPort: 11434 port: 11434 nodePort: 30001 externalIPs: - 80. 12 EOF service/ollamanodeportservice created Execute the HTTP request to download the LLM model called gemma:2b in Ollama. 1 2 3 4 5 $ curl http://192. 103:30001/api/pull -d '{ "name": "gemma:2b" }' {"status":"pulling manifest"}. Execute the HTTP request to generate a response to the given question. 1 2 3 4 5 6 $ curl http://192. 103:30001/api/generate -d '{ "model": "gemma:2b", "prompt": "Why is the sky blue?", "stream": false }' {"model":"gemma:2b","created_at":"2024-07-18T08:00:02. 461703025Z","response":"The sky appears blue due to Rayleigh scattering. Rayleigh scattering is the scattering of light by small particles, such as molecules in the atmosphere. Blue light has a shorter wavelength than other colors of light, so it is scattered more strongly. This is why the sky appears blue. ","done":true,"done_reason":"cease","context":[968,2997,235298,559,235298,15508,235313,1645,108,4385,603,573,8203,3868,181537,615,235298,559,235298,15508,235313,108,235322,2997,235298,559,235298,15508,235313,2516,108,651,8203,8149,3868,3402,577,153902,38497,235265,153902,38497,603,573,38497,576,2611,731,2301,16071,235269,1582,685,24582,575,573,13795,235265,7640,2611,919,476,25270,35571,1178,1156,9276,576,2611,235269,712,665,603,30390,978,16066,235265,1417,603,3165,573,8203,8149,3868,235265],"total_duration":4802224355,"load_duration":33326246,"prompt_eval_count":32,"prompt_eval_duration":324835000,"eval_count":55,"eval_duration":4400550000} Everything is working correctly, Ollama answers a question and generates a response quickly. Check the usage of the GPU to observe if it was preempted following the deployment of the Ollama application. 1 2 3 4 5 6 7 8 9 10 11 12 $ kubectl describe nodes node-master. Allocated resources: (Total limits may be over 100 percent, i. ) Resource Requests Limits --- --- --- cpu 1150m (9%) 500m (4%) memory 350Mi (1%) 690Mi (2%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) nvidia. com/gpu 1 1 The GPU resources of the Kubernetes cluster are no longer available as they have all been preempted. Thus, if a Pod requiring GPU resources needs to be deployed, the Kubernetes cluster will put it on hold until the GPU resources are freed. To validate this scenario, we will deploy a new instance of Ollama. Create a ollama2 namespace to group all resources related to the Ollama application. 1 2 $ kubectl create ns ollama2 namespace/ollama2 created Deploy Ollama application in the Kubernetes cluster within the ollama2 namespace. 1 2 3 4 5 6 7 8 9 10 11 12 $ helm install appli-ollama ollama-helm/ollama --namespace ollama --set ollama. enabled=true --set ollama. number=1 --set ollama. type=nvidia NAME: appli-ollama LAST DEPLOYED: Thu Jul 18 09:43:56 2024 NAMESPACE: ollama STATUS: deployed REVISION: 1 NOTES: 1. obtain the application URL by running these commands: export POD_NAME=$(kubectl obtain pods --namespace ollama -l "app. io/instance=appli-ollama" -o jsonpath="{. name}") export CONTAINER_PORT=$(kubectl obtain pod --namespace ollama $POD_NAME -o jsonpath="{. containerPort}") echo "Visit http://127. 1:8080 to utilize your application" kubectl --namespace ollama port-forward $POD_NAME 8080:$CONTAINER_PORT Display the status of the Pods in the ollama2 namespace. 1 2 3 $ kubectl obtain pods -n ollama2 NAME READY STATUS RESTARTS AGE appli-ollama-8665457c88-ngtpf 0/1 Pending 0 116 As expected, the Pod is in the Pending state. Check the Podâ€™s description to determine the reason for its Pending state. 1 2 3 4 5 6 $ kubectl describe pod appli-ollama -n ollama2. Events: Type Reason Age From Message --- --- --- --- --- Warning FailedScheduling 3m24s default-scheduler 0/2 nodes are available: 2 Insufficient nvidia. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod. As indicated in the message, no node in the cluster can accommodate this new Pod. Conclusion This experiment showed the setup of a Kubernetes cluster and the discovery of GPU nodes via the GPU Operator. There are still many aspects to explore, particularly updating the components installed by the GPU Operator (drivers, libraries, etc. It may also be worthwhile to examine how to manage NVIDIA cards with MIG technology, which aims to partition a GPU into multiple sub-GPUs. Stay tuned and give your feedbacks in the comments. Resources https://docs. com/datacenter/cloud-native/gpu-operator/latest/index. io/posts/nvidia-rtx-gpu-kubernetes-setup/ https://blog. org/2019/08/15/reconstructing-the-join-command-for-kubeadm/ https://blog. org/2019/07/12/calculating-ca-certificate-hash-for-kubeadm/ https://github. com/otwld/ollama-helm Commentaire Vous pouvez laisser un commentaire en utilisant les Github Issues. Je suis MickaÃ«l BARON IngÃ©nieur de Recherche en Informatique Ã  l' ISAE-ENSMA et membre du laboratoire LIAS le jour Veilleur Technologique la nuit #Java #Container #VueJS #Services #WebSemantic Derniers articles et billets Advanced Kubernetes Deployment on an NVIDIA GPUs Cluster: NGINX Ingress Controler, Local Storage, and Prometheus Monitoring kubernetes 13 fÃ©v. 2025 A step-by-step pratical guide for deploying NVIDIA GPUs on Kubernetes kubernetes 19 juil. 2024 Create and utilize custom XCP-NG templates: a guide for Ubuntu XCP-NG Xen 07 juin 2024 Environnements de dÃ©veloppement Ã  distance. Les solutions existantes ? PrÃ©sentation d'Onyxia Onyxia 17 sept. 2025 Article Running Coder in a K3s cluster self-hosted DevOps Kubernetes Coder 02 juin 2023




SYå°ç«™ kubernetesåˆ†æExitCode å…³æ³¨ä½œè€… è…¾è®¯äº‘ å¼€å‘è€…ç¤¾åŒº æ–‡æ¡£ å»ºè®®åé¦ˆ æ§åˆ¶å° ç™»å½•/æ³¨å†Œ é¦–é¡µ å­¦ä¹  æ´»åŠ¨ ä¸“åŒº åœˆå±‚ å·¥å…· MCPå¹¿åœº æ–‡ç« /ç­”æ¡ˆ/æŠ€æœ¯å¤§ç‰› æœç´¢ æœç´¢ å…³é—­ å‘å¸ƒ SYå°ç«™ ç¤¾åŒºé¦–é¡µ > ä¸“æ  > kubernetesåˆ†æExitCode kubernetesåˆ†æExitCode SYå°ç«™ å…³æ³¨ å‘å¸ƒ äº 2020-06-15 12:28:08 å‘å¸ƒ äº 2020-06-15 12:28:08 3. 8K 0 ä¸¾æŠ¥ æ–‡ç« è¢«æ”¶å½•äºä¸“æ ï¼š SYå°ç«™çš„ä¸“æ  SYå°ç«™çš„ä¸“æ  01 é—®é¢˜ æœ€è¿‘æ€»æœ‰å¼€å‘å°ä¼™ä¼´æ¥æ‰¾æˆ‘ï¼Œä¸ºä»€ä¹ˆæˆ‘çš„å®¹å™¨æ€»é€€å‡ºå‘¢ï¼Œåœ¨å“ªèƒ½çœ‹åˆ°åŸå› ã€‚æ•…å†™ç¯‡æ–‡ç« æ•´ç†ä¸‹dockeré€€å‡ºçš„çŠ¶æ€ç ã€‚ 02 å¦‚ä½•æŸ¥çœ‹é€€å‡ºç  æŸ¥çœ‹podä¸­çš„å®¹å™¨é€€å‡ºç  ä»£ç è¯­è¨€ï¼š javascript å¤åˆ¶ $ kubectl describe pod xxx Port: <none> Host Port: <none> State: Running Started: Tue, 26 May 2020 20:01:04 +0800 Last State: Terminated Reason: Error Exit Code: 137 Started: Tue, 26 May 2020 19:58:40 +0800 Finished: Tue, 26 May 2020 20:01:04 +0800 Ready: True Restart Count: 2363 dockeræŸ¥çœ‹ ä»£ç è¯­è¨€ï¼š javascript å¤åˆ¶ $ docker ps --filter "status=exited" $ docker inspect <container-id> --format='{{. ExitCode}}' 03 å¸¸è§é€€å‡ºç  Exit Code 0 é€€å‡ºä»£ç 0è¡¨ç¤ºç‰¹å®šå®¹å™¨æ²¡æœ‰é™„åŠ å‰å°è¿›ç¨‹ã€‚ è¯¥é€€å‡ºä»£ç æ˜¯æ‰€æœ‰å…¶ä»–åç»­é€€å‡ºä»£ç çš„ä¾‹å¤–ã€‚ è¿™ä¸ä¸€å®šæ„å‘³ç€å‘ç”Ÿäº†ä¸å¥½çš„äº‹æƒ…ã€‚å¦‚æœå¼€å‘äººå‘˜æƒ³è¦åœ¨å®¹å™¨å®Œæˆå…¶å·¥ä½œåè‡ªåŠ¨åœæ­¢å…¶å®¹å™¨ï¼Œåˆ™ä½¿ç”¨æ­¤é€€å‡ºä»£ç ã€‚ Exit Code 1 ç¨‹åºé”™è¯¯ï¼Œæˆ–è€…Dockerfileä¸­å¼•ç”¨ä¸å­˜åœ¨çš„æ–‡ä»¶ï¼Œå¦‚ entrypointä¸­å¼•ç”¨äº†é”™è¯¯çš„åŒ… ç¨‹åºé”™è¯¯å¯ä»¥å¾ˆç®€å•ï¼Œä¾‹å¦‚â€œé™¤ä»¥0â€ï¼Œä¹Ÿå¯ä»¥å¾ˆå¤æ‚ï¼Œæ¯”å¦‚ç©ºå¼•ç”¨æˆ–è€…å…¶ä»–ç¨‹åº crash Exit Code 137 æ­¤çŠ¶æ€ç ä¸€èˆ¬æ˜¯å› ä¸º pod ä¸­å®¹å™¨å†…å­˜è¾¾åˆ°äº†å®ƒçš„èµ„æºé™åˆ¶( resources. limits )ï¼Œä¸€èˆ¬æ˜¯å†…å­˜æº¢å‡º(OOM)ï¼ŒCPUè¾¾åˆ°é™åˆ¶åªéœ€è¦ä¸åˆ†æ—¶é—´ç‰‡ç»™ç¨‹åºå°±å¯ä»¥ã€‚å› ä¸ºé™åˆ¶èµ„æºæ˜¯é€šè¿‡ linux çš„ cgroup å®ç°çš„ï¼Œæ‰€ä»¥ cgroup ä¼šå°†æ­¤å®¹å™¨å¼ºåˆ¶æ€æ‰ï¼Œç±»ä¼¼äº kill -9 è¿˜å¯èƒ½æ˜¯ å®¿ä¸»æœº æœ¬èº«èµ„æºä¸å¤Ÿç”¨äº†(OOM)ï¼Œå†…æ ¸ä¼šé€‰å–ä¸€äº›è¿›ç¨‹æ€æ‰æ¥é‡Šæ”¾å†…å­˜ ä¸ç®¡æ˜¯ cgroup é™åˆ¶æ€æ‰è¿›ç¨‹è¿˜æ˜¯å› ä¸ºèŠ‚ç‚¹æœºå™¨æœ¬èº«èµ„æºä¸å¤Ÿå¯¼è‡´è¿›ç¨‹æ­»æ‰ï¼Œéƒ½å¯ä»¥ä»ç³»ç»Ÿæ—¥å¿—ä¸­æ‰¾åˆ°è®°å½•( journalctl -k ) Exit Code 139 è¡¨æ˜å®¹å™¨æ”¶åˆ°äº†SIGSEGVä¿¡å·ï¼Œæ— æ•ˆçš„å†…å­˜å¼•ç”¨ï¼Œå¯¹åº”kill -11 ä¸€èˆ¬æ˜¯ä»£ç æœ‰é—®é¢˜ï¼Œæˆ–è€… docker çš„åŸºç¡€é•œåƒæœ‰é—®é¢˜ Exit Code 143 è¡¨æ˜å®¹å™¨æ”¶åˆ°äº†SIGTERMä¿¡å·ï¼Œç»ˆç«¯å…³é—­ï¼Œå¯¹åº”kill -15 ä¸€èˆ¬å¯¹åº”docker cease å‘½ä»¤ æœ‰æ—¶docker stopä¹Ÿä¼šå¯¼è‡´Exit Code 137ã€‚å‘ç”Ÿåœ¨ä¸ä»£ç æ— æ³•å¤„ç†SIGTERMçš„æƒ…å†µä¸‹ï¼Œdockerè¿›ç¨‹ç­‰å¾…åç§’é’Ÿç„¶åå‘å‡ºSIGKILLå¼ºåˆ¶é€€å‡ºã€‚ Exit Code 1 å’Œ 255 è¿™ç§å¯èƒ½æ˜¯ä¸€èˆ¬é”™è¯¯ï¼Œå…·ä½“é”™è¯¯åŸå› åªèƒ½çœ‹å®¹å™¨æ—¥å¿—ï¼Œå› ä¸ºå¾ˆå¤šç¨‹åºå‘˜å†™å¼‚å¸¸é€€å‡ºæ—¶ä¹ æƒ¯ç”¨ exit(1) æˆ– exit(-1) ï¼Œ-1 ä¼šæ ¹æ®è½¬æ¢è§„åˆ™è½¬æˆ 255 æœ¬æ–‡å‚ä¸ è…¾è®¯äº‘è‡ªåª’ä½“åŒæ­¥æ›å…‰è®¡åˆ’ ï¼Œåˆ†äº«è‡ªå¾®ä¿¡å…¬ä¼—å·ã€‚ åŸå§‹å‘è¡¨ï¼š2020-05-27 ï¼Œå¦‚æœ‰ä¾µæƒè¯·è”ç³» cloudcommunity@tencent. com åˆ é™¤ ç¼–ç¨‹ç®—æ³• å®¹å™¨ å®¹å™¨é•œåƒæœåŠ¡ linux æœ¬æ–‡åˆ†äº«è‡ª SYæŠ€æœ¯å°ç«™ å¾®ä¿¡å…¬ä¼—å·ï¼Œ å‰å¾€æŸ¥çœ‹ å¦‚æœ‰ä¾µæƒï¼Œè¯·è”ç³» cloudcommunity@tencent. com åˆ é™¤ã€‚ æœ¬æ–‡å‚ä¸ è…¾è®¯äº‘è‡ªåª’ä½“åŒæ­¥æ›å…‰è®¡åˆ’ ï¼Œæ¬¢è¿çƒ­çˆ±å†™ä½œçš„ä½ ä¸€èµ·å‚ä¸ï¼ ç¼–ç¨‹ç®—æ³• å®¹å™¨ å®¹å™¨é•œåƒæœåŠ¡ linux è¯„è®º ç™»å½• åå‚ä¸è¯„è®º 0 æ¡è¯„è®º çƒ­åº¦ æœ€æ–° ç™»å½• åå‚ä¸è¯„è®º æ¨èé˜…è¯» ç›®å½• 01 é—®é¢˜ 02 å¦‚ä½•æŸ¥çœ‹é€€å‡ºç  03 å¸¸è§é€€å‡ºç  Exit Code 0 Exit Code 1 Exit Code 137 Exit Code 139 Exit Code 143 Exit Code 1 å’Œ 255 ç›¸å…³äº§å“ä¸æœåŠ¡ å®¹å™¨æœåŠ¡ è…¾è®¯äº‘å®¹å™¨æœåŠ¡ï¼ˆTencent Kubernetes Engine, TKEï¼‰åŸºäºåŸç”Ÿ kubernetes æä¾›ä»¥å®¹å™¨ä¸ºæ ¸å¿ƒçš„ã€é«˜åº¦å¯æ‰©å±•çš„ä¼ä¸šçº§å®¹å™¨ç®¡ç†æœåŠ¡ã€‚é¦–åˆ›å•é›†ç¾¤æ··åˆèŠ‚ç‚¹çš„èµ„æºç®¡ç†æ¨¡å¼ï¼Œå…¨é¢å›´ç»• Agentic AI åº”ç”¨éƒ¨ç½²ä¸æè‡´èµ„æºæ•ˆèƒ½æä¾›å…¨åœºæ™¯è§£å†³æ–¹æ¡ˆï¼Œä¸ºç”¨æˆ·é‡Šæ”¾ AI æ—¶ä»£çš„æ— é™ç®—åŠ›ã€‚ äº§å“ä»‹ç» äº§å“æ–‡æ¡£ AIé©±åŠ¨ æ™ºé¢†æœªæ¥ é¢†åˆ¸ ç¤¾åŒº æŠ€æœ¯æ–‡ç«  æŠ€æœ¯é—®ç­” æŠ€æœ¯æ²™é¾™ æŠ€æœ¯è§†é¢‘ å­¦ä¹ ä¸­å¿ƒ æŠ€æœ¯ç™¾ç§‘ æŠ€æœ¯ä¸“åŒº æ´»åŠ¨ è‡ªåª’ä½“åŒæ­¥æ›å…‰è®¡åˆ’ é‚€è¯·ä½œè€…å…¥é©» è‡ªèä¸Šé¦–é¡µ æŠ€æœ¯ç«èµ› åœˆå±‚ è…¾è®¯äº‘æœ€å…·ä»·å€¼ä¸“å®¶ è…¾è®¯äº‘æ¶æ„å¸ˆæŠ€æœ¯åŒç›Ÿ è…¾è®¯äº‘åˆ›ä½œä¹‹æ˜Ÿ è…¾è®¯äº‘TDP å…³äº ç¤¾åŒºè§„èŒƒ å…è´£å£°æ˜ è”ç³»æˆ‘ä»¬ å‹æƒ…é“¾æ¥ MCPå¹¿åœºå¼€æºç‰ˆæƒå£°æ˜ è…¾è®¯äº‘å¼€å‘è€… æ‰«ç å…³æ³¨è…¾è®¯äº‘å¼€å‘è€… é¢†å–è…¾è®¯äº‘ä»£é‡‘åˆ¸ çƒ­é—¨äº§å“ åŸŸåæ³¨å†Œ äº‘æœåŠ¡å™¨ åŒºå—é“¾æœåŠ¡ æ¶ˆæ¯é˜Ÿåˆ— ç½‘ç»œåŠ é€Ÿ äº‘æ•°æ®åº“ åŸŸåè§£æ äº‘å­˜å‚¨ è§†é¢‘ç›´æ’­ çƒ­é—¨æ¨è äººè„¸è¯†åˆ« è…¾è®¯ä¼šè®® ä¼ä¸šäº‘ CDNåŠ é€Ÿ è§†é¢‘é€šè¯ å›¾åƒåˆ†æ MySQL æ•°æ®åº“ SSL è¯ä¹¦ è¯­éŸ³è¯†åˆ« æ›´å¤šæ¨è æ•°æ®å®‰å…¨ è´Ÿè½½å‡è¡¡ çŸ­ä¿¡ æ–‡å­—è¯†åˆ« äº‘ç‚¹æ’­ å¤§æ•°æ® å°ç¨‹åºå¼€å‘ ç½‘ç«™ç›‘æ§ æ•°æ®è¿ç§» Copyright Â© 2013 - 2025 Tencent Cloud. è…¾è®¯äº‘ ç‰ˆæƒæ‰€æœ‰ æ·±åœ³å¸‚è…¾è®¯è®¡ç®—æœºç³»ç»Ÿæœ‰é™å…¬å¸ ICPå¤‡æ¡ˆ/è®¸å¯è¯å·ï¼š ç²¤B2-20090059 æ·±å…¬ç½‘å®‰å¤‡å· 44030502008569 è…¾è®¯äº‘è®¡ç®—ï¼ˆåŒ—äº¬ï¼‰æœ‰é™è´£ä»»å…¬å¸ äº¬ICPè¯150476å· | äº¬ICPå¤‡11018762å· | äº¬å…¬ç½‘å®‰å¤‡å·11010802020287 é—®é¢˜å½’æ¡£ ä¸“æ æ–‡ç«  å¿«è®¯æ–‡ç« å½’æ¡£ å…³é”®è¯å½’æ¡£ å¼€å‘è€…æ‰‹å†Œå½’æ¡£ å¼€å‘è€…æ‰‹å†Œ Section å½’æ¡£ Copyright Â© 2013 - 2025 Tencent Cloud. è…¾è®¯äº‘ ç‰ˆæƒæ‰€æœ‰ ç™»å½• åå‚ä¸è¯„è®º 0 0 0 æ¨è
