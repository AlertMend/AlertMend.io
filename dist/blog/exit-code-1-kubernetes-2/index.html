<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>exit code 1 kubernetes | AlertMend AI</title>
  <meta name="description" content="REFERENCE CONTENT FROM TOP 8 GOOGLE SEARCH RESULTS This content is gathered from the top-ranking pages for comprehensive reference">
  <meta name="keywords" content="exit, code, kubernetes, Kubernetes, AlertMend AI, AIOps, container orchestration, DevOps">
  <meta name="author" content="AlertMend Team">
  <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1">
  <link rel="canonical" href="https://www.alertmend.io/blog/exit-code-1-kubernetes-2">
  <!-- Favicon - uses SVG logo -->
  <link rel="icon" type="image/svg+xml" href="/logos/alertmend-logo.svg" />
  <link rel="icon" type="image/svg+xml" href="/favicon.ico" />
  <link rel="apple-touch-icon" href="/logos/alertmend-logo.svg" />
  
  <!-- Open Graph -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://www.alertmend.io/blog/exit-code-1-kubernetes-2">
  <meta property="og:title" content="exit code 1 kubernetes | AlertMend AI">
  <meta property="og:description" content="REFERENCE CONTENT FROM TOP 8 GOOGLE SEARCH RESULTS This content is gathered from the top-ranking pages for comprehensive reference">
  <meta property="og:image" content="https://alertmend.io/og-image.jpg">
  
  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:url" content="https://www.alertmend.io/blog/exit-code-1-kubernetes-2">
  <meta name="twitter:title" content="exit code 1 kubernetes | AlertMend AI">
  <meta name="twitter:description" content="REFERENCE CONTENT FROM TOP 8 GOOGLE SEARCH RESULTS This content is gathered from the top-ranking pages for comprehensive reference">
  <meta name="twitter:image" content="https://alertmend.io/og-image.jpg">
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "exit code 1 kubernetes | AlertMend AI",
    "description": "REFERENCE CONTENT FROM TOP 8 GOOGLE SEARCH RESULTS This content is gathered from the top-ranking pages for comprehensive reference",
    "image": "https://alertmend.io/og-image.jpg",
    "datePublished": "2025-12-18",
    "dateModified": "2025-12-18",
    "author": {
      "@type": "Person",
      "name": "AlertMend Team"
    },
    "publisher": {
      "@type": "Organization",
      "name": "AlertMend AI",
      "logo": {
        "@type": "ImageObject",
        "url": "https://alertmend.io/logos/alertmend-logo.svg"
      }
    },
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://www.alertmend.io/blog/exit-code-1-kubernetes-2"
    },
    "articleSection": "Kubernetes"
  }
  </script>
  
  <!-- SearchAtlas Dynamic Optimization -->
  <script nowprocket nitro-exclude type="text/javascript" id="sa-dynamic-optimization" data-uuid="457086dd-8bfb-46dd-a38d-2f4a6efd0e7e" src="data:text/javascript;base64,dmFyIHNjcmlwdCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoInNjcmlwdCIpO3NjcmlwdC5zZXRBdHRyaWJ1dGUoIm5vd3Byb2NrZXQiLCAiIik7c2NyaXB0LnNldEF0dHJpYnV0ZSgibml0cm8tZXhjbHVkZSIsICIiKTtzY3JpcHQuc3JjID0gImh0dHBzOi8vZGFzaGJvYXJkLnNlYXJjaGF0bGFzLmNvbS9zY3JpcHRzL2R5bmFtaWNfb3B0aW1pemF0aW9uLmpzIjtzY3JpcHQuZGF0YXNldC51dWlkID0gIjQ1NzA4NmRkLThiZmItNDZkZC1hMzhkLTJmNGE2ZWZkMGU3ZSI7c2NyaXB0LmlkID0gInNhLWR5bmFtaWMtb3B0aW1pemF0aW9uLWxvYWRlciI7ZG9jdW1lbnQuaGVhZC5hcHBlbmRDaGlsZChzY3JpcHQpOw=="></script>
  
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
      line-height: 1.7;
      color: #1f2937;
      background: #ffffff;
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
    }
    .main-container {
      max-width: 1280px;
      margin: 0 auto;
      padding: 96px 16px 32px;
      margin-top: 64px;
    }
    @media (min-width: 640px) {
      .main-container {
        padding: 96px 24px 32px;
      }
    }
    @media (min-width: 1024px) {
      .main-container {
        padding: 96px 32px 48px;
      }
    }
    .content-wrapper {
      display: grid;
      grid-template-columns: 1fr;
      gap: 32px;
      margin-top: 32px;
    }
    @media (min-width: 1024px) {
      .content-wrapper {
        grid-template-columns: 8fr 4fr;
        gap: 32px;
      }
    }
    .main-content {
      display: flex;
      gap: 24px;
    }
    .social-sidebar {
      display: flex;
      flex-direction: column;
      gap: 12px;
      padding-top: 8px;
    }
    .social-icon {
      width: 40px;
      height: 40px;
      border-radius: 50%;
      background: #f3f4f6;
      display: flex;
      align-items: center;
      justify-content: center;
      transition: background 0.2s;
      text-decoration: none;
    }
    .social-icon:hover {
      background: #f3e8ff;
    }
    .article-content {
      flex: 1;
    }
    article {
      background: #ffffff;
    }
    header {
      margin-bottom: 32px;
    }
    h1 {
      color: #581c87;
      font-size: 2.25rem;
      font-weight: 700;
      line-height: 1.2;
      margin-bottom: 24px;
    }
    @media (min-width: 768px) {
      h1 {
        font-size: 3rem;
      }
    }
    @media (min-width: 1024px) {
      h1 {
        font-size: 3.75rem;
      }
    }
    h2 {
      color: #581c87;
      font-size: 1.875rem;
      font-weight: 700;
      margin-top: 40px;
      margin-bottom: 20px;
      line-height: 1.2;
    }
    @media (min-width: 768px) {
      h2 {
        font-size: 2.25rem;
      }
    }
    h3 {
      color: #581c87;
      font-size: 1.5rem;
      font-weight: 700;
      margin-top: 32px;
      margin-bottom: 16px;
      line-height: 1.2;
    }
    @media (min-width: 768px) {
      h3 {
        font-size: 1.875rem;
      }
    }
    h4, h5, h6 {
      color: #581c87;
      font-weight: 600;
      margin-top: 24px;
      margin-bottom: 12px;
    }
    p {
      margin-bottom: 24px;
      font-size: 1.125rem;
      line-height: 1.75;
      color: #1f2937;
    }
    .author-info {
      display: flex;
      align-items: center;
      gap: 16px;
      margin-bottom: 16px;
    }
    .author-avatar {
      width: 40px;
      height: 40px;
      border-radius: 50%;
      background: #e9d5ff;
      display: flex;
      align-items: center;
      justify-content: center;
      color: #9333ea;
      font-weight: 600;
      font-size: 1rem;
    }
    .author-details {
      display: flex;
      flex-direction: column;
    }
    .author-name {
      font-weight: 600;
      color: #111827;
      font-size: 1rem;
    }
    .author-meta {
      font-size: 0.875rem;
      color: #6b7280;
    }
    .category-tag {
      display: inline-block;
      padding: 4px 12px;
      background: #dbeafe;
      color: #1e40af;
      border-radius: 6px;
      font-size: 0.875rem;
      font-weight: 600;
      margin-top: 16px;
    }
    code {
      background: #f3f4f6;
      color: #9333ea;
      padding: 0.2em 0.4em;
      border-radius: 4px;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', 'Consolas', 'source-code-pro', monospace;
      font-size: 0.9em;
      border: 1px solid #e5e7eb;
    }
    pre {
      background: #1f2937;
      color: #f9fafb;
      padding: 1.5rem;
      border-radius: 8px;
      overflow-x: auto;
      margin: 1.5rem 0;
      border: 1px solid #374151;
    }
    pre code {
      background: none;
      color: #f9fafb;
      padding: 0;
      border: none;
      font-size: 0.875rem;
    }
    a {
      color: #9333ea;
      text-decoration: none;
      font-weight: 500;
      transition: color 0.2s;
    }
    a:hover {
      color: #7c3aed;
      text-decoration: underline;
    }
    ul, ol {
      margin-bottom: 24px;
      padding-left: 24px;
      font-size: 1.125rem;
      line-height: 1.75;
    }
    li {
      margin-bottom: 12px;
      color: #1f2937;
    }
    blockquote {
      border-left: 4px solid #a855f7;
      padding-left: 24px;
      margin: 32px 0;
      color: #374151;
      font-style: italic;
      font-size: 1.125rem;
      line-height: 1.75;
    }
    img {
      max-width: 100%;
      height: auto;
      border-radius: 8px;
      margin: 2rem 0;
      box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
    }
    .table-wrapper {
      width: 100%;
      overflow-x: auto;
      margin: 2rem 0;
      -webkit-overflow-scrolling: touch;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 0;
      font-size: 1rem;
      background: #ffffff;
      border: 2px solid #d1d5db;
      border-radius: 8px;
      overflow: hidden;
      box-shadow: 0 1px 3px 0 rgba(0, 0, 0, 0.1);
    }
    thead {
      background: #faf5ff;
    }
    th {
      padding: 1rem;
      text-align: left;
      font-weight: 600;
      color: #581c87;
      border-right: 1px solid #e9d5ff;
      border-bottom: 2px solid #c084fc;
      background: #faf5ff;
      font-size: 0.9375rem;
    }
    th:first-child {
      border-left: none;
    }
    th:last-child {
      border-right: none;
    }
    td {
      padding: 1rem;
      text-align: left;
      border-right: 1px solid #e5e7eb;
      border-bottom: 1px solid #e5e7eb;
      vertical-align: top;
      color: #1f2937;
      line-height: 1.6;
    }
    td:first-child {
      border-left: none;
    }
    td:last-child {
      border-right: none;
    }
    tbody tr:last-child td {
      border-bottom: none;
    }
    tbody tr:nth-child(even) {
      background: #f9fafb;
    }
    tbody tr:hover {
      background: #f3e8ff;
    }
    @media (max-width: 768px) {
      .table-wrapper {
        margin: 1.5rem 0;
      }
      table {
        font-size: 0.875rem;
      }
      th, td {
        padding: 0.75rem;
      }
    }
    hr {
      border: none;
      border-top: 2px solid #e5e7eb;
      margin: 3rem 0;
    }
    .content {
      font-size: 1.125rem;
      line-height: 1.75;
      color: #1f2937;
    }
    .promotional-section {
      margin-top: 48px;
      padding-top: 32px;
      border-top: 1px solid #e5e7eb;
    }
    .promotional-section p {
      color: #1f2937;
      font-size: 1.125rem;
      line-height: 1.75;
      margin-bottom: 12px;
    }
    .profile-section {
      display: flex;
      flex-direction: column;
      gap: 24px;
      padding-bottom: 32px;
      border-bottom: 1px solid #e5e7eb;
      margin-top: 32px;
    }
    @media (min-width: 640px) {
      .profile-section {
        flex-direction: row;
      }
    }
    .profile-image {
      flex-shrink: 0;
      width: 128px;
      height: 128px;
      border-radius: 8px;
      object-fit: cover;
      border: 1px solid #e5e7eb;
    }
    .profile-content {
      flex: 1;
    }
    .profile-placeholder-arvind {
      display: none;
      width: 128px;
      height: 128px;
      border-radius: 8px;
      background: #f3e8ff;
      border: 1px solid #e5e7eb;
      align-items: center;
      justify-content: center;
      color: #9333ea;
      font-weight: 700;
      font-size: 2rem;
      flex-shrink: 0;
    }
    .profile-placeholder-arvind.show {
      display: flex;
    }
    .profile-name {
      font-size: 1.5rem;
      font-weight: 700;
      color: #581c87;
      margin-bottom: 8px;
    }
    .profile-bio {
      color: #1f2937;
      font-size: 1rem;
      line-height: 1.75;
      margin-bottom: 16px;
    }
    .profile-bio p {
      margin-bottom: 16px;
      font-size: 1rem;
    }
    .linkedin-link {
      display: inline-flex;
      align-items: center;
      color: #9333ea;
      text-decoration: none;
      transition: color 0.2s;
    }
    .linkedin-link:hover {
      color: #7c3aed;
    }
    footer {
      margin-top: 64px;
      padding-top: 32px;
      border-top: 1px solid #e5e7eb;
      color: #6b7280;
      font-size: 0.95rem;
      text-align: center;
    }
    footer a {
      color: #9333ea;
      font-weight: 600;
    }
    .sidebar {
      display: none;
    }
    @media (min-width: 1024px) {
      .sidebar {
        display: block;
      }
    }
    .sidebar-content {
      display: flex;
      flex-direction: column;
      gap: 24px;
      position: sticky;
      top: 96px;
    }
    .sidebar-card {
      background: #faf5ff;
      border-radius: 12px;
      padding: 24px;
      border: 1px solid #e9d5ff;
    }
    .sidebar-card h3 {
      font-size: 1.125rem;
      font-weight: 700;
      color: #581c87;
      margin-bottom: 16px;
      margin-top: 0;
    }
    .signup-form {
      display: flex;
      flex-direction: column;
      gap: 12px;
    }
    .signup-form input {
      width: 100%;
      padding: 12px 16px;
      border-radius: 8px;
      border: 1px solid #d1d5db;
      font-size: 1rem;
    }
    .signup-form input:focus {
      outline: none;
      border-color: #9333ea;
      box-shadow: 0 0 0 3px rgba(147, 51, 234, 0.1);
    }
    .signup-form button {
      width: 100%;
      padding: 12px;
      background: linear-gradient(to right, #6b21a8, #581c87);
      color: white;
      font-weight: 600;
      border-radius: 8px;
      border: none;
      cursor: pointer;
      transition: all 0.2s;
    }
    .signup-form button:hover {
      background: linear-gradient(to right, #581c87, #4c1d95);
      box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
    }
    .related-content-title {
      font-size: 0.875rem;
      font-weight: 700;
      color: #111827;
      text-transform: uppercase;
      letter-spacing: 0.05em;
      margin-bottom: 16px;
      margin-top: 0;
    }
    .related-posts-list {
      list-style: none;
      padding: 0;
      margin: 0;
      display: flex;
      flex-direction: column;
      gap: 12px;
    }
    .related-posts-list li {
      margin: 0;
    }
    .related-post-link {
      color: #2563eb;
      text-decoration: underline;
      font-size: 0.875rem;
      line-height: 1.5;
      display: block;
      transition: color 0.2s;
    }
    .related-post-link:hover {
      color: #1e40af;
    }
    .view-more-link {
      display: flex;
      align-items: center;
      gap: 4px;
      margin-top: 16px;
      color: #9333ea;
      font-size: 0.875rem;
      font-weight: 500;
      text-decoration: none;
      transition: color 0.2s;
    }
    .view-more-link:hover {
      color: #7c3aed;
    }
    @media (max-width: 768px) {
      .main-container {
        padding: 80px 16px 32px;
      }
      h1 {
        font-size: 2rem;
      }
      h2 {
        font-size: 1.75rem;
      }
      h3 {
        font-size: 1.25rem;
      }
      p, ul, ol {
        font-size: 1rem;
      }
      .social-sidebar {
        display: none;
      }
      .main-content {
        flex-direction: column;
      }
    }
    .navbar {
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      width: 100%;
      background: rgba(255, 255, 255, 0.98);
      backdrop-filter: blur(12px);
      border-bottom: 1px solid rgba(229, 231, 235, 0.8);
      box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
      z-index: 50;
    }
    .navbar-container {
      max-width: 1280px;
      margin: 0 auto;
      padding: 0 16px;
    }
    @media (min-width: 640px) {
      .navbar-container {
        padding: 0 24px;
      }
    }
    @media (min-width: 1024px) {
      .navbar-container {
        padding: 0 32px;
      }
    }
    .navbar-content {
      display: flex;
      justify-content: space-between;
      align-items: center;
      height: 64px;
    }
    .navbar-logo {
      display: flex;
      align-items: center;
      gap: 8px;
      text-decoration: none;
      color: inherit;
      padding: 6px 8px;
      border-radius: 8px;
      transition: background 0.2s;
    }
    .navbar-logo:hover {
      background: #f9fafb;
    }
    .navbar-logo-icon {
      width: auto;
      height: 32px;
      max-height: 32px;
      object-fit: contain;
    }
    .navbar-logo-text {
      font-size: 1.25rem;
      font-weight: 700;
      background: linear-gradient(to right, #6b21a8, #7c3aed);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }
    .navbar-links {
      display: none;
      align-items: center;
      gap: 4px;
    }
    @media (min-width: 1024px) {
      .navbar-links {
        display: flex;
      }
    }
    .navbar-link {
      padding: 8px 16px;
      font-size: 0.875rem;
      font-weight: 500;
      color: #7c3aed;
      text-decoration: none;
      border-radius: 8px;
      transition: all 0.2s;
    }
    .navbar-link:hover {
      color: #581c87;
      background: #f9fafb;
    }
    .navbar-link.active {
      color: #7c3aed;
      background: #faf5ff;
    }
    .navbar-actions {
      display: none;
      align-items: center;
      gap: 10px;
      margin-left: 16px;
      padding-left: 16px;
      border-left: 1px solid #e5e7eb;
    }
    @media (min-width: 1024px) {
      .navbar-actions {
        display: flex;
      }
    }
    .navbar-button {
      padding: 8px 16px;
      font-size: 0.875rem;
      font-weight: 500;
      border-radius: 8px;
      border: none;
      cursor: pointer;
      transition: all 0.2s;
      text-decoration: none;
      display: inline-flex;
      align-items: center;
      gap: 6px;
    }
    .navbar-button-primary {
      background: linear-gradient(to right, #6b21a8, #581c87);
      color: white;
      font-weight: 600;
      box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
    }
    .navbar-button-primary:hover {
      background: linear-gradient(to right, #581c87, #4c1d95);
      box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
    }
    .navbar-button-secondary {
      color: #7c3aed;
      background: transparent;
    }
    .navbar-button-secondary:hover {
      color: #581c87;
      background: #f9fafb;
    }
    .navbar-button-playground {
      background: #7c3aed;
      color: white;
      font-weight: 600;
    }
    .navbar-button-playground:hover {
      background: #6b21a8;
    }
    .mobile-menu-button {
      display: flex;
      align-items: center;
      justify-content: center;
      width: 40px;
      height: 40px;
      border: none;
      background: transparent;
      cursor: pointer;
      color: #374151;
    }
    @media (min-width: 1024px) {
      .mobile-menu-button {
        display: none;
      }
    }
  </style>
</head>
<body>
  <!-- Navbar -->
  <nav class="navbar">
    <div class="navbar-container">
      <div class="navbar-content">
        <!-- Logo -->
        <a href="/" class="navbar-logo">
          <img src="/logos/alertmend-logo.svg" alt="AlertMend AI" class="navbar-logo-icon" onerror="this.style.display='none'; this.nextElementSibling.style.display='inline'; this.parentElement.querySelector('.navbar-logo-text').style.display='inline';" />
          <svg class="navbar-logo-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24" style="display: none;">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 10V3L4 14h7v7l9-11h-7z" />
          </svg>
          <span class="navbar-logo-text" style="display: none;">AlertMend AI</span>
        </a>

        <!-- Desktop Navigation -->
        <div class="navbar-links">
          <a href="/#how-it-works" class="navbar-link">How It Works</a>
          <a href="/#solutions" class="navbar-link">Solutions</a>
          <a href="/#benefits" class="navbar-link">Benefits</a>
          <a href="/case-studies" class="navbar-link">Case Studies</a>
          <a href="/blog" class="navbar-link active">Blog</a>
          <a href="/pricing" class="navbar-link">Pricing</a>
        </div>

        <!-- Desktop Actions -->
        <div class="navbar-actions">
          <a href="https://demo.alertmend.io/playground" target="_blank" rel="noopener noreferrer" class="navbar-button navbar-button-playground">
            <svg width="16" height="16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M14.752 11.168l-3.197-2.132A1 1 0 0010 9.87v4.263a1 1 0 001.555.832l3.197-2.132a1 1 0 000-1.664z" />
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
            </svg>
            Playground
          </a>
          <a href="https://demo.alertmend.io/signup" target="_blank" rel="noopener noreferrer" class="navbar-button navbar-button-secondary">Register</a>
          <a href="https://calendly.com/hello-alertmend/30min" target="_blank" rel="noopener noreferrer" class="navbar-button navbar-button-primary">Book a Demo</a>
        </div>

        <!-- Mobile Menu Button -->
        <button class="mobile-menu-button" aria-label="Toggle menu">
          <svg width="24" height="24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16" />
          </svg>
        </button>
      </div>
    </div>
  </nav>

  <div class="main-container">
    <div class="content-wrapper">
      <!-- Main Content Area (70%) -->
      <div class="main-content">
        <!-- Social Share Icons (Left Sidebar) -->
        <div class="social-sidebar">
          <a href="#" class="social-icon" aria-label="Share on Facebook">
            <svg width="20" height="20" fill="currentColor" viewBox="0 0 24 24"><path d="M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z"/></svg>
          </a>
          <a href="#" class="social-icon" aria-label="Share on Twitter">
            <svg width="20" height="20" fill="currentColor" viewBox="0 0 24 24"><path d="M23.953 4.57a10 10 0 01-2.825.775 4.958 4.958 0 002.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 00-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 00-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 01-2.228-.616v.06a4.923 4.923 0 003.946 4.827 4.996 4.996 0 01-2.212.085 4.936 4.936 0 004.604 3.417 9.867 9.867 0 01-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 007.557 2.209c9.053 0 13.998-7.496 13.998-13.985 0-.21 0-.42-.015-.63A9.935 9.935 0 0024 4.59z"/></svg>
          </a>
          <a href="#" class="social-icon" aria-label="Share on LinkedIn">
            <svg width="20" height="20" fill="currentColor" viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
          </a>
          <a href="#" class="social-icon" aria-label="Copy link">
            <svg width="20" height="20" fill="currentColor" viewBox="0 0 24 24"><path d="M8.465 11.293c1.133-1.133 3.109-1.133 4.242 0l.707.707 1.414-1.414-.707-.707c-1.498-1.498-3.94-1.498-5.439 0l-.707.707 1.414 1.414.707-.707zm-2.829 2.829l.707.707c1.498 1.498 3.94 1.498 5.439 0l.707-.707-1.414-1.414-.707.707c-1.133 1.133-3.109 1.133-4.242 0l-.707-.707-1.414 1.414zm11.314-8.485l-6.364 6.364c-.39.39-1.023.39-1.414 0s-.39-1.023 0-1.414l6.364-6.364c.39-.39 1.023-.39 1.414 0s.39 1.023 0 1.414z"/></svg>
          </a>
        </div>

        <!-- Article Content -->
        <div class="article-content">
          <article>
            <header>
              <h1>exit code 1 kubernetes</h1>
              
              <!-- Author Info -->
              <div class="author-info">
                <div class="author-avatar">
                  A
                </div>
                <div class="author-details">
                  <div class="author-name">AlertMend Team</div>
                  <div class="author-meta">52 min read • December 18, 2025</div>
                </div>
              </div>

              <!-- Category Tag -->
              <div class="category-tag">Kubernetes</div>
            </header>

            <!-- Content -->
            <div class="content">
              <h2>exit code 1 kubernetes</h2>
<p>REFERENCE CONTENT FROM TOP 8 GOOGLE SEARCH RESULTS</p>
<p>This content is gathered from the top-ranking pages for comprehensive reference.
Sources:</p>
<ol>
<li><a href="https://alertmend.io.com/learn/exit-codes-in-containers-and-kubernetes-the-complete-guide/">https://alertmend.io.com/learn/exit-codes-in-containers-and-kubernetes-the-complete-guide/</a></li>
<li><a href="https://github.com/nuclio/nuclio/issues/2029">https://github.com/nuclio/nuclio/issues/2029</a></li>
<li><a href="https://stackoverflow.com/questions/74713374/for-kubernetes-pods-how-to-discover-cause-of-exit-code-2">https://stackoverflow.com/questions/74713374/for-kubernetes-pods-how-to-discover-cause-of-exit-code-2</a></li>
<li><a href="https://www.linkedin.com/posts/donald-lutz-5a9b0b2_exit-codes-in-containers-kubernetes-complete-activity-6872604824941199360-TIMc">https://www.linkedin.com/posts/donald-lutz-5a9b0b2_exit-codes-in-containers-kubernetes-complete-activity-6872604824941199360-TIMc</a></li>
<li><a href="https://qiita.com/naomichi-y/items/b18eb09d016a6bf33568">https://qiita.com/naomichi-y/items/b18eb09d016a6bf33568</a></li>
<li><a href="https://hackernoon.com/top-5-kubernetes-coding-errors-and-how-to-solve-them">https://hackernoon.com/top-5-kubernetes-coding-errors-and-how-to-solve-them</a></li>
<li><a href="https://mickael-baron.fr/blog/2024/07/19/guide-deploying-nvidiagpu-k8s">https://mickael-baron.fr/blog/2024/07/19/guide-deploying-nvidiagpu-k8s</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1644610">https://cloud.tencent.com/developer/article/1644610</a></li>
</ol>
<p>The following sections contain content from each source, organized for reference.
utilize this information to comprehend the topic comprehensively, identify key points,
related keywords, and best practices. Then create original, SEO-optimized content
that synthesizes insights from all sources while using completely original wording.</p>
<p>Home Learning Center Exit Codes in Docker and Kubernetes: The Complete Guide Exit Codes in Docker and Kubernetes: The Complete Guide Nir Shtein, Software Engineer 14 min read November 2nd, 2021 Kubernetes Troubleshooting What are Container Exit Codes Exit codes are used by container engines, when a container terminates, to report why it was terminated. If you represent Kubernetes user, container failures are one of the most common causes of pod exceptions, and understanding container exit codes can assist you obtain to the root cause of pod failures when troubleshooting. The most common exit codes used by containers are: Code # Name What it means Exit Code 0 Purposely stopped Used by developers to indicate that the container was automatically stopped Exit Code 1 Application error Container was stopped due to application error or incorrect reference in the image specification Exit Code 125 Container failed to run error The docker run command did not execute successfully Exit Code 126 Command invoke error A command specified in the image specification could not be invoked Exit Code 127 File or directory not found File or directory specified in the image specification was not found Exit Code 128 Invalid argument used on exit Exit was triggered with an invalid exit code (valid codes are integers between 0-255) Exit Code 134 Abnormal termination (SIGABRT) The container aborted itself using the abort() function. Exit Code 137 Immediate termination ( SIGKILL ) Container was immediately terminated by the operating system via SIGKILL signal Exit Code 139 Segmentation fault ( SIGSEGV ) Container attempted to access memory that was not assigned to it and was terminated Exit Code 143 Graceful termination ( SIGTERM ) Container received warning that it was about to be terminated, then terminated Exit Code 255 Exit Status Out Of Range Container exited, returning an exit code outside the acceptable range, meaning the cause of the error is not known Below we’ll elaborate on how to troubleshoot failed containers on a self-managed host and in Kubernetes, and provide more details on all of the exit codes listed above. This is part of an extensive series of guides about Observability. The Container Lifecycle To better comprehend the causes of container failure, let’s discuss the lifecycle of a container first. Taking Docker as an example – at any given time, a Docker container may be in one of several states: Created – the Docker container is created but not started yet (this is the status after running docker create, but before actually running the container) Up – the Docker container is currently running. This means the operating system process managed by the container is running. This happens when you utilize the commands docker begin or docker run can happen using docker begin or docker run. Paused – the container process was running, but Docker purposely paused the container. Typically this happens when you run the Docker pause command Exited – the Docker container was terminated, usually because the container’s process was killed When a container reaches the Exited status, Docker will report an exit code in the logs, to inform you what happened to the container that caused it to shut down. Tips from the expert Itiel Shwartz Co-Founder &amp; CTO Itiel is the CTO and co-founder of alertmend.io. He’s a big believer in dev empowerment and moving fast, has worked at eBay, Forter and Rookout (as the founding engineer). Itiel represents backend and infra developer turned “DevOps”, an avid public speaker that loves talking about things such as cloud infrastructure, Kubernetes, Python, observability, and R&amp;D culture. In my experience, here are tips that can assist you better handle exit codes in containers and Kubernetes: Monitor resource limits Set appropriate resource limits to prevent OOMKilled (exit code 137) and other resource-related issues. utilize liveness and readiness probes Implement these probes to detect and respond to container failures automatically. Leverage container restart policies Configure restart policies to handle transient issues without manual intervention. Enable detailed logging Collect and analyze logs for all containers to quickly identify the root cause of failures. utilize health checks Implement health checks in your containerized applications to detect and resolve issues proactively. Understanding Docker Container Exit Codes Below we cover each of the exit codes in more detail. Exit Code 0: Purposely Stopped Exit Code 0 is triggered by developers when they purposely cease their container after a task completes. Technically, Exit Code 0 means that the foreground process is not attached to a specific container. What to do if a container terminated with Exit Code 0? Check the container logs to identify which library caused the container to exit Review the code of the existing library and identify why it triggered Exit Code 0, and whether it is functioning correctly Exit Code 1: Application Error Exit Code 1 indicates that the container was stopped due to one of the following: An application error – this could be a simple programming error in code run by the container, such as “divide by zero”, or advanced errors related to the runtime environment, such as Java, Python, etc An invalid reference – this means the image specification refers to a file that does not exist in the container image What to do if a container terminated with Exit Code 1? Check the container log to observe if one of the files listed in the image specification could not be found. If this is the issue, correct the image specification to point to the correct path and filename. If you cannot discover an incorrect file reference, check the container logs for an application error, and debug the library that caused the error. Exit Code 125: Container Failed to Run Exit Code 125 means that the command is used to run the container. For example docker run was invoked in the system shell but did not execute successfully. Here are common reasons this might happen: An undefined flag was used in the command, for example docker run --abcd The user-defined in the image specification does not have sufficient permissions on the machine Incompatibility between the container engine and the host operating system or hardware What to do if a container terminated with Exit Code 125? Check if the command used to run the container uses the proper syntax Check if the user running the container, or the context in which the command is executed in the image specification, has sufficient permissions to create containers on the host If your container engine provides other options for running a container, attempt them. For example, in Docker, attempt docker begin instead of docker run Test if you are able to run other containers on the host using the same username or context. If not, reinstall the container engine, or resolve the underlying compatibility issue between the container engine and the host setup Exit Code 126: Command Invoke Error Exit Code 126 means that a command used in the container specification could not be invoked. This is often the cause of a missing dependency or an error in a continuous integration script used to run the container. What to do if a container terminated with Exit Code 126? Check the container logs to observe which command could not be invoked attempt running the container specification without the command to ensure you isolate the problem Troubleshoot the command to ensure you are using the correct syntax and all dependencies are available Correct the container specification and retry running the container Exit Code 127: File or Directory Not Found Exit Code 127 means a command specified in the container specification refers to a non-existent file or directory. What to do if a container terminated with Exit Code 127? Same as Exit Code 126, identify the failing command and create sure you reference a valid filename and file path available within the container image. Learn more in our detailed guide to exit code 127. Exit Code 128: Invalid Argument Used on Exit Exit Code 128 means that code within the container triggered an exit command, but did not provide a valid exit code. The Linux exit command only allows integers between 0-255, so if the process was exited with, for example, exit code 3. 5 , the logs will report Exit Code 128. What to do if a container terminated with Exit Code 128? Check the container logs to identify which library caused the container to exit. Identify where the offending library uses the exit command, and correct it to provide a valid exit code. Exit Code 134: Abnormal Termination (SIGABRT) Exit Code 134 means that the container abnormally terminated itself, closed the process and flushed open streams. This operation is irreversible, like SIGKILL (observe Exit Code 137 below). A process can trigger SIGABRT by doing one of the following: Calling the abort() function in the libc library Calling the assert() macro, used for debugging. The process is then aborted if the assertion is false. What to do if a container terminated with Exit Code 134? Check container logs to observe which library triggered the SIGABRT signal Check if process abortion was planned (for example because the library was in debug mode), and if not, troubleshoot the library and modify it to avoid aborting the container. Exit Code 137: Immediate Termination (SIGKILL) Exit Code 137 means that the container has received a SIGKILL signal from the host operating system. This signal instructs a process to terminate immediately, with no grace period. This may be either: Triggered when a container is killed via the container engine, for example when using the docker kill command Triggered by a Linux user sending a kill -9 command to the process Triggered by Kubernetes after attempting to terminate a container and waiting for a grace period of 30 seconds (by default) Triggered automatically by the host, usually due to running out of memory. In this case, the docker inspect command will indicate an OOMKilled error. Learn more in our detailed guide to exit code 137. What to do if a container terminated with Exit Code 137? Check logs on the host to observe what happened prior to the container terminating, and whether it previously received a SIGTERM signal (graceful termination) before receiving SIGKILL If there was a prior SIGTERM signal, check if your container process handles SIGTERM and is able to gracefully terminate If there was no SIGTERM and the container reported an OOMKilled error, troubleshoot memory issues on the host Learn more in our detailed guide to the SIGKILL signal &gt;&gt; Exit Code 139: Segmentation Fault (SIGSEGV) Exit Code 139 means that the container received a SIGSEGV signal from the operating system. This indicates a segmentation error – a memory violation, caused by a container trying to access a memory location to which it does not have access. There are three common causes of SIGSEGV errors: Coding error —container process did not initialize properly, or it tried to access memory through a pointer to previously freed memory Incompatibility between binaries and libraries —container process runs a binary file that is not compatible with a shared library, and thus may attempt to access inappropriate memory addresses Hardware incompatibility or misconfiguration —if you observe multiple segmentation errors across multiple libraries, there may be a problem with memory subsystems on the host or a system configuration issue What to do if a container terminated with Exit Code 139? Check if the container process handles SIGSEGV. On both Linux and Windows, you can handle a container’s response to segmentation violations. For example, the container can collect and report a stack trace If you require to further troubleshoot SIGSEGV, you may require to set the operating system to allow programs to run even after a segmentation fault occurs, to allow for investigation and debugging. Then, attempt to intentionally cause a segmentation violation and debug the library causing the issue If you cannot replicate the issue, check memory subsystems on the host and troubleshoot memory configuration Learn more in our detailed guide to the SIGSEGV signal &gt;&gt; Exit Code 143: Graceful Termination (SIGTERM) Exit Code 143 means that the container received a SIGTERM signal from the operating system, which asks the container to gracefully terminate, and the container succeeded in gracefully terminating (otherwise you will observe Exit Code 137). This exit code may be: Triggered by the container engine stopping the container, for example when using the docker cease or docker-compose down commands Triggered by Kubernetes setting a pod to Terminating status, and giving containers a 30 second period to gracefully shut down What to do if a container terminated with Exit Code 143? Check host logs to observe the context in which the operating system sent the SIGTERM signal. If you are using Kubernetes, check the kubelet logs to observe if and when the pod was shut down. In general, Exit Code 143 does not require troubleshooting. It means the container was properly shut down after being instructed to do so by the host. Learn more in our detailed guide to the SIGTERM signal &gt;&gt; Exit Code 1: Application Error Exit Code 1 indicates that the container was stopped due to one of the following: An application error – this could be a simple programming error in code run by the container, such as “divide by zero”, or advanced errors related to the runtime environment, such as Java, Python, etc An invalid reference – this means the image specification refers to a file that does not exist in the container image What to do if a container terminated with Exit Code 1? Check the container log to observe if one of the files listed in the image specification could not be found. If this is the issue, correct the image specification to point to the correct path and filename. If you cannot discover an incorrect file reference, check the container logs for an application error, and debug the library that caused the error. Exit Code 125 Exit Code 125: Container Failed to Run Exit Code 125 means that the command is used to run the container. For example docker run was invoked in the system shell but did not execute successfully. Here are common reasons this might happen: An undefined flag was used in the command, for example docker run --abcd The user-defined in the image specification does not have sufficient permissions on the machine Incompatibility between the container engine and the host operating system or hardware What to do if a container terminated with Exit Code 125? Check if the command used to run the container uses the proper syntax Check if the user running the container, or the context in which the command is executed in the image specification, has sufficient permissions to create containers on the host If your container engine provides other options for running a container, attempt them. For example, in Docker, attempt docker begin instead of docker run Test if you are able to run other containers on the host using the same username or context. If not, reinstall the container engine, or resolve the underlying compatibility issue between the container engine and the host setup Exit Code 126: Command Invoke Error Exit Code 126 means that a command used in the container specification could not be invoked. This is often the cause of a missing dependency or an error in a continuous integration script used to run the container. What to do if a container terminated with Exit Code 126? Check the container logs to observe which command could not be invoked attempt running the container specification without the command to ensure you isolate the problem Troubleshoot the command to ensure you are using the correct syntax and all dependencies are available Correct the container specification and retry running the container Exit Code 127: File or Directory Not Found Exit Code 127 means a command specified in the container specification refers to a non-existent file or directory. What to do if a container terminated with Exit Code 127? Same as Exit Code 126 above, identify the failing command and create sure you reference a valid filename and file path available within the container image. Exit Code 128: Invalid Argument Used on Exit Exit Code 128 means that code within the container triggered an exit command, but did not provide a valid exit code. The Linux exit command only allows integers between 0-255, so if the process was exited with, for example, exit code 3. 5 , the logs will report Exit Code 128. What to do if a container terminated with Exit Code 128? Check the container logs to identify which library caused the container to exit. Identify where the offending library uses the exit command, and correct it to provide a valid exit code. Exit Code 134: Abnormal Termination (SIGABRT) Exit Code 134 means that the container abnormally terminated itself, closed the process and flushed open streams. This operation is irreversible, like SIGKILL (observe Exit Code 137 below). A process can trigger SIGABRT by doing one of the following: Calling the abort() function in the libc library Calling the assert() macro, used for debugging. The process is then aborted if the assertion is false. What to do if a container terminated with Exit Code 134? Check container logs to observe which library triggered the SIGABRT signal Check if process abortion was planned (for example because the library was in debug mode), and if not, troubleshoot the library and modify it to avoid aborting the container. Exit Code 137: Immediate Termination (SIGKILL) Exit Code 137 means that the container has received a SIGKILL signal from the host operating system. This signal instructs a process to terminate immediately, with no grace period. This may be either: Triggered when a container is killed via the container engine, for example when using the docker kill command Triggered by a Linux user sending a kill -9 command to the process Triggered by Kubernetes after attempting to terminate a container and waiting for a grace period of 30 seconds (by default) Triggered automatically by the host, usually due to running out of memory. In this case, the docker inspect command will indicate an OOMKilled error. What to do if a container terminated with Exit Code 137? Check logs on the host to observe what happened prior to the container terminating, and whether it previously received a SIGTERM signal (graceful termination) before receiving SIGKILL If there was a prior SIGTERM signal, check if your container process handles SIGTERM and is able to gracefully terminate If there was no SIGTERM and the container reported an OOMKilled error, troubleshoot memory issues on the host Learn more in our detailed guide to the SIGKILL signal &gt;&gt; Exit Code 139: Segmentation Fault (SIGSEGV) Exit Code 139 means that the container received a SIGSEGV signal from the operating system. This indicates a segmentation error – a memory violation, caused by a container trying to access a memory location to which it does not have access. There are three common causes of SIGSEGV errors: Coding error —container process did not initialize properly, or it tried to access memory through a pointer to previously freed memory Incompatibility between binaries and libraries —container process runs a binary file that is not compatible with a shared library, and thus may attempt to access inappropriate memory addresses Hardware incompatibility or misconfiguration —if you observe multiple segmentation errors across multiple libraries, there may be a problem with memory subsystems on the host or a system configuration issue What to do if a container terminated with Exit Code 139? Check if the container process handles SIGSEGV. On both Linux and Windows, you can handle a container’s response to segmentation violations. For example, the container can collect and report a stack trace If you require to further troubleshoot SIGSEGV, you may require to set the operating system to allow programs to run even after a segmentation fault occurs, to allow for investigation and debugging. Then, attempt to intentionally cause a segmentation violation and debug the library causing the issue If you cannot replicate the issue, check memory subsystems on the host and troubleshoot memory configuration Learn more in our detailed guide to the SIGSEGV signal &gt;&gt; Exit Code 143: Graceful Termination (SIGTERM) Exit Code 143 means that the container received a SIGTERM signal from the operating system, which asks the container to gracefully terminate, and the container succeeded in gracefully terminating (otherwise you will observe Exit Code 137). This exit code may be: Triggered by the container engine stopping the container, for example when using the docker cease or docker-compose down commands Triggered by Kubernetes setting a pod to Terminating status, and giving containers a 30 second period to gracefully shut down What to do if a container terminated with Exit Code 143? Check host logs to observe the context in which the operating system sent the SIGTERM signal. If you are using Kubernetes, check the kubelet logs to observe if and when the pod was shut down. In general, Exit Code 143 does not require troubleshooting. It means the container was properly shut down after being instructed to do so by the host. Learn more in our detailed guide to the SIGTERM signal &gt;&gt; Exit Code 255: Exit Status Out Of Range When you observe exit code 255, it implies the main entrypoint of a container stopped with that status. It means that the container stopped, but it is not known for what reason. What to do if a container terminated with Exit Code 255? If the container is running in a virtual machine, first attempt removing overlay networks configured on the virtual machine and recreating them. If this does not solve the problem, attempt deleting and recreating the virtual machine, then rerunning the container on it. Failing the above, bash into the container and examine logs or other clues about the entrypoint process and why it is failing. Which Kubernetes Errors are Related to Container Exit Codes? Whenever containers fail within a pod, or Kubernetes instructs a pod to terminate for any reason, containers will shut down with exit codes. Identifying the exit code can assist you comprehend the underlying cause of a pod exception. You can utilize the following command to view pod errors: kubectl describe pod [name] The result will look something like this: Containers: kubedns: Container ID:. State: Running Started: Fri, 15 Oct 2021 12:06:01 +0800 Last State: Terminated Reason: Error Exit Code: 255 Started: Fri, 15 Oct 2021 11:43:42 +0800 Finished: Fri, 15 Oct 2021 12:05:17 +0800 Ready: True Restart Count: 1 utilize the Exit Code provided by kubectl to troubleshoot the issue: If the Exit Code is 0 – the container exited normally, no troubleshooting is required If the Exit Code is between1-128 – the container terminated due to an internal error, such as a missing or invalid command in the image specification If the Exit Code is between 129-255 – the container was stopped as the result of an operating signal, such as SIGKILL or SIGINT If the Exit Code was exit(-1) or another value outside the 0-255 range, kubectl translates it to a value within the 0-255 range. Refer to the relevant section above to observe how to troubleshoot the container for each exit code. Troubleshooting Kubernetes Pod Termination with alertmend.io As a Kubernetes administrator or user, pods or containers terminating unexpectedly may be a pain and can result in severe production issues. The troubleshooting process in Kubernetes is complex and, without the right tools, may be stressful, ineffective, and time-consuming. Some best practices can assist minimize the chances of container failure affecting your applications, but eventually, something will go wrong—simply because it can. This is the reason why we created alertmend.io, a tool that helps dev and ops teams cease wasting their precious time looking for needles in (hay)stacks every time things go wrong. Acting as a single source of truth (SSOT) for all of your k8s troubleshooting needs, alertmend.io offers: Change intelligence: Every issue represents result of a change. Within seconds we can assist you comprehend exactly who did what and when. In-depth visibility: A complete activity timeline, showing all code and config changes, deployments, alerts, code diffs, pod logs and etc. All within one pane of glass with easy drill-down options. Insights into service dependencies: An easy way to comprehend cross-service changes and visualize their ripple effects across your entire system. Seamless notifications: Direct integration with your existing communication channels (e. , Slack) so you’ll have all the information you require, when you require it. observe Additional Guides on Key Observability Topics Together with our content partners, we have authored in-depth guides on several other topics that can also be useful as you explore the world of observability. eBPF Authored by Tigera [Guide] eBPF Explained: utilize Cases, Concepts, and Architecture [Guide] eBPF XDP: The Basics and a Quick Tutorial [Blog] Introducing the Calico eBPF Dataplane [Product] Tigera | Security and Observability for Containers and Kubernetes Cloud Security Authored by Tigera [Guide] Cloud Security: Challenges and 5 Technologies That Can assist [Guide] Micro-segmentation in the Cloud Native World [eBook] O’Reilly eBook: Kubernetes Security and Observability [Product] Tigera | Security and Observability for Containers and Kubernetes Git Errors Authored by alertmend.io [Guide] Common Git Errors, How to Fix, and 5 Ways to Avoid Them [Guide] Git Error: ‘failed to push some refs to’: Steps to Fix [Blog] Kubernetes health checks [Product] alertmend.io | Kubernetes Management and Troubleshooting Share: Latest Articles Kubernetes Certificates: A Practical Guide K8sGPT: Improving K8s Cluster Management with LLMs Top 7 Kubernetes GUI Tools in 2024</p>
<p>nuclio / nuclio Public Notifications You must be signed in to change notification settings Fork 557 Star 5. 6k Dashboard error on Kubernetes 1. x (AKS) #2029 New issue Copy link New issue Copy link Closed Closed Dashboard error on Kubernetes 1. x (AKS) #2029 Copy link Description turowicz opened on Jan 3, 2021 Issue body actions I obtain an error after creating a namespace, a secret and deploying the helm chart according to this guide: <a href="https://github">https://github</a>. com/nuclio/nuclio/tree/development/hack/k8s/helm/nuclio The dashboard pod logs: Running in parallel Starting dashboard Starting nginx 21. 508 �[0;37m dashboard. platform�[0m �[0;32m(D)�[0m Using kubeconfig {&quot;kubeconfigPath&quot;: &quot;&quot;} 21. runner�[0m �[0;32m(D)�[0m Executing {&quot;command&quot;: &quot;docker version&quot;} 21. runner�[0m �[0;32m(D)�[0m Failed to execute command {&quot;output&quot;: &quot;Client: Docker Engine - Community\n Version: 19. 10\n Git commit: 48a66213fe\n Built: Mon Jun 22 15:42:53 2020\n OS/Arch: linux/amd64\n Experimental: false\nCannot connect to the Docker daemon at unix:///var/run/docker. Is the docker daemon running?\n&quot;, &quot;stderr&quot;: &quot;&quot;, &quot;exitCode&quot;: 1, &quot;err&quot;: &quot;exit status 1&quot;} Error - exit status 1 /nuclio/pkg/cmdrunner/shellrunner. go:95 Call stack: stdout: Client: Docker Engine - Community Version: 19. 10 Git commit: 48a66213fe Built: Mon Jun 22 15:42:53 2020 OS/Arch: linux/amd64 Experimental: false Cannot connect to the Docker daemon at unix:///var/run/docker. Is the docker daemon running? stderr: /nuclio/pkg/cmdrunner/shellrunner. go:95 No docker client found /nuclio/pkg/dockerclient/shell. go:65 Failed to create docker client. /pkg/containerimagebuilderpusher/docker. go:31 Failed to create a Docker builder /nuclio/pkg/platform/kube/platform. go:117 Failed to create kube platform /nuclio/pkg/platform/factory/factory. go:69 parallel: This job failed: /runners/dashboard. sh Exiting Metadata Metadata Assignees No one assigned Labels No labels No labels Type No type Projects No projects Milestone No milestone Relationships None yet Development No branches or pull requests Issue actions</p>
<p>4 I have pods that are of kind Cronjob running in parallel. They complete their task and run again after a fixed interval of 20 minutes as per the cron expression. I noticed that some pods are restarting 2-3 times before completing their task. I checked for details with the kubectl describe pod command and found that the pods terminate with exit code 2 when it restart due to some error: Last State: Terminated Reason: Error Exit Code: 2 I searched about exit code 2 and found that it is misuse of a shell builtin commands. How I can discover which shell builtin is misused? How to debug the cause of exit code 2? docker kubernetes Share Improve this question Follow edited Feb 11, 2024 at 14:46 Alexis Wilke 21. 2k 11 11 gold badges 111 111 silver badges 183 183 bronze badges asked Dec 7, 2022 at 7:55 anujprashar 6,359 8 8 gold badges 55 55 silver badges 92 92 bronze badges 2 1 This is incredibly dependent on the code running inside the pod. There&#39;s nothing that special about the number 2 that would point at a single definite cause. Can you edit the question to include a minimal reproducible example ? David Maze – David Maze 2022-12-07 14:10:45 +00:00 Commented Dec 7, 2022 at 14:10 I found issue was with code after carefully going though code and application logs. There was null check that was causing this issue. Thanks for providing direction. anujprashar – anujprashar 2022-12-08 09:03:30 +00:00 Commented Dec 8, 2022 at 9:03 Add a comment | 2 Answers 2 Sorted by: Reset to default Highest score (default) Trending (recent votes count more) Date modified (newest first) Date created (oldest first) 3 An exit code of 2 indicates either that the application chose to return that error code, or (by convention) there was a misuse of a shell built-in. Check your pod’s command specification to ensure that the command is correct. If you think it is correct, attempt running the image locally with a shell and run the command directly. Refer to this link for more information. Share Improve this answer Follow edited Dec 7, 2022 at 10:23 Harsh Manvar 30. 6k 8 8 gold badges 64 64 silver badges 125 125 bronze badges answered Dec 7, 2022 at 10:18 Fariya Rahmat 3,330 7 7 silver badges 17 17 bronze badges Sign up to request clarification or add additional context in comments. Comments Add a comment 0 You can obtain logs with kubectl logs my-pod Post output here if you can&#39;t fix it. Share Improve this answer Follow answered Dec 7, 2022 at 8:14 Christoph Fischer 115 10 10 bronze badges 3 Comments Add a comment anujprashar anujprashar Over a year ago Thanks for reply. I have already check logs from kubectl logs. It only demonstrate application logs and nothing unusal in logs. 113Z+00:00 0 Reply Copy link Christoph Fischer Christoph Fischer Over a year ago What&#39;s your output for kubectl obtain pods ? 2022-12-07T09:15:50. 677Z+00:00 0 Reply Copy link Christoph Fischer Christoph Fischer Over a year ago Also, did you check this one? containersolutions. io/runbooks/posts/kubernetes/… 2022-12-07T09:16:31. 49Z+00:00 0 Reply Copy link Add a comment Your Answer Draft saved Draft discarded Sign up or log in Sign up using Google Sign up using Email and Password Submit Post as a guest Name Email Required, but never shown Post Your Answer Discard By clicking “Post Your Answer”, you agree to our terms of service and acknowledge you have read our privacy policy. begin asking to obtain answers discover the answer to your question by asking. Ask question Explore related questions docker kubernetes observe similar questions with these tags</p>
<p>Donald Lutz 4y Report this post Exit Codes in Containers and Kubernetes: The Complete Guide #containers #kubernetes #exitcodes #completeguide <a href="https://lnkd">https://lnkd</a>. in/ev7_bExd 3 Like Comment Share Copy LinkedIn Facebook X To view or add a comment, sign in</p>
<p>info More than 5 years have passed since last update. @ naomichi-y ( Naomichi Yamakita ) Docker環境でRails停止時にExit code 1が発生する Rails puma Docker kubernetes ECS Last updated at 2020-09-13 Posted at 2020-09-13 問題 Docker上でRails (Puma) を実行中に docker cease でコンテナを停止させようとするとExit 1 (SIGHUP) が発生します。 ECSやKubernetesを利用している場合、コンテナが正しく終了せず、予期しない問題を引き起こす可能性があります。 % docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES xxx rails &quot;bundle exec rails s…&quot; 44 seconds ago Exited (1) 3 seconds ago api xxx 原因 俗に言う「PID 1問題 1 」が原因。PID 1はinitプロセスと呼ばれるもので、システム起動時にカーネルによって呼び出される特別なプロセスです。initプロセスはシグナルハンドリングや子プロセスの生成、ゾンビプロセスの削除などを行います。 今回はRails ( 2 )を起動した際にPID 1が使われてしまい、シグナルを正しくハンドリングできないといった問題が発生していました。 # コンテナ上でtopコマンドを実行した結果 PID USER PR NI VIRT RES %CPU %MEM TIME+ S COMMAND 1 root 20 0 2. 02 S /bin/bash /bin/docker-entrypoint. sh bundle exec rails s -b 0. プロセスを見ると、PID 1でRailsが起動していることが分かります。 対策 tini や dumb-init といったプログラムを使うことで、PID 1の子プロセスとしてアプリケーションを起動することが可能となります。 docker-compose 3. 7以降が利用可能であれば、 docker-compose. yml に init パラメータを付けることで問題を回避することができます ( 3 )。 # Railsがinitプロセスの子プロセスとして起動する PID USER PR NI VIRT RES %CPU %MEM TIME+ S COMMAND 1 root 20 0 1. 03 S /sbin/docker-init -- /bin/docker-entrypoint. sh bundle exec rails s -b 0. 00 S `- /bin/bash /bin/docker-entrypoint. sh bundle exec rails s -b 0. 0 docker-compose down 実行後にプロセスを見ると、143 (SIGTERM) でコンテナが停止しています。 % docker-compose ps Name Command State Ports --- api /bin/docker-entrypoint. Exit 143 ECSを利用している場合 タスク定義に initProcessEnabled: true ( 4 ) を追加します。 initProcessEnabled は docker run の --init に相当します。 Kubernetesを利用している場合 tiniなどの軽量initを使う方法もありますが、Kubernetes 1. 17からは Share Process Namespace ( 5 ) を使うことで問題を回避できるようです。 Docker and the PID 1 zombie reaping problem ↩ Puma returns exit code 1 in container when received SIGTERM ↩ Compose ファイル バージョン 3 リファレンス - init ↩ AWS::ECS::TaskDefinition LinuxParameters ↩ Share Process Namespace between Containers in a Pod ↩ 25 Go to list of users who liked 12 comment 0 Go to list of comments Register as a new user and utilize Qiita more conveniently You obtain articles that match your needs You can efficiently read back useful information You can utilize dark theme What you can do with signing up Sign up Login</p>
<h2>MACHINE-LEARNING 10 Best Python Machine Learning Tutorials Gilad...</h2>
<p>A step-by-step pratical guide for deploying NVIDIA GPUs on Kubernetes 19 July 2024 - 27 mins read time Tags: kubernetes Prerequisites Kubernetes nodes configuration Client machine configuration Create a Kubernetes cluster Add GPU support to the Kubernetes cluster Deploying an Application Requiring a GPU Conclusion Resources This blog post guides you through the creation of a Kubernetes cluster with NVIDIA GPU resources. We will utilize the kubeadm deployment tool to setup the Kubernetes cluster. For the discovery and configuration of nodes with GPU cards, we will integrate GPU Operator. Finally, we will deploy the Ollama application on the cluster and verify that it correctly uses the GPU resources. This experiment was validated by the intern Frédéric Alleron, student in the Network and Telecom department at the IUT Châtellerault, who completed his internship from June to July 2024 at the LIAS laboratory. Prerequisites The hardware prerequisites to reproduce this experiment are: two Linux machines (my setup: Ubuntu 22. 04 LTS), one of which has at least one NVIDIA GPU (my setup: NVIDIA P400 with 2GB). The disk size and memory are not crucial, a client machine (my setup: macOS Sonoma) for accessing the Kubernetes cluster. Since the component GPU Operator , which allows the installation of NVIDIA GPU support on Kubernetes, does not support version 24. 04 LTS ( platform-support. html ), we will limit ourselves to Ubuntu 22. The three machines are identified in the network as follows: client machine: 192. 102, master/worker node (one with GPU): 192. 103 named k8s-gpu-node1 , worker node: 192. 104 named k8s-gpu-node2. Versions of the used components/tools: containerd: 1. 13, kubeadm, kubelet, kubectl: 1. For learning about Kubernetes, you can consult my training courses: course material: Mise en œuvre d’architectures microservices avec Kubernetes 🇫🇷 ; hands on lab: Tutoriel Microservices avec Kubernetes - Les bases de K8s 🇫🇷. Kubernetes nodes configuration This section details the configuration of both nodes prior to setting up the Kubernetes cluster (installation of components and operating system configuration). All operations must be performed identically on all nodes. Update the repositories and install the latest versions of packages already present on the operating system. 1 2 $ sudo apt-obtain update $ sudo apt-obtain upgrade -y A Kubernetes cluster requires a container runtime manager on each cluster node. The container runtime manager is responsible for managing the entire lifecycle of containers, including image management, container startup and shutdown. We will utilize Containerd , which appears to be the most widely used. An incomplete list of various container runtimes is available at <a href="https://kubernetes">https://kubernetes</a>. io/docs/setup/production-environment/container-runtimes. Download the latest current version of Containerd from GitHub and extract the contents into the directory /usr/local/. 1 2 $ wget <a href="https://github">https://github</a>. com/containerd/containerd/releases/download/v1. gz $ sudo tar -C /usr/local -xzvf containerd-1. gz Download the containerd service description file. 1 2 $ wget <a href="https://raw">https://raw</a>. com/containerd/containerd/main/containerd. service $ sudo mv containerd. service /lib/systemd/system/containerd. service begin the containerd service using the command below. 1 2 $ sudo systemctl daemon-reload $ sudo systemctl enable --now containerd Verify that the containerd service is started. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ sudo systemctl status containerd ● containerd. service - containerd container runtime Loaded: loaded (/lib/systemd/system/containerd. service; enabled; vendor preset: enabled) Active: active (running) since Mon 2024-07-15 14:11:19 UTC; 1min 1s ago Docs: <a href="https://containerd">https://containerd</a>. io Process: 2449 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 2450 (containerd) Tasks: 9 Memory: 13. 7M CPU: 148ms CGroup: /system. service └─2450 /usr/local/bin/containerd Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time=&quot;2024-07-15T14:11:19. 305055646Z&quot; level=info msg=&quot;begin subscribing containerd event&quot; Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time=&quot;2024-07-15T14:11:19. 305117731Z&quot; level=info msg=&quot;begin recovering state&quot; Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time=&quot;2024-07-15T14:11:19. 305182076Z&quot; level=info msg=&quot;begin event monitor&quot; Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time=&quot;2024-07-15T14:11:19. 305200106Z&quot; level=info msg=&quot;begin snapshots syncer&quot; Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time=&quot;2024-07-15T14:11:19. 305216073Z&quot; level=info msg=&quot;begin cni network conf syncer for default&quot; Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time=&quot;2024-07-15T14:11:19. 305229234Z&quot; level=info msg=&quot;begin streaming server&quot; Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time=&quot;2024-07-15T14:11:19. 305092597Z&quot; level=info msg=serving. address=/run/containerd/containerd. ttrpc Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time=&quot;2024-07-15T14:11:19. 305368243Z&quot; level=info msg=serving. address=/run/containerd/containerd. sock Jul 15 14:11:19 k8s-gpu-node1 containerd[2450]: time=&quot;2024-07-15T14:11:19. 305468660Z&quot; level=info msg=&quot;containerd successfully booted in 0. 033130s&quot; Jul 15 14:11:19 k8s-gpu-node1 systemd[1]: Started containerd container runtime. Containerd is associated with a container runtime that interacts directly with the Linux kernel to configure and run containers. We will utilize runC , which also appears to be widely used. Download the latest current version of runC from GitHub and install it in the directory /usr/local/sbin. 1 2 $ wget <a href="https://github">https://github</a>. com/opencontainers/runc/releases/download/v1. amd64 $ sudo install -m 755 runc. amd64 /usr/local/sbin/runc From the configuration of Containerd , the CGroup driver for runC must be configured. 1 2 3 $ sudo mkdir -p /etc/containerd/ $ containerd config default | sudo tee /etc/containerd/config. toml $ sudo sed -i &#39;s/SystemdCgroup = false/SystemdCgroup = true/g&#39; /etc/containerd/config. toml Restart Containerd to apply the previous modifications. 1 $ sudo systemctl restart containerd Ensure that the Containerd service is still running. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ systemctl status containerd ● containerd. service - containerd container runtime Loaded: loaded (/lib/systemd/system/containerd. service; enabled; vendor preset: enabled) Active: active (running) since Mon 2024-07-15 14:51:27 UTC; 17min ago Docs: <a href="https://containerd">https://containerd</a>. io Process: 2907 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 2908 (containerd) Tasks: 9 Memory: 14. service └─2908 /usr/local/bin/containerd Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time=&quot;2024-07-15T14:51:27. 678801098Z&quot; level=info msg=&quot;begin subscribing containerd event&quot; Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time=&quot;2024-07-15T14:51:27. 678888229Z&quot; level=info msg=&quot;begin recovering state&quot; Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time=&quot;2024-07-15T14:51:27. 678895101Z&quot; level=info msg=serving. address=/run/containerd/containerd. ttrpc Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time=&quot;2024-07-15T14:51:27. 678980481Z&quot; level=info msg=serving. address=/run/containerd/containerd. sock Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time=&quot;2024-07-15T14:51:27. 678988030Z&quot; level=info msg=&quot;begin event monitor&quot; Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time=&quot;2024-07-15T14:51:27. 679069975Z&quot; level=info msg=&quot;begin snapshots syncer&quot; Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time=&quot;2024-07-15T14:51:27. 679091006Z&quot; level=info msg=&quot;begin cni network conf syncer for default&quot; Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time=&quot;2024-07-15T14:51:27. 679129596Z&quot; level=info msg=&quot;begin streaming server&quot; Jul 15 14:51:27 k8s-gpu-node1 containerd[2908]: time=&quot;2024-07-15T14:51:27. 679219448Z&quot; level=info msg=&quot;containerd successfully booted in 0. 029455s&quot; Jul 15 14:51:27 k8s-gpu-node1 systemd[1]: Started containerd container runtime. Some components ( kubelet , for example) of Kubernetes do not work well with Linux SWAP. Therefore, Linux SWAP must be disabled. Disable Linux SWAP without rebooting and permanently way from by editing the etc/fstab file. 1 2 $ sudo swapoff -a $ sudo sed -i &#39;/swap/s/^/#/&#39; /etc/fstab The Linux kernel does not allow IPv4 packet routing between interfaces by default. 1 2 3 4 5 cat &lt;&lt;EOF | sudo tee /etc/sysctl. ip_forward = 1 EOF sudo sysctl --system Four tools shall be installed: kubelet , kubeadm , kubectl and helm. The first tool kubelet is responsible for the runtime state on each node, ensuring all containers run within a Pod. The second tool kubeadm handles cluster creation. The third kubectl represents command-line utility for administering the Kubernetes cluster. Finally, helm represents tool used to define, install, and upgrade applications using charts for Kubernetes. Note that kubectl and helm are client tools and are not necessarily required on cluster nodes. However, they are required on the client machine. Install kubelet , kubeadm and kubectl. 1 2 3 4 5 6 7 8 9 $ sudo apt-obtain update $ sudo apt-obtain install -y apt-transport-https ca-certificates curl gpg $ curl -fsSL <a href="https://pkgs">https://pkgs</a>. key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring. gpg $ echo &#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring. 30/deb/ /&#39; | sudo tee /etc/apt/sources. list $ sudo apt-obtain update $ sudo apt-obtain install -y kubelet kubeadm kubectl $ sudo apt-mark hold kubelet kubeadm kubectl Install helm. 1 2 3 $ curl -fsSL -o get_helm. com/helm/helm/main/scripts/obtain-helm-3 $ chmod 700 get_helm. sh Enable and begin the kubelet service. 1 2 3 $ sudo systemctl daemon-reload $ sudo systemctl enable --now kubelet $ sudo systemctl begin kubelet obtain the status of the kubelet component. 1 2 3 4 5 6 7 8 9 10 $ systemctl status kubelet ● kubelet. service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/lib/systemd/system/kubelet. service; enabled; preset: enabled) Drop-In: /usr/lib/systemd/system/kubelet. conf Active: activating (auto-restart) (Result: exit-code) since Mon 2024-07-15 15:31:41 UTC; 3s ago Docs: <a href="https://kubernetes">https://kubernetes</a>. io/docs/ Process: 11727 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=1/FAILURE) Main PID: 11727 (code=exited, status=1/FAILURE) CPU: 47ms You can observe that the kubelet component is enabled but not started; it will become active once the cluster is set up. Client machine configuration On the client machine, the kubectl and helm tools shall be necessary. Below, we detail the installation of these tools on macOS and Linux. macOS : to install kubectl and helm via Homebrew. 1 $ brew install kubectl helm Linux : to install kubectl and helm on any Linux distribution. 1 2 3 4 5 6 7 8 9 # kubectl $ curl -LO <a href="https://storage">https://storage</a>. com/kubernetes-release/release/$(curl -s <a href="https://storage">https://storage</a>. com/kubernetes-release/release/stable. txt)/bin/linux/amd64/kubectl $ chmod +x. /kubectl /usr/local/bin/kubectl # helm $ curl -fsSL -o get_helm. com/helm/helm/main/scripts/obtain-helm-3 $ chmod 700 get_helm. sh Create a Kubernetes cluster This section shows how to create a Kubernetes cluster using the kubeadm tool. kubeadm represents command-line tool for managing a Kubernetes cluster by installing various components. Only kubelet needs to be installed before, as described in the previous section. From the master node ( k8s-gpu-node1 ), initialize the Kubernetes cluster. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ sudo kubeadm init --node-name node-master` --cri-socket /run/containerd/containerd. Your Kubernetes control-plane has initialized successfully! To begin using your cluster, you require to run the following as a regular user: mkdir -p $HOME/. kube sudo cp -i /etc/kubernetes/admin. kube/config sudo chown $(id -u):$(id -g) $HOME/. kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin. conf You should now deploy a pod network to the cluster. Run &quot;kubectl apply -f [podnetwork]. yaml&quot; with one of the options listed at: <a href="https://kubernetes">https://kubernetes</a>. io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192. 103:6443 --token YOUR_TOKEN \ --discovery-token-ca-cert-hash sha256:YOUR_CA_CERT_HASH At the end of the installation, you shall be prompted to perform some operations to access the Kubernetes cluster. The first one involves storing the Kubernetes cluster access information in $HOME/. This file may be used by kubectl to interact with the cluster. The second operation is to add a node to the Kubernetes cluster. 1 2 3 $ mkdir -p $HOME/. kube $ sudo cp -i /etc/kubernetes/admin. kube/config $ sudo chown $(id -u):$(id -g) $HOME/. kube/config To allow the client machine to connect to the Kubernetes cluster, copy the config file from master node ( k8s-gpu-node1 ) to the client machine. kube $ scp k8suser@192. kube Still from the client machine, test the communication with the Kubernetes cluster. 1 2 3 $ kubectl obtain nodes NAME STATUS ROLES AGE VERSION node-master NotReady control-plane 25h v1. 3 The master node is currently the only node in the Kubernetes cluster. Additionally, our cluster cannot schedule Pods on this master node for security reasons. Since our cluster may not have many nodes, the security feature shall be disabled. 1 2 $ kubectl taint nodes node-master node-role. io/control-plane- $ kubectl label nodes node-master node. io/exclude-from-external-load-balancers- To re-enable this security feature. 1 2 $ kubectl taint nodes node-master node-role. io/control-plane:NoSchedule $ kubectl label nodes node-master node. io/exclude-from-external-load-balancers=true Let’s add a second node to the Kubernetes cluster. Connect to the worker node k8s-gpu-node2 and execute the following command-line instructions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ sudo kubeadm join 192. 103:6443 --token gf8ui6. ulo4gcme68k7j1zv \ --discovery-token-ca-cert-hash sha256:2563ef8edc1fb9e4bdfdde6c0e723b9812647405be819eff95596eeae0ac254e [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster. [preflight] FYI: You can look at this config file with &#39;kubectl -n kube-system obtain cm kubeadm-config -o yaml&#39; [kubelet-begin] Writing kubelet configuration to file &quot;/var/lib/kubelet/config. yaml&quot; [kubelet-begin] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags. env&quot; [kubelet-begin] Starting the kubelet [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s [kubelet-check] The kubelet is healthy after 507. 199197ms [kubelet-begin] Waiting for the kubelet to perform the TLS Bootstrap This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run &#39;kubectl obtain nodes&#39; on the control-plane to observe this node join the cluster. The values for the --token and --discovery-token-ca-cert-hash parameters are provided during the creation of the Kubernetes cluster. However, the token value is valid for only 24 hours and you may not have had time to save this information from the console. Don’t worry, both pieces of information may be retrieved from the master node. To retrieve the token value ( --token ) if the 24-hour deadline has not been reached. 1 2 3 $ kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS gf8ui6. ulo4gcme68k7j1zv 23h 2024-07-17T16:58:57Z authentication,signing <none> system:bootstrappers:kubeadm:default-node-token To generate a new value of the token. 1 2 3 4 5 6 $ kubeadm token create zuku5f. gjtnq2bcupmg0902 $ kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS gf8ui6. ulo4gcme68k7j1zv 23h 2024-07-17T16:58:57Z authentication,signing <none> system:bootstrappers:kubeadm:default-node-token zuku5f. gjtnq2bcupmg0902 23h 2024-07-17T17:24:39Z authentication,signing <none> system:bootstrappers:kubeadm:default-node-token To retrieve the value of the certificate authority hash ( --discovery-token-ca-cert-hash ). 1 2 $ openssl x509 -in /etc/kubernetes/pki/ca. crt -pubkey -noout | openssl pkey -pubin -outform DER | openssl dgst -sha256 SHA2-256(stdin)= 2563ef8edc1fb9e4bdfdde6c0e723b9812647405be819eff95596eeae0ac254e From the client machine, check that both nodes are available. 1 2 3 4 $ kubectl obtain nodes NAME STATUS ROLES AGE VERSION k8s-gpu-node2 NotReady <none> 15m v1. 3 node-master NotReady control-plane 25h v1. 3 Also, ensure that the internal Kubernetes components installed by kubeadm are deployed. 1 2 3 4 5 6 7 8 9 10 $ kubectl obtain pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-7db6d8ff4d-ht8wc 0/1 Pending 0 25h coredns-7db6d8ff4d-rlzh5 0/1 Pending 0 25h etcd-node-master 1/1 Running 0 25h kube-apiserver-node-master 1/1 Running 0 25h kube-controller-manager-node-master 1/1 Running 0 25h kube-proxy-f66kq 1/1 Running 0 20m kube-proxy-rvcxh 1/1 Running 0 25h kube-scheduler-node-master 1/1 Running 0 25h In the previous outputs, both nodes are in NotReady status and the Pods coredns-7db6d8ff4d-ht8wc and coredns-7db6d8ff4d-rlzh5 are not deployed. To resolve this issue, a network plugin compliant with the CNI project must be installed. This will enable Pods to communicate within the Kubernetes cluster. There are numerous network plugins available, and the choice was made to utilize Cilium. Cilium offers the significant advantage of leveraging eBPF (extended Berkeley Packet Filter) technology, which has recently been integrated into Linux kernels. With eBPF, there is no require to load modules into the Linux kernel as was necessary with IPTables. From the master node, download the latest version of Cilium. 1 2 3 4 $ CILIUM_CLI_VERSION=$(curl -s <a href="https://raw">https://raw</a>. com/cilium/cilium-cli/main/stable. txt) $ curl -L --remote-name-all <a href="https://github">https://github</a>. com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-amd64. gz $ sudo tar xzvfC cilium-linux-amd64. gz /usr/local/bin $ rm cilium-linux-amd64. gz To configure the installation of Cilium , you can utilize a configuration file. For instance, in the values. yaml file shown below, the CIDR (Classless Inter-Domain Routing) used to assign IPs to the Pods is modified. 1 2 3 4 ipam: mode: &quot;cluster-pool&quot; operator: clusterPoolIPv4PodCIDRList: [&quot;172. 0/16&quot;] Install Cilium. The installed version shall be shown. 1 2 3 4 $ cilium install --helm-values values. yaml ℹ️ Using Cilium version 1. 6 🔮 Auto-detected cluster name: kubernetes 🔮 Auto-detected kube-proxy was installed Wait a few seconds for the images to be downloaded and the pods to be deployed, and so check that the network plugin was successfully installed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ cilium status /¯¯\ /¯¯_<em>/¯¯\ Cilium: OK _</em>/¯¯_<em>/ Operator: OK /¯¯_</em>/¯¯\ Envoy DaemonSet: disabled (using embedded mode) _<em>/¯¯_</em>/ Hubble Relay: disabled __/ ClusterMesh: disabled Deployment cilium-operator Desired: 1, Ready: 1/1, Available: 1/1 DaemonSet cilium Desired: 2, Ready: 2/2, Available: 2/2 Containers: cilium Running: 2 cilium-operator Running: 1 Cluster Pods: 2/2 managed by Cilium Helm chart version: Image versions cilium quay. 6@sha256:6aa840986a3a9722cd967ef63248d675a87add7e1704740902d5d3162f0c0def: 2 cilium-operator quay. io/cilium/operator-generic:v1. 6@sha256:5789f0935eef96ad571e4f5565a8800d3a8fbb05265cf6909300cd82fd513c3d: 1 From the client machine, verify that both nodes are available and operational. 1 2 3 4 kubectl obtain nodes NAME STATUS ROLES AGE VERSION k8s-gpu-node2 Ready <none> 20h v1. 3 node-master Ready control-plane 45h v1. 3 The status of both nodes must be Ready. Add GPU support to the Kubernetes cluster At this stage, a Kubernetes cluster with two nodes is configured. One of the nodes has a GPU card, but the Kubernetes cluster does not understand that this node has a GPU. The goal of this section is to declare the GPU in the cluster and identify it as a resource, similar to a CPU or memory resource. However, configuring a GPU in a Kubernetes cluster is not trivial since it requires installing the GPU driver, identifying it with the container manager Containerd , detecting and labeling the nodes with GPUs, and installing specific libraries (such as CUDA). NVIDIA has provided an operator called GPU Operator that simplifies all these tasks. This section aims to detail the installation of this operator. Create a namespace called gpu-operator. The GPU Operator shall be deployed in this namespace. 1 2 $ kubectl create ns gpu-operator namespace/gpu-operator created Add the NVIDIA helm repository. 1 $ helm repo add nvidia <a href="https://helm">https://helm</a>. com/nvidia Install GPU Operator. 1 2 3 4 5 6 7 $ helm install --wait --generate-name -n gpu-operator --create-namespace nvidia/gpu-operator NAME: gpu-operator-1721224440 LAST DEPLOYED: Wed Jul 17 15:54:02 2024 NAMESPACE: gpu-operator STATUS: deployed REVISION: 1 TEST SUITE: None The operator will perform several tasks to discover that an NVIDIA GPU is available on the master node. New labels were added to the description of the master node. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 $ kubectl describe nodes node-master | grep nvidia. timestamp=1721229911 nvidia. com/gpu-driver-upgrade-state=upgrade-done nvidia. container-toolkit=true nvidia. dcgm-exporter=true nvidia. device-plugin=true nvidia. gpu-feature-discovery=true nvidia. node-status-exporter=true nvidia. operator-validator=true nvidia. machine=Precision-3450 nvidia. product=Quadro-P400 nvidia. sharing-strategy=none nvidia. strategy=single nvidia. com/gpu-driver-upgrade-enabled: true The previous description shows that a GPU is available: nvidia. count=1 , and the detected card represents Quadro-P400 with 2048 MB of memory. A Pod called cuda-vectoradd based on the image nvcr. io/nvidia/k8s/cuda-sample:vectoradd-cuda12. 04 is deployed to verify that the GPU may be used by a program for GPU computations. Once the computations are completed, the Pod stops. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 cat &lt;&lt;EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: cuda-vectoradd spec: restartPolicy: OnFailure containers: - name: cuda-vectoradd image: &quot;nvcr. io/nvidia/k8s/cuda-sample:vectoradd-cuda12. 04&quot; resources: limits: nvidia. com/gpu: 1 EOF Display the logs of the cuda-vectoradd pod. 1 2 3 4 5 6 7 $ kubectl logs pod/cuda-vectoradd [Vector addition of 50000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done The GPU usage by the cuda-vectoradd Pod works perfectly. Let’s now focus on an application that continuously uses the GPU and will demonstrate the utilization of a constrained GPU resource. Deploying an Application Requiring a GPU The experimental application used shall be Ollama. It is an application that exposes generative AI models, such as LLMs, via a REST API. It is possible to download LLM models and to run them either using only the CPU or by combining the CPU with a GPU to reduce execution time. The outcome of this experiment should demonstrate that if Ollama utilizes a GPU resource, it is preempted and not available for other applications until Ollama releases it. The Ollama application is available through helm. All the following operations shall be performed from the client machine. Before deploying the Ollama application, check that the GPU resource is available by querying the description of the master node. 1 2 3 4 5 6 7 8 9 10 11 12 $ kubectl describe nodes node-master. Allocated resources: (Total limits may be over 100 percent, i. ) Resource Requests Limits --- --- --- cpu 1150m (9%) 500m (4%) memory 350Mi (1%) 690Mi (2%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) nvidia. com/gpu 0 0 The resource identified by nvidia. com/gpu shows that it is not in utilize. Add the Ollama helm repository. 1 2 $ helm repo add ollama-helm <a href="https://otwld">https://otwld</a>. io/ollama-helm/ &quot;ollama-helm&quot; was added to your repositories Create a ollama namespace to group all resources related to the Ollama application. 1 2 $ kubectl create ns ollama namespace/ollama created Deploy Ollama application into the Kubernetes cluster. 1 2 3 4 5 6 7 8 9 10 11 12 $ helm install appli-ollama ollama-helm/ollama --namespace ollama --set ollama. enabled=true --set ollama. number=1 --set ollama. type=nvidia NAME: appli-ollama LAST DEPLOYED: Thu Jul 18 09:43:56 2024 NAMESPACE: ollama STATUS: deployed REVISION: 1 NOTES: 1. obtain the application URL by running these commands: export POD_NAME=$(kubectl obtain pods --namespace ollama -l &quot;app. io/instance=appli-ollama&quot; -o jsonpath=&quot;{. name}&quot;) export CONTAINER_PORT=$(kubectl obtain pod --namespace ollama $POD_NAME -o jsonpath=&quot;{. containerPort}&quot;) echo &quot;Visit <a href="http://127">http://127</a>. 1:8080 to utilize your application&quot; kubectl --namespace ollama port-forward $POD_NAME 8080:$CONTAINER_PORT It specifies that GPU support must be enabled ( --set ollama. enabled=true ), the required number of GPUs is one ( --set ollama. number=1 ) and the GPU type must be NVIDIA ( --set ollama. Check that the Ollama application was deployed (packaged into a Pod) on the master node that has the GPU. 1 2 3 $ kubectl obtain pods -n ollama -o wide NAME READY STATUS RESTARTS AGE IP NODE appli-ollama-8665457c88-gz8ch 1/1 Running 0 2m59s 10. 242 node-master The Pod (related to the Ollama application) is correctly located on the master node. In the deployment output for the Ollama application, it is also explained how to utilize the deployed application via a port-forward. However, this is not the deployment method we will utilize; instead, we will utilize a classic NodePort service. Apply the following service configuration to expose Ollama at the addresses 192. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ cat &lt;&lt;EOF | kubectl create -n ollama -f - kind: Service apiVersion: v1 metadata: name: ollamanodeportservice spec: selector: app. io/name: ollama type: NodePort ports: - protocol: TCP targetPort: 11434 port: 11434 nodePort: 30001 externalIPs: - 80. 12 EOF service/ollamanodeportservice created Execute the HTTP request to download the LLM model called gemma:2b in Ollama. 1 2 3 4 5 $ curl <a href="http://192">http://192</a>. 103:30001/api/pull -d &#39;{ &quot;name&quot;: &quot;gemma:2b&quot; }&#39; {&quot;status&quot;:&quot;pulling manifest&quot;}. Execute the HTTP request to generate a response to the given question. 1 2 3 4 5 6 $ curl <a href="http://192">http://192</a>. 103:30001/api/generate -d &#39;{ &quot;model&quot;: &quot;gemma:2b&quot;, &quot;prompt&quot;: &quot;Why is the sky blue?&quot;, &quot;stream&quot;: false }&#39; {&quot;model&quot;:&quot;gemma:2b&quot;,&quot;created_at&quot;:&quot;2024-07-18T08:00:02. 461703025Z&quot;,&quot;response&quot;:&quot;The sky appears blue due to Rayleigh scattering. Rayleigh scattering is the scattering of light by small particles, such as molecules in the atmosphere. Blue light has a shorter wavelength than other colors of light, so it is scattered more strongly. This is why the sky appears blue. &quot;,&quot;done&quot;:true,&quot;done_reason&quot;:&quot;cease&quot;,&quot;context&quot;:[968,2997,235298,559,235298,15508,235313,1645,108,4385,603,573,8203,3868,181537,615,235298,559,235298,15508,235313,108,235322,2997,235298,559,235298,15508,235313,2516,108,651,8203,8149,3868,3402,577,153902,38497,235265,153902,38497,603,573,38497,576,2611,731,2301,16071,235269,1582,685,24582,575,573,13795,235265,7640,2611,919,476,25270,35571,1178,1156,9276,576,2611,235269,712,665,603,30390,978,16066,235265,1417,603,3165,573,8203,8149,3868,235265],&quot;total_duration&quot;:4802224355,&quot;load_duration&quot;:33326246,&quot;prompt_eval_count&quot;:32,&quot;prompt_eval_duration&quot;:324835000,&quot;eval_count&quot;:55,&quot;eval_duration&quot;:4400550000} Everything is working correctly, Ollama answers a question and generates a response quickly. Check the usage of the GPU to observe if it was preempted following the deployment of the Ollama application. 1 2 3 4 5 6 7 8 9 10 11 12 $ kubectl describe nodes node-master. Allocated resources: (Total limits may be over 100 percent, i. ) Resource Requests Limits --- --- --- cpu 1150m (9%) 500m (4%) memory 350Mi (1%) 690Mi (2%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) nvidia. com/gpu 1 1 The GPU resources of the Kubernetes cluster are no longer available as they have all been preempted. Thus, if a Pod requiring GPU resources needs to be deployed, the Kubernetes cluster will put it on hold until the GPU resources are freed. To validate this scenario, we will deploy a new instance of Ollama. Create a ollama2 namespace to group all resources related to the Ollama application. 1 2 $ kubectl create ns ollama2 namespace/ollama2 created Deploy Ollama application in the Kubernetes cluster within the ollama2 namespace. 1 2 3 4 5 6 7 8 9 10 11 12 $ helm install appli-ollama ollama-helm/ollama --namespace ollama --set ollama. enabled=true --set ollama. number=1 --set ollama. type=nvidia NAME: appli-ollama LAST DEPLOYED: Thu Jul 18 09:43:56 2024 NAMESPACE: ollama STATUS: deployed REVISION: 1 NOTES: 1. obtain the application URL by running these commands: export POD_NAME=$(kubectl obtain pods --namespace ollama -l &quot;app. io/instance=appli-ollama&quot; -o jsonpath=&quot;{. name}&quot;) export CONTAINER_PORT=$(kubectl obtain pod --namespace ollama $POD_NAME -o jsonpath=&quot;{. containerPort}&quot;) echo &quot;Visit <a href="http://127">http://127</a>. 1:8080 to utilize your application&quot; kubectl --namespace ollama port-forward $POD_NAME 8080:$CONTAINER_PORT Display the status of the Pods in the ollama2 namespace. 1 2 3 $ kubectl obtain pods -n ollama2 NAME READY STATUS RESTARTS AGE appli-ollama-8665457c88-ngtpf 0/1 Pending 0 116 As expected, the Pod is in the Pending state. Check the Pod’s description to determine the reason for its Pending state. 1 2 3 4 5 6 $ kubectl describe pod appli-ollama -n ollama2. Events: Type Reason Age From Message --- --- --- --- --- Warning FailedScheduling 3m24s default-scheduler 0/2 nodes are available: 2 Insufficient nvidia. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod. As indicated in the message, no node in the cluster can accommodate this new Pod. Conclusion This experiment showed the setup of a Kubernetes cluster and the discovery of GPU nodes via the GPU Operator. There are still many aspects to explore, particularly updating the components installed by the GPU Operator (drivers, libraries, etc. It may also be worthwhile to examine how to manage NVIDIA cards with MIG technology, which aims to partition a GPU into multiple sub-GPUs. Stay tuned and give your feedbacks in the comments. Resources <a href="https://docs">https://docs</a>. com/datacenter/cloud-native/gpu-operator/latest/index. io/posts/nvidia-rtx-gpu-kubernetes-setup/ <a href="https://blog">https://blog</a>. org/2019/08/15/reconstructing-the-join-command-for-kubeadm/ <a href="https://blog">https://blog</a>. org/2019/07/12/calculating-ca-certificate-hash-for-kubeadm/ <a href="https://github">https://github</a>. com/otwld/ollama-helm Commentaire Vous pouvez laisser un commentaire en utilisant les Github Issues. Je suis Mickaël BARON Ingénieur de Recherche en Informatique à l&#39; ISAE-ENSMA et membre du laboratoire LIAS le jour Veilleur Technologique la nuit #Java #Container #VueJS #Services #WebSemantic Derniers articles et billets Advanced Kubernetes Deployment on an NVIDIA GPUs Cluster: NGINX Ingress Controler, Local Storage, and Prometheus Monitoring kubernetes 13 fév. 2025 A step-by-step pratical guide for deploying NVIDIA GPUs on Kubernetes kubernetes 19 juil. 2024 Create and utilize custom XCP-NG templates: a guide for Ubuntu XCP-NG Xen 07 juin 2024 Environnements de développement à distance. Les solutions existantes ? Présentation d&#39;Onyxia Onyxia 17 sept. 2025 Article Running Coder in a K3s cluster self-hosted DevOps Kubernetes Coder 02 juin 2023</p>
<p>SY小站 kubernetes分析ExitCode 关注作者 腾讯云 开发者社区 文档 建议反馈 控制台 登录/注册 首页 学习 活动 专区 圈层 工具 MCP广场 文章/答案/技术大牛 搜索 搜索 关闭 发布 SY小站 社区首页 &gt; 专栏 &gt; kubernetes分析ExitCode kubernetes分析ExitCode SY小站 关注 发布 于 2020-06-15 12:28:08 发布 于 2020-06-15 12:28:08 3. 8K 0 举报 文章被收录于专栏： SY小站的专栏 SY小站的专栏 01 问题 最近总有开发小伙伴来找我，为什么我的容器总退出呢，在哪能看到原因。故写篇文章整理下docker退出的状态码。 02 如何查看退出码 查看pod中的容器退出码 代码语言： javascript 复制 $ kubectl describe pod xxx Port: <none> Host Port: <none> State: Running Started: Tue, 26 May 2020 20:01:04 +0800 Last State: Terminated Reason: Error Exit Code: 137 Started: Tue, 26 May 2020 19:58:40 +0800 Finished: Tue, 26 May 2020 20:01:04 +0800 Ready: True Restart Count: 2363 docker查看 代码语言： javascript 复制 $ docker ps --filter &quot;status=exited&quot; $ docker inspect <container-id> --format=&#39;{{. ExitCode}}&#39; 03 常见退出码 Exit Code 0 退出代码0表示特定容器没有附加前台进程。 该退出代码是所有其他后续退出代码的例外。 这不一定意味着发生了不好的事情。如果开发人员想要在容器完成其工作后自动停止其容器，则使用此退出代码。 Exit Code 1 程序错误，或者Dockerfile中引用不存在的文件，如 entrypoint中引用了错误的包 程序错误可以很简单，例如“除以0”，也可以很复杂，比如空引用或者其他程序 crash Exit Code 137 此状态码一般是因为 pod 中容器内存达到了它的资源限制( resources. limits )，一般是内存溢出(OOM)，CPU达到限制只需要不分时间片给程序就可以。因为限制资源是通过 linux 的 cgroup 实现的，所以 cgroup 会将此容器强制杀掉，类似于 kill -9 还可能是 宿主机 本身资源不够用了(OOM)，内核会选取一些进程杀掉来释放内存 不管是 cgroup 限制杀掉进程还是因为节点机器本身资源不够导致进程死掉，都可以从系统日志中找到记录( journalctl -k ) Exit Code 139 表明容器收到了SIGSEGV信号，无效的内存引用，对应kill -11 一般是代码有问题，或者 docker 的基础镜像有问题 Exit Code 143 表明容器收到了SIGTERM信号，终端关闭，对应kill -15 一般对应docker cease 命令 有时docker stop也会导致Exit Code 137。发生在与代码无法处理SIGTERM的情况下，docker进程等待十秒钟然后发出SIGKILL强制退出。 Exit Code 1 和 255 这种可能是一般错误，具体错误原因只能看容器日志，因为很多程序员写异常退出时习惯用 exit(1) 或 exit(-1) ，-1 会根据转换规则转成 255 本文参与 腾讯云自媒体同步曝光计划 ，分享自微信公众号。 原始发表：2020-05-27 ，如有侵权请联系 cloudcommunity@tencent. com 删除 编程算法 容器 容器镜像服务 linux 本文分享自 SY技术小站 微信公众号， 前往查看 如有侵权，请联系 cloudcommunity@tencent. com 删除。 本文参与 腾讯云自媒体同步曝光计划 ，欢迎热爱写作的你一起参与！ 编程算法 容器 容器镜像服务 linux 评论 登录 后参与评论 0 条评论 热度 最新 登录 后参与评论 推荐阅读 目录 01 问题 02 如何查看退出码 03 常见退出码 Exit Code 0 Exit Code 1 Exit Code 137 Exit Code 139 Exit Code 143 Exit Code 1 和 255 相关产品与服务 容器服务 腾讯云容器服务（Tencent Kubernetes Engine, TKE）基于原生 kubernetes 提供以容器为核心的、高度可扩展的企业级容器管理服务。首创单集群混合节点的资源管理模式，全面围绕 Agentic AI 应用部署与极致资源效能提供全场景解决方案，为用户释放 AI 时代的无限算力。 产品介绍 产品文档 AI驱动 智领未来 领券 社区 技术文章 技术问答 技术沙龙 技术视频 学习中心 技术百科 技术专区 活动 自媒体同步曝光计划 邀请作者入驻 自荐上首页 技术竞赛 圈层 腾讯云最具价值专家 腾讯云架构师技术同盟 腾讯云创作之星 腾讯云TDP 关于 社区规范 免责声明 联系我们 友情链接 MCP广场开源版权声明 腾讯云开发者 扫码关注腾讯云开发者 领取腾讯云代金券 热门产品 域名注册 云服务器 区块链服务 消息队列 网络加速 云数据库 域名解析 云存储 视频直播 热门推荐 人脸识别 腾讯会议 企业云 CDN加速 视频通话 图像分析 MySQL 数据库 SSL 证书 语音识别 更多推荐 数据安全 负载均衡 短信 文字识别 云点播 大数据 小程序开发 网站监控 数据迁移 Copyright © 2013 - 2025 Tencent Cloud. 腾讯云 版权所有 深圳市腾讯计算机系统有限公司 ICP备案/许可证号： 粤B2-20090059 深公网安备号 44030502008569 腾讯云计算（北京）有限责任公司 京ICP证150476号 | 京ICP备11018762号 | 京公网安备号11010802020287 问题归档 专栏文章 快讯文章归档 关键词归档 开发者手册归档 开发者手册 Section 归档 Copyright © 2013 - 2025 Tencent Cloud. 腾讯云 版权所有 登录 后参与评论 0 0 0 推荐</p>

            </div>

            <!-- Promotional Section -->
            <div class="promotional-section">
              <p>Ready to eliminate manual firefighting and achieve autonomous infrastructure operations?</p>
              <p>See how AlertMend AI can help you reduce costs by 50%, achieve zero downtime, and automate incident remediation across Kubernetes, VMs, and ECS. <a href="https://calendly.com/hello-alertmend/30min" target="_blank" rel="noopener noreferrer">Book a demo.</a></p>
            </div>

            <!-- Horizontal Separator -->
            <hr />

            <!-- Arvind Rajpurohit Profile Section -->
            <div class="profile-section">
              <img src="/logos/arvind.jpeg" alt="Arvind Rajpurohit" class="profile-image" onerror="this.style.display='none'; const placeholder = this.nextElementSibling; if (placeholder) placeholder.classList.add('show');" />
              <div class="profile-placeholder-arvind">AR</div>
              <div class="profile-content">
                <h3 class="profile-name">Arvind Rajpurohit</h3>
                <p class="profile-title" style="color: #9333ea; font-weight: 600; margin-bottom: 1rem; font-size: 1rem;">Co-Founder & CEO</p>
                <div class="profile-bio">
                  <p>Arvind is a Kubestronaut and Kubernetes expert with 15+ years of experience in infrastructure automation. Previously DevOps Team Lead at Roambee and Customer Success Engineer at Shoreline.io (acquired by NVIDIA), he's helped hundreds of teams achieve 99.97% uptime, reduce costs by 50%, and eliminate 90% of manual operations work.</p>
                  <p>As CEO of AlertMend AI, Arvind is building the future of autonomous infrastructure management—where AI doesn't just monitor systems, but understands, predicts, and automatically resolves issues while continuously learning and improving.</p>
                </div>
                <a href="https://www.linkedin.com/in/arvind-rajpurohit-4a332523/" target="_blank" rel="noopener noreferrer" class="linkedin-link">
                  <svg width="20" height="20" fill="currentColor" viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
                </a>
              </div>
            </div>
          </article>
        </div>
      </div>

      <!-- Right Sidebar (30%) -->
      <aside class="sidebar">
        <div class="sidebar-content">
          <!-- Email Signup -->
          <div class="sidebar-card">
            <h3>Receive blog and product updates</h3>
            <form class="signup-form">
              <input type="email" placeholder="Email*" required />
              <button type="submit">SIGN UP</button>
            </form>
          </div>

          <!-- Related Content -->
          
          <div class="sidebar-card">
            <h3 class="related-content-title">RELATED CONTENT</h3>
            <ul class="related-posts-list">
              
                <li>
                  <a href="/blog/oomkilled-in-kubernetes" class="related-post-link">How to Fix OOMKilled Errors in Kubernetes</a>
                </li>
              
                <li>
                  <a href="/blog/graceful-shutdown-kubernetes" class="related-post-link">Graceful Shutdown in Kubernetes: Ensuring Safe Pod Termination</a>
                </li>
              
                <li>
                  <a href="/blog/load-balancing-long-lived-connections-kubernetes" class="related-post-link">Load Balancing and Scaling Long-Lived Connections in Kubernetes</a>
                </li>
              
                <li>
                  <a href="/blog/5-ways-aiops-transforming-infrastructure" class="related-post-link">5 Ways AIOps is Transforming Infrastructure Management</a>
                </li>
              
                <li>
                  <a href="/blog/cost-optimization-multi-cloud" class="related-post-link">Cost Optimization Strategies for Multi-Cloud Infrastructure</a>
                </li>
              
                <li>
                  <a href="/blog/troubleshooting-unhealthy-elasticsearch-nodes-kubernetes" class="related-post-link">Troubleshooting Unhealthy Elasticsearch Nodes on Kubernetes: Causes and Solutions</a>
                </li>
              
                <li>
                  <a href="/blog/troubleshooting-elasticsearch-unassigned-shards-kubernetes" class="related-post-link">Troubleshooting Elasticsearch Unassigned Shards Incident on Kubernetes: Causes and Solutions</a>
                </li>
              
                <li>
                  <a href="/blog/elasticsearch-cluster-yellow-incident-kubernetes" class="related-post-link">Elasticsearch Cluster Yellow Incident on Kubernetes</a>
                </li>
              
            </ul>
            <a href="/blog" class="view-more-link">
              View All Posts
              <svg width="16" height="16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
              </svg>
            </a>
          </div>
          

          <!-- Additional Internal Links -->
          <div class="sidebar-card">
            <h3 class="related-content-title">EXPLORE ALERTMEND</h3>
            <ul class="related-posts-list">
              <li><a href="/" class="related-post-link">Home</a></li>
              <li><a href="/auto-remediation" class="related-post-link">Automated Incident Remediation</a></li>
              <li><a href="/kubernetes-management" class="related-post-link">Kubernetes Management</a></li>
              <li><a href="/on-call-management" class="related-post-link">On-Call Management</a></li>
              <li><a href="/kubernetes-cost-optimization" class="related-post-link">Cost Optimization</a></li>
              <li><a href="/case-studies" class="related-post-link">Case Studies</a></li>
              <li><a href="/pricing" class="related-post-link">Pricing</a></li>
              <li><a href="/blog" class="related-post-link">All Blog Posts</a></li>
            </ul>
          </div>
        </div>
      </aside>
    </div>
  </div>
</body>
</html>