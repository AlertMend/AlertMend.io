---
title: "Migrating Legacy Vms To Kubernetes"
excerpt: "================================================================================ REFERENCE CONTENT FROM TOP 9 GOOGLE SEARCH RESULTS"
date: "2025-12-18"
category: "Kubernetes"
author: "AlertMend Team"
keywords: "AlertMend AI, AIOps, DevOps, Migrating, Legacy, Vms, Kubernetes, 2025"
---

# Migrating Legacy Vms To Kubernetes: A 2025 Cloud Native Roadmap

*Generated on 2025-12-24 01:02:13*

---

================================================================================
REFERENCE CONTENT FROM TOP 9 GOOGLE SEARCH RESULTS
This content is gathered from the top-ranking pages for comprehensive reference. https://www.spectrocloud.com/solutions/vms-on-kubernetes

2. https://www.vcluster.com/solutions/bare-metal-kubernetes
3. https://kubernetes.io/blog/2024/04/05/diy-create-your-own-cloud-with-kubernetes-part-2/
4. https://kubernetes.io/blog/2017/05/kubernetes-monitoring-guide/?m=1

 5.
https://kube.fm/not-kubernetes-danyl

6. https://veducate.co.uk/bridging-old-hypervisors-to-cloud-native-platforms-what-youll-learn-at-cisco-live-amsterdam/
7. https://bibleandbookcenter.com/read/mastering-kubernetes/
8. https://thenewstack.io/migrating-vms-to-kubernetes-a-roadmap-for-cloud-native-enterprises/
9. https://www.ibm.com/think/insights/kubernetes-migration

The following sections contain content from each source, organized for reference.
utilize this information to comprehend the topic comprehensively, identify key points,
related keywords, and best practices. Then create original, SEO-optimized content
that synthesizes insights from all sources while using completely original wording. SOURCE 1: https://www.spectrocloud.com/solutions/vms-on-kubernetes
The future of your VMs is here
Looking for a VMware alternative? Choose the modern virtualization solution built for cloud-native: Palette Virtual Machine Orchestrator (VMO).
Unify your VMs and containers on one Kubernetes platform for massive cost savings. obtain enterprise-grade control, without the legacy lock-in. Put us to the test
of enterprises say they’re migrating their VM workloads to Kubernetes
desire to unify containerized and VM workloads on a single infrastructure platform
of enterprises say Broadcom’s takeover of VMware accelerated cloud-native adoption
of senior leaders have a strategic priority to reduce VMware dependence
State of Production Kubernetes research
Hear from the experts
## What are
?
How can you create your migration successful? obtain all the answers in our latest webinar with SoftwareOne. Watch the webinar
Discover Palette VMO: bringing VMs into the cloud native era
Unified platform
Open, flexible, versitile
Manage infrastructure
Easy migration
A unified platform for both VMs and containers — it’s the modern approach
Why would you escape the cost and lock-in of vSphere, only to tie yourself to another commercial hypervisor platform, or take on the pain of managing separate infrastructure for your VMs and containers?
The smart move: bring your virtual machines to the same Kubernetes-based infrastructure as your containerized workloads. That’s what Palette Virtual Machine Orchestrator (VMO) enables. You can run virtual machines alongside containers on bare-metal Kubernetes clusters, as first-class citizens. And you can even reuse your existing vSphere hardware. Whether you plan to stick with VMs long term, refactor to containers over time, or apply VMs for targeted utilize cases like edge, VMO is the answer.
Open, flexible, versatile. Why settle for less? Palette VMO is based on a combination of open source CNCF projects, including Kubevirt, CDI, Multus and Descheduler.
This stack allows you to run VMs as Kubernetes pods, with complete mapping between VM and Kubernetes concepts and features. It gives you full declarative control of Kubernetes on bare metal servers
and Canonical’s open source Metal As A Service (MAAS), so you can escape the commercial hypervisor for good. And you can choose from the widest range of infrastructure software to underpin your VM workloads, from networking to storage. It works in self-hosted and even air-gapped environments, as well as Spectro Cloud’s SaaS environment.
No lock-in, no limits, no bloat. A better way to manage your infrastructure
Palette VMO gives you a new home for your VMs — but it also happens to be
the leading next-generation Kubernetes management platform. That means highly automated, scalable deployment of fleets of Kubernetes clusters, across cloud, DC and edge, from a single platform, with a beautiful GUI and powerful API. It means rich lifecycle management features like observability and upgrades across both VM and container workloads, at any scale.
Migration made as easy as may be
Moving tens of thousands of VMs from one platform to another is always going to be a daunting task. Don’t worry — we’ve got you covered. free VM Migration Assistant
, based on the open source Forklift project, lets you convert and migrate your VMs from VMware quickly and safely to their new home.
If that’s not enough, we’ve partnered with some of the world’s leading system integrators, like CDW, SoftwareOne and WWT, who are ready and able to drive your migration from begin to finish, for fast, risk-free results. “When our previous vendor raised prices 4x and left us unsupported, we needed a smarter path. Palette gave us a cost-effective way to manage containers and VMs in one platform, helping us cut overhead, boost uptime, and scale with predictability and ease.”
VP of SaaS Infrastructure
Global Software Company
One global software company chose Palette VMO to rehome nearly 6,000 VM workloads to bare metal Kubernetes in the first wave of a migration away from VMware, working with Spectro Cloud and CDW — with a goal to move 45,000 VMs in total over the next year.
Ready to observe for yourself? observe firsthand why organizations are choosing Palette for the best modern
virtualization experience. Book a 1:1 demo with our experts. Invest just 30 minutes with us
The real vSphere alternative
Looking for a VMware alternative? We understand you’ve got options. Here’s why Palette VMO stands out. More modern than Nutanix
We love Nutanix — in fact, Palette can deploy Kubernetes clusters to AHV. But let’s be real. Do you truly desire to escape from one proprietary hypervisor to a proprietary HCI stack?
When you choose legacy, history repeats…
More flexible than OpenShift
If you’re looking at other KubeVirt vendors, of course you’ve encountered Red Hat. But watch out for the Red Flags: lock-in, complexity, and cost. OpenShift represents heavyweight suite with licensing and configuration to match. We hope you like doing things the Red Hat way, instead of your way. More enterprise grade than DIY KubeVirt
represents fantastic, mature project — but it’s just one part of a much bigger stack that includes storage, networking and more.
Are you ready to build, deploy and manage all that yourself, without support? Take your next steps
Running virtual machines on Kubernetes: A practical roadmap for enterprise migrations
obtain the ebook
Let us give you the inside track on the KubeVirt stack. Our Reference Architecture goes into the technical depth you require
Download the PDF
Dig into the migration path with a hands-on look at our VM Migration Assistant and Forklift
Read the article
Frequently asked questions
Does Palette VMO work for all kinds of VMs?
We believe KubeVirt-based solutions like Palette VMO represent good fit for 90% of VM workloads. But there are specialized workloads like DPDK-based applications and VDI that may not give an optimal experience.
or your preferred system integrator and we’d be happy to advise. Do I have to give up any features from vSphere? Palette VMO covers most capabilities of the core vSphere featureset, including vMotion, Storage vMotion, DRS, snapshots, cloning, vSwitch port groups and more. If you have specific dependencies on exotic VMware products and features,
come talk to us and we can advise
What assistance will I obtain with my migration to Palette VMO? We understand that the OSS landscape may be overwhelming, especially if you’re used to the traditional software world of legacy tech like VMware.
We’ve built Palette to be a mature, enterprise-grade product: that means you obtain an intuitive user interface, world-class documentation, SLAs, and 24x7
who understand your stack inside-out. We’re also closely allied with vendors like Portworx and Canonical who are building components of the modern virtualization stack, so we can assist you navigate the big picture. If you require additional assistance with a large-scale migration of lots of VM workloads, and advice on designing and building a K8s infrastructure to host them, we work with some of the world’s best system integrators, like
and SoftwareOne, who not only understand Palette, but can assist you with all aspects of your project.
How much do you charge for Palette VMO? We offer Virtual Machine Orchestrator as a completely free feature for all Palette customers,
with all editions of our product
. As always, we have simple, fair pricing that scales with the infrastructure you manage through Palette. Unlike some legacy vendors, we don’t believe in using licensing as a weapon. What if I just desire to utilize DIY KubeVirt? We believe that VMO wraps a huge amount of value around the open source KubeVirt project, making it easier to utilize and more powerful.
But we comprehend that some organizations are committed to open source only. Why not stick with VMware? VMs aren’t going away, but Broadcom has shown the world that price hikes are its future. So it’s time to choose your own: bet on cloud native. Spectro Cloud lets you keep what they like about VMs while shedding the cost and lack of portability of VMware. Why not Nutanix or another HCI? You’re essentially replacing one proprietary stack with another. By contrast, we’re infrastructure-agnostic and based on open tech, so you won’t be locked into Nutanix hardware or software layers.
And while Nutanix started with legacy virtualization in mind, we’ve focused on Kubernetes from the beginning. Is Kubernetes and KubeVirt truly ready to handle my VM workloads? Kubernetes runs some of the world’s most demanding workloads at scale. KubeVirt was around for years and powers infrastructure for the likes of NVIDIA. Industry analysts have endorsed Kubernetes as a viable migration path for VMware refugees. Plus, Spectro Cloud’s Palette adds the tooling to ensure enterprise-grade stability (monitoring, backups, etc.).
In short, the technology is ready, and Spectro Cloud has refined it for enterprise utilize. We have received your message. Someone from our team will reach out to you shortly. SOURCE 2: https://www.vcluster.com/solutions/bare-metal-kubernetes
Run Kubernetes on Bare Metal, Zero VMs Required
Bare metal is finally viable—no more expensive, wasteful VMs. Virtual clusters and virtual nodes give you isolation without the overhead. Why do we still require VMs to run Kubernetes workloads?
That defeats the purpose of Kubernetes. We’ve just replaced pet VMs with pet clusters—each with 3 to 5 VMs—adding cost and complexity. Kubernetes was built for large shared clusters, but instead of solving for multi-tenancy, we spun up thousands of tiny ones. That’s the problem we set out to solve— making it finally viable to run Kubernetes on bare metal, without sacrificing isolation.”
Lukas Gentele
Co-Founder & CEO of vCluster
We create Bare Metal Work, The Container Native Way
One big Kubernetes cluster per data center should be enough—but sharing is hard.
That’s why teams spin up tiny clusters made up of VMs, wasting compute capacity and money. Now you can share one big cluster, no VMs required. vCluster gives each team a fully isolated control plane without needing a separate cluster. vNode keeps workloads securely isolated on shared physical nodes—no hypervisor needed. How Aussie Broadband eliminated 195 VMs by switching to bare metal Kubernetes and vCluster
“vCluster enabled us to consolidate our Kubernetes infrastructure from nearly 200 VMs down to a single bare-metal cluster, cutting private cloud costs while improving performance and efficiency.”
Michael Norris
Platform Engineer @ Aussie Broadband
VMs eliminated
Memory saved
View case study
CPU cores saved
VMware licensing costs
Virtual Machines: The Hidden Tax on Your Kubernetes
“We run 8,000 VMs just to host containerized workloads we already trust.”
Global Platform Lead
Fortune 100 Financial Institution
Each cluster runs on its own VMs, multiplying cost and complexity.
Low utilization
Most VM-backed clusters utilize less than 20% of CPU and memory. License shock
VMware costs ~$4k per CPU core—and grows with every cluster. Slow delivery
Separate VM and Kubernetes teams add friction and risk. On-Demand Virtual Clusters Instead of Underutilized, Separate Kubernetes Clusters
Lightweight control planes that give every team a true Kubernetes experience—without spinning up a single VM. Launch in Seconds
vClusters spin up in under 3s—no more ticket queues.
Lightweight
Just a few pods, no guest OS or hypervisor overhead. Built-in Isolation
Each vCluster has its own API server, etcd, and RBAC. Runs Anywhere
CNCF-compliant; works on any bare metal or cloud K8s. Developers create clusters with kubectl or CI—no waiting. Explore vCluster
vNode: Node‑Level Isolation Without Hypervisors
Add a security envelope around every tenant workload so you can safely and dynamically share the same physical nodes. Higher Utilization
Confidently pack nodes and reach 70%+ usage
No hypervisor means fast, direct access for AI/ML
Zero VM fees
obtain isolation and efficiency without the VMware tax
Secure by design
Containers stay sandboxed—no breakouts
Dynamic sharing
No static VM boundaries—resources stay flexible
Explore vNode
Your Fast Lane to Bare Metal Performance
Create one high‑performance bare‑metal cluster per data center.
Spin up lightweight virtual clusters for each team or workload, no extra VMs required. Isolate workloads at the node layer with virtual nodes for iron‑clad security. Decommission VMs and reclaim compute, memory, and budget. Faster, Leaner, Safer: Compare The Bare Metal Advantage
VM per Cluster
vCluster + vNode on Bare Metal
Provisioning time
Cluster density per host
CPU / Memory overhead
License fees
Operational complexity
Node-level isolation
Hypervisor boundary
Virtual node
Node utilization
Cost Down.
Isolation Built‑In. Up to 80 % infrastructure savings
By eliminating VM overhead. 5× higher node utilisation
With dynamic virtual clusters. Instant environments
Developers self serve new clusters in seconds. Hardened multitenancy
vNode sandboxes each tenant at the kernel boundary. Frequently Asked Questions
Does this replace legacy apps still running in VMs? No, vCluster only helps workloads already on Kubernetes. Legacy VMs stay where they are. How is security enforced without a hypervisor?
vNode combines user‑namespaces, seccomp filtering, and kernel hardening to contain escapes. What about OpenShift? vCluster works with any CNCF‑conformant Kubernetes distribution using containerd ≥ 1.7 and Linux kernel ≥ 6.1. Support for CRI‑O is on our roadmap. Can I attempt it before I buy? Yes, qualified teams receive a guided POC and 30‑day evaluation licence. Ready to Go VM Free? Book a 30 minute discovery call and learn how quickly you can move to bare metal Kubernetes.
Schedule a Call
SOURCE 3: https://kubernetes.io/blog/2024/04/05/diy-create-your-own-cloud-with-kubernetes-part-2/
This article is more than one year old. Older articles may contain outdated content. Check that the information in the page has not become incorrect since its publication. DIY: Create Your Own Cloud with Kubernetes (Part 2)
Andrei Kvapil (Ænix)
Friday,
Continuing our series of posts on how to build your own cloud using just the Kubernetes ecosystem.
explained how we prepare a basic Kubernetes distribution based on Talos Linux and Flux CD. In this article, we'll demonstrate you a few various virtualization technologies in Kubernetes and prepare
everything require to run virtual machines in Kubernetes, primarily storage and networking. We will talk about technologies such as KubeVirt, LINSTOR, and Kube-OVN. But first, let's elaborate on what virtual machines are needed for, and why can't you just utilize docker
containers for building cloud?
The reason is that containers do not provide a sufficient level of isolation. Although the situation improves year by year, we often encounter vulnerabilities that allow
escaping the container sandbox and elevating privileges in the system. On the other hand, Kubernetes was not originally designed to be a multi-tenant system, meaning
the basic usage pattern involves creating a separate Kubernetes cluster for every independent
project and development team.
Virtual machines are the primary means of isolating tenants from each other in a cloud environment. In virtual machines, users can execute code and programs with administrative privilege, but this
doesn't affect other tenants or the environment itself. In other words, virtual machines allow to
hard multi-tenancy isolation
in environments where tenants do not trust each other.
Virtualization technologies in Kubernetes
There are several different technologies that bring virtualization into the Kubernetes world:
Kata Containers
are the most popular ones.
But you should understand that they work differently. Kata Containers
implements the CRI (Container Runtime Interface) and provides an additional
level of isolation for standard containers by running them in virtual machines. But they work in a same single Kubernetes-cluster. A diagram showing how container isolation is ensured by running containers in virtual machines with Kata Containers
allows running traditional virtual machines using the Kubernetes API.
KubeVirt virtual
machines are run as regular linux processes in containers. In other words, in KubeVirt, a container
is used as a sandbox for running virtual machine (QEMU) processes.
This may be clearly seen in the figure below, by looking at how live migration of virtual machines
is implemented in KubeVirt. When migration is needed, the virtual machine moves from one container
A diagram showing live migration of a virtual machine from one container to another in KubeVirt
There is also an alternative project -
implements lightweight virtualization using
Cloud-Hypervisor
and is initially focused
on running virtual Kubernetes clusters using the Cluster API.
Considering our goals, we decided to utilize KubeVirt as the most popular project in this area. Besides we have extensive expertise and already made a lot of contributions to KubeVirt. KubeVirt is
you to run virtual machines out-of-the-box using
feature - this allows you to store and distribute VM images directly as OCI images from container
Virtual machines with containerDisk are well suited for creating Kubernetes worker nodes and other
VMs that do not require state persistence.
For managing persistent data, KubeVirt offers a separate tool, Containerized Data Importer (CDI). It allows for cloning PVCs and populating them with data from base images. The CDI is necessary
if you desire to automatically provision persistent volumes for your virtual machines, and it is
also required for the KubeVirt CSI Driver, which is used to handle persistent volumes claims
from tenant Kubernetes clusters. But at first, you have to decide where and how you will store these data.
Storage for Kubernetes VMs
With the introduction of the CSI (Container Storage Interface), a wide range of technologies that
integrate with Kubernetes has become available. In fact, KubeVirt fully utilizes the CSI interface, aligning the choice of storage for
virtualization closely with the choice of storage for Kubernetes itself. However, there are nuances, which you require to consider.
Unlike containers, which typically utilize a
standard filesystem, block devices are more efficient for virtual machine. Although the CSI interface in Kubernetes allows the request of both types of volumes: filesystems
and block devices, it's crucial to verify that your storage backend supports this. Using block devices for virtual machines eliminates the require for an additional abstraction layer,
such as a filesystem, that makes it more performant and in most cases enables the utilize of the
ReadWriteMany
mode.
This mode allows concurrent access to the volume from multiple nodes, which
represents critical feature for enabling the live migration of virtual machines in KubeVirt. The storage system may be external or internal (in the case of hyper-converged infrastructure). Using external storage in many cases makes the whole system more stable, as your data is stored
separately from compute nodes. A diagram showing external data storage communication with the compute nodes
External storage solutions are often popular in enterprise systems because such storage is
frequently provided by an external vendor, that takes care of its operations.
The integration with
Kubernetes involves only a small component installed in the cluster - the CSI driver. This driver
is responsible for provisioning volumes in this storage and attaching them to pods run by Kubernetes. However, such storage solutions can also be implemented using purely open-source technologies.
One of the popular solutions is
A diagram showing local data storage running on the compute nodes
On the other hand, hyper-converged systems are often implemented using local storage (when you do
not require replication) and with software-defined storages, often installed directly in Kubernetes,
A diagram showing clustered data storage running on the compute nodes
A hyper-converged system has its advantages. For example, data locality: when your data is stored
locally, access to such data is faster.
But there are disadvantages as such a system is usually
more difficult to manage and maintain. At Ænix, we wanted to provide a ready-to-utilize solution that could be used without the require to
purchase and setup an additional external storage, and that was optimal in terms of speed and
resource utilization. LINSTOR became that solution. The time-tested and industry-popular technologies such as LVM and ZFS as backend gives confidence
that data is securely stored.
DRBD-based replication is incredible fast and consumes a small amount
of computing resources. For installing LINSTOR in Kubernetes, there is the Piraeus project, which already provides a
ready-made block storage to utilize with KubeVirt. In case you are using Talos Linux, as we described in the
require to enable the necessary kernel modules in advance, and configure piraeus as described in the
Networking for Kubernetes VMs
Despite having the similar interface - CNI, The network architecture in Kubernetes is actually more
complex and typically consists of many independent components that are not directly connected to
each other.
In fact, you can split Kubernetes networking into four layers, which are described below. Node Network (Data Center Network)
The network through which nodes are interconnected with each other. This network is usually not
managed by Kubernetes, but it is an crucial one because, without it, nothing would work. In practice, the bare metal infrastructure usually has more than one of such networks e.g. one for node-to-node communication, second for storage replication, third for external access, etc.
A diagram showing the role of the node network (data center network) on the Kubernetes networking scheme
Configuring the physical network interaction between nodes goes beyond the scope of this article,
as in most situations, Kubernetes utilizes already existing network infrastructure. Pod Network
This is the network provided by your CNI plugin. The task of the CNI plugin is to ensure transparent
connectivity between all containers and nodes in the cluster.
Most CNI plugins implement a flat
network from which separate blocks of IP addresses are allocated for utilize on each node. A diagram showing the role of the pod network (CNI-plugin) on the Kubernetes network scheme
In practice, your cluster can have several CNI plugins managed by
. This approach is often used in
virtualization solutions based on KubeVirt -
The primary CNI plugin is used for integration with Kubernetes services, while additional CNI
plugins are used to implement private networks (VPC) and integration with the physical networks
be used to connect bridges or physical interfaces.
Additionally, there are specialized plugins
which are designed to provide
One additional aspect to keep in mind when running virtual machines in Kubernetes is the require for
IPAM (IP Address Management), especially for secondary interfaces provided by Multus.
This is
commonly managed by a DHCP server operating within your infrastructure. Additionally, the allocation
of MAC addresses for virtual machines may be managed by
Kubemacpool
Although in our platform, we decided to go another way and fully rely on
. This CNI plugin is based on OVN (Open Virtual Network) which
was originally developed for OpenStack and it provides a complete network solution for virtual
machines in Kubernetes, features Custom Resources for managing IPs and MAC addresses, supports
live migration with preserving IP addresses between the nodes, and enables the creation of VPCs
for physical network separation between tenants.
In Kube-OVN you can assign separate subnets to an entire namespace or connect them as additional
network interfaces using Multus. Services Network
In addition to the CNI plugin, Kubernetes also has a services network, which is primarily needed
for service discovery.
Contrary to traditional virtual machines, Kubernetes is originally designed to run pods with a
And the services network provides a convenient abstraction (stable IP addresses and DNS names)
that will always direct traffic to the correct pod. The same approach is also commonly used with virtual machines in clouds despite the fact that
their IPs are usually static. A diagram showing the role of the services network (services network plugin) on the Kubernetes network scheme
The implementation of the services network in Kubernetes is handled by the services network plugin,
The standard implementation is called
and is used in most clusters.
But nowadays, this functionality might be provided as part of the CNI plugin. The most advanced
implementation is offered by the
project, which may be run in kube-proxy replacement mode. Cilium is based on the eBPF technology, which allows for efficient offloading of the Linux
networking stack, thereby improving performance and security compared to traditional methods based
In practice, Cilium and Kube-OVN may be easily
unified solution that offers seamless, multi-tenant networking for virtual machines, as well as
advanced network policies and combined services network functionality.
External Traffic Load Balancer
At this stage, you already have everything needed to run virtual machines in Kubernetes. But there is actually one more thing. You still require to access your services from outside your cluster, and an external load balancer
will assist you with organizing this. For bare metal Kubernetes clusters, there are several load balancers available:
provides built-in implementation.
The role of a external load balancer is to provide a stable address available externally and direct
external traffic to the services network. The services network plugin will direct it to your pods and virtual machines as usual. A diagram showing the role of the external load balancer on the Kubernetes network scheme
In most cases, setting up a load balancer on bare metal is achieved by creating floating IP address
on the nodes within the cluster, and announce it externally using ARP/NDP or BGP protocols.
After exploring various options, we decided that MetalLB is the simplest and most reliable solution,
although we do not strictly enforce the utilize of only it. Another benefit is that in L2 mode, MetalLB speakers continuously check their neighbour's state by
sending preforming liveness checks using a memberlist protocol. This enables failover that works independently of Kubernetes control-plane. This concludes our overview of virtualization, storage, and networking in Kubernetes.
The technologies mentioned here are available and already pre-configured on the
platform, where you can attempt them with no limitations.
I'll detail how, on top of this, you can implement the provisioning of fully functional Kubernetes
clusters with just the click of a button. SOURCE 4: https://kubernetes.io/blog/2017/05/kubernetes-monitoring-guide/?m=1
Kubernetes: a monitoring guide
Jean-Mathieu Saponaro (Datadog)
Friday,
Container technologies are taking the infrastructure world by storm. While containers solve or simplify infrastructure management processes, they also introduce significant complexity in terms of orchestration.
That’s where Kubernetes comes to our rescue. Just like a conductor directs an orchestra,
oversees our ensemble of containers—starting, stopping, creating, and destroying them automatically to keep our applications humming along. Kubernetes makes managing a containerized infrastructure much easier by creating levels of abstractions such as
. We no longer have to worry about where applications are running or if they have enough resources to work properly.
But that doesn’t change the fact that, in order to ensure good performance, we require to monitor our applications, the containers running them, and Kubernetes itself. Rethinking monitoring for the Kubernetes era
Just as containers have completely transformed how we think about running services on virtual machines, Kubernetes has changed the way we interact with containers. The good news is that with proper monitoring, the abstraction levels inherent to Kubernetes provide a comprehensive view of your infrastructure, even if the containers and applications are constantly moving.
But Kubernetes monitoring requires us to rethink and reorient our strategies, since it differs from monitoring traditional hosts such as VMs or physical machines in several ways. Tags and labels become essential
With containers and their orchestration completely managed by Kubernetes, labels are now the only way we have to interact with pods and containers. That’s why they are absolutely crucial for monitoring since all metrics and events shall be sliced and diced using
across the different layers of your infrastructure.
Defining your labels with a logical and easy-to-comprehend schema is essential so your metrics shall be as useful as possible. There are now more components to monitor
In traditional, host-centric infrastructure, we were used to monitoring only two layers: applications and the hosts running them.
Now with containers in the middle and Kubernetes itself needing to be monitored, there are four different components to monitor and collect metrics from.
Applications are constantly moving
Kubernetes schedules applications dynamically based on scheduling policy, so you don’t always understand where applications are running.
But they still require to be monitored. That’s why using a monitoring system or tool with service discovery represents must. It will automatically adapt metric collection to moving containers so applications may be continuously monitored without interruption. Be prepared for distributed clusters
Kubernetes has the
to distribute containerized applications across multiple data centers and potentially different cloud providers. That means metrics must be collected and aggregated among all these different sources.
For more details about all these new monitoring challenges inherent to Kubernetes and how to overcome them, we recently published an
in-depth Kubernetes monitoring guide
. Part 1 of the series covers how to adapt your monitoring strategies to the Kubernetes era. Metrics to monitor
Whether you utilize
data or a monitoring tool integrating with Kubernetes and its different APIs, there are several key types of metrics that require to be closely tracked:
Running pods
such as CPU, memory usage, and disk I/O
Container-native
Application metrics for which a service discovery feature in your monitoring tool is essential
All these metrics should be aggregated using Kubernetes labels and correlated with events from Kubernetes and container technologies.
of our series on Kubernetes monitoring guides you through all the data that needs to be collected and tracked. Collecting these metrics
Whether you desire to track these key performance metrics by combining Heapster, a storage backend, and a graphing tool, or by integrating a monitoring tool with the different components of your infrastructure,
, about Kubernetes metric collection, has you covered. Anchors aweigh! Using Kubernetes drastically simplifies container management.
But it requires us to rethink our monitoring strategies on several fronts, and to create sure all the key metrics from the different components are properly collected, aggregated, and tracked. We hope our monitoring guide will assist you to effectively monitor your Kubernetes clusters. Feedback and suggestions
are more than welcome. obtain involved with the Kubernetes project on
Post questions (or answer questions) on
Stack Overflow
Connect with the community on
Follow us on Twitter
@Kubernetesio
SOURCE 5: https://kube.fm/not-kubernetes-danyl
Original series
Not Every Problem Needs Kubernetes
Bart Farrell
Danyl Novhorodov
This episode is brought to you by Testkube—where teams run millions of performance tests in real Kubernetes infrastructure.
From air-gapped environments to massive scale deployments, orchestrate every testing tool in one platform. Check it out at
Danyl Novhorodov
, a veteran .NET engineer and architect at Eneco, presents his controversial thesis that 90% of teams don't actually require Kubernetes. He walks through practical decision-making frameworks, explores powerful alternatives like BEAM runtimes and Actor models, and explains why starting with modular monoliths often beats premature microservices adoption.
You will learn:
The COST decision framework

- How to evaluate infrastructure choices based on Complexity, Ownership, Skills, and Time rather than industry hype

Platform engineering vs.
managed services

- How to honestly assess whether your team can compete with AWS, Azure, and Google's managed container platforms

Evolutionary architecture approach

- Why modular monoliths with clear boundaries often provide better foundations than distributed systems from day one

Relevant links
Not Every Problem Needs Kubernetes
Complexity tax
Modular monolith
ASP.NET Aspire
Technology rader from ThoughtWorks
Actor models
Proto.actor
Google Cloud Run
Transcription
Does every team truly require Kubernetes
?
Or are we paying a
that shouldn't be necessary? In this episode of
, we sit down with Daniel, a veteran
engineer and architect at
, to pressure test his thesis that not every problem needs Kubernetes. Daniel walks through the cost model, complexity, ownership, skills, and time—why a
often beats premature microservices, and how
changes the build versus buy calculus. We also explore alternatives such as
) for resilience without orchestration sprawl. Plus, the real edge cases where multi-region hybrid deployments justify Kubernetes.
If you're making 2025 infrastructure choices, this episode will definitely sharpen your decision tree. Special thanks to
for sponsoring today's episode. require to run tests in air-gapped environments? TestKube works completely offline with your private registries and restricted infrastructure. Whether you're in government, healthcare, or finance, you can orchestrate all your testing tools. Performance, API, and browser tests without any external dependencies.
Certificate-based auth, private NPM registries, enterprise OAuth—it's all supported. Your compliance requirements are finally met. Learn more at
Now, let's obtain into the episode. I'm glad to be here. I'm not a Kubernetes expert, but an engineer who knows how to build, deploy, and run apps in Kubernetes—like writing
charts. In this conversation, I'll primarily share perspectives from an engineer or architect's viewpoint. Tools that improve developer experience are of particular interest to me.
One such tool is
(D-A-P-R), which is part of the
. Another is
ASP.NET Aspire
, which is related to the
stack I work with. Introduced in
, it's a developer experience and application composition layer focused on building, wiring, and running distributed apps. I'll also mention
, a tool I used when deploying apps in Kubernetes. Could you give us a quick introduction about what you do and where you work? I've been a software professional
for more than 20 years.
During my career, my roles ranged from
. Early in my career, I started as a
engineer but later switched to
. I've worked in different companies from startups and scale-ups to big enterprises, across various industries including automotive, telecom, agriculture, hospitality, and banking. Currently, I'm employed by
, a producer of natural gas and supplier of electricity and heat in the Netherlands. I'm working on re-architecting some business-critical code and making it
.
Since you mentioned
, how did you obtain into cloud-native? Around 2013, I was working for a startup building a
for house property owners—a local analog of Nextdoor. It was a quite ambitious project. We were running workloads on a local VPS hosting provider, mainly Windows servers. was emerging at that time, and we started using basic Azure functionality and services like running websites, Cluelift mobile services,
Azure API Management
, and so on, eventually migrating more workloads to the cloud.
This was my first experience with cloud. We didn't have a lot of documentation and guidelines on how to build
applications back then. I believe the cloud-native term itself was introduced much later. So it was a trial and error approach, experimentation, and it was a lot of fun. This ecosystem of
and Kubernetes moves quite quickly. How do you keep up to date? Is it through blogs, videos, official documentation? For example, when you desire to learn about
, where are the places that you go?
I ask because we also have a monthly content analysis report where we track different trends about what people are looking at and what resources are most helpful. So in your case, how do you stay up to date with all the changes happening in these technologies? I read a lot of blogs from various sources. From time to time, I follow
blogs, but as I mentioned, I'm not a Kubernetes expert. I also keep an eye on the
technology reader from ThoughtWorks
, where you can observe emerging and trending technologies, and those which probably require to be retired.
There are quite a lot of technical books on different topics. These are my primary sources. If you could go back in time and share one career tip with your younger self, what would it be? Spend more time learning basics and foundation
. Most of the things in our industry are not new and were invented in the 50s, 60s, 70s, and 80s—
. What changes is the packaging, the syntax, and the hype. Mastering the boring fundamentals is often the most differentiating thing you can do in a world obsessed with shiny new tools.
As part of our monthly content discovery, we found an article that you wrote titled "
Not Every Problem Needs Kubernetes
". The article claims that 90% of teams don't require Kubernetes. Let that sink in. That could be controversial. Let's begin with some context: How did Kubernetes become such a dominant force in
container orchestration
explored containers around 2013, making them usable for everyone. Containers solved the "it works on my machine" problem by bundling code and dependencies.
However, a new challenge emerged: how to run hundreds or thousands of containers in production? Several orchestration tools emerged to solve scheduling, service discovery, and scaling problems.
Tools like
Docker Swarm
, and Kubernetes were developed. While others existed, Kubernetes stood out because it was open source, extensible, and backed by
. It was also battle-tested by
, Google's internal cluster management tool. Google open-sourced its Borg-like orchestrator at the right time, and support from
and cloud providers created a snowball effect. This effectively ended the orchestration wars, making Kubernetes the default choice. There's an argument that Kubernetes adoption is often driven by buzzwords and resume building rather than actual technical needs.
What patterns have you observed in organizations when they're choosing Kubernetes? Kubernetes often enters the room not through a clearly defined problem but through aspiration. Teams desire to level up, be
, or match what they think big players do. In many cases, this decision is politically or emotionally driven, not technically. Some examples I've seen include early-stage startups with five or ten developers and a single
server spending weeks
everything and creating
charts to deploy something that could run on a $20 VPS or
Azure App Service
.
The ops overhead is huge and essentially slows down development. Another example represents mid-sized enterprise where the decision came from the top down after a few engineers attended
. Suddenly, a small .NET monolith got split and shipped into a Kubernetes cluster, and no one knew how to debug anything anymore. Deploying pipelines became fragile, and the cost of development and onboarding increased. I also worked for a media company that handled hundreds of video streams daily.
On the surface, it sounds like a textbook utilize case for Kubernetes—with elasticity, batch workloads, and hardware scheduling. They did benefit in a few areas, such as services handling traffic spikes during elections or sports events. However, most media workloads like transcoding or packaging were offloaded to external services like
.
The team's actual job was orchestration, coordinating APIs, metadata, and availability. Even in this seemingly valid case, Kubernetes was partially justified but slightly over-engineered for most workflows. Of course, there are valid utilize cases for Kubernetes in companies with hundreds or millions of customers and often spiky, unpredictable usage patterns.
Kubernetes: The article suggests that most teams end up with a system so bloated and fragile when using Kubernetes. Is this a fair characterization or does it reflect poor implementation rather than inherent platform issues? Kubernetes itself is not a culprit. The platform is powerful and flexible, but with that power comes enormous complexity. Most teams underestimate what it takes to own it responsibly. Kubernetes may be clean, resilient, and efficient, but only in the hands of a team with a deep platform
## Understanding
, and other critical practices.
Without these, it's like giving a Formula One car to someone who just got their driving license. It's not that the platform is broken, but how it's often misapplied. The article mentions that without proper developer platforms built on top, Kubernetes becomes hostile to developers, requiring
or runtime layers like
. Is this additional complexity inevitable, or are there ways to keep Kubernetes developer-friendly without these layers? Kubernetes complexity is not inevitable, but without intentional design, Kubernetes is default.
Out of the box, Kubernetes is an infrastructure tool, not a developer experience. If we desire developers to be productive on Kubernetes without introducing
, or an entire platform engineering organization, we require to build a lightweight developer experience layer. At minimum, this should include simple deployment tooling because
is not enough. It should have clear abstractions that hide YAML as much as possible (avoiding complex
configurations). It should have built-in observability like logging, tracing, and metrics out of the box, preferably without requiring developers to configure
The platform should be easy to roll back and provide preview environments.
It's not developer-friendly unless it's debuggable and reversible. With these elements in place, Kubernetes becomes tolerable, maybe even smooth for developers. Without them, it could be a jungle of YAML, role-based access errors, and constant guesswork. Regarding service meshes like
, Runtime, and
: they solve real problems like traffic routing, retries, telemetry, and circuit breakers. However, these solutions are only necessary at a certain level of complexity.
You probably don't require them if you have a handful of internal APIs, basic network topology, and simple routing. But if you're running dozens of microservices with zero trust requirements, blue-green rollouts, and multi-tenant traffic policies, service meshes can simplify things by centralizing complexity that would otherwise be scattered across application code. However, these layers are not free—they require operational expertise, upgrade and compatibility management, and
## Debugging
There's an interesting point about
, and Actor models as alternatives for fault tolerance and scalability without orchestration complexity.
How do you observe these architectural patterns comparing to the Kubernetes approach? This represents hugely under-discussed area. While Kubernetes has become a default tool for orchestration, it's not the only or even the most elegant way to achieve it. Actor models
Proto.actor
-based runtimes (such as
) often offer a radically different paradigm that builds fault tolerance into the application layer itself. Both Kubernetes and actor-based models aim to solve problems like resilience, fault tolerance, and scalability, but they approach these from opposite ends.
Kubernetes is platform-agnostic, heavy, yet appealing because it's language-agnostic. It provides a unified control plane for compute, storage, and networking, and is backed by major cloud providers and ecosystems. However, Kubernetes achieves reliability through external orchestration.
It uses
, sidecars, nodes, and rescheduling, while the application itself remains unaware of its own lifecycle. This complexity comes at a cost—lots of plumbing,
configurations, and operational overhead. In contrast,
, and other Actor model systems offer intrinsic resilience. Crashing is expected and even embraced. Supervisors automatically restart failed processes. Processes are isolated and extremely lightweight. You obtain distributed messaging, concurrency, and fault recovery without needing an orchestrator.
When building apps in
, fault tolerance isn't an add-on—it's the foundation. Similarly, .NET's
provide virtual actors with concepts like location transparency and automatic lifecycle management. These systems scale horizontally without thinking in terms of pods or deployments—it's just actors and messages. Too many teams jump into Kubernetes for scalability and resilience without realizing these could be achieved with less complexity by choosing the right runtime model.
You don't always require a cluster of nodes; sometimes you just require a runtime that knows how to take care of itself, like
Kubernetes represents general-purpose tool, while the Actor Model is purposely built to solve these exact problems.
If you can afford to go all-in on these paradigms, Actor Models will often obtain you there faster and cheaper, with less configuration. You write high-level code, and the code knows how to handle itself. The article critiques teams that claim to have hundreds of
but actually have distributed monoliths with tightly coupled services. It raises the question of how teams can honestly evaluate whether their architecture truly benefits from Kubernetes orchestration.
proudly claim they have 300 or 400
, but under the hood, it's often a spaghetti mess of services that can't be deployed independently, fail together, or understand too much about each other's internal state. That's not truly microservices; it's a
, and Kubernetes won't fix this problem. It might even create it worse by giving just enough scaffolding to pretend it's working until it doesn't. microservice architecture
has certain aspects: a service should be independently deployable without coordination, owned by distinct teams with clear domain boundaries.
They should communicate over well-defined
or events, not internal contracts. They can fail independently without cascading impact, utilize separate storage or at least separate schemas to reduce coupling, and be tested and versioned in isolation. If all of this is true, you're on the right track of building microservices. Otherwise, you're probably not. Kubernetes is great at managing complexity, but it won't save you from complexity you invent yourself.
Platform engineering
is presented as essential for Kubernetes success, with the article suggesting that most organizations can't compete with managed services from major cloud providers like
Google Cloud
. What's your perspective on build versus buy for developer platforms? Platform engineering
is essential for Kubernetes to succeed in any meaningful and sustainable way. However, most companies are not in a position to build a reliable platform, nor should they attempt.
Building an internal developer platform on top of Kubernetes is not just about writing
templates and adding a CI pipeline. It requires a dedicated team of platform engineers with strong infrastructure code practices,
culture, consistent monitoring, logging, tracing, and alerting across all services. The platform should support an onboarding and training model, as well as incident response. This represents real engineering effort that requires at least three to five experienced engineers dedicated to platform work.
Otherwise, the internal platform is likely to become a patchwork of half-solutions that developers quietly resent. An uncomfortable truth is that cloud providers have already built great platforms and will do it better, cheaper, and more securely than 95% of internal teams. For example,
Azure App Services
AWS App Runner
Google Cloud Run
offer managed deployment, auto-scaling, zero-cluster operations, and easy rollback.
Platforms like
, and others provide self-service infrastructure without starting from scratch. My take is that if you're not ready to treat your platform like a product with a complete roadmap, support, and lifecycle management, you probably should not build one. The legitimate utilize cases mentioned include
multi-region redundancy
environments. Are these requirements becoming more common? Are they still edge cases for most organizations? Multi-region
are often thrown around as justifications for Kubernetes and infrastructure.
For most organizations, these are still edge cases. A real require emerges when you are legally or contractually required to keep data in specific regions. This could be, for example,
data residency laws, or serving latency-sensitive workloads across continents like low-latency gaming or live trading.
You might also be running workloads on-prem due to hardware requirements or legacy integrations. In these cases, Kubernetes might assist. However, you're not adopting Kubernetes because you are multi-region; you're adopting it despite the complexity of multi-region infrastructure.
Kubernetes helps to orchestrate some challenges, but most of the pain is still yours to own. Kubernetes doesn't solve multi-region problems completely. You still require to build things like global DNS, traffic routing, data replication strategies, and cross-region service discovery. For most companies, multi-region infrastructure is unnecessary. But if you genuinely require it, then proceed. There's a provocative statement that
is not a role, but a practice, in the context of needing dedicated Kubernetes teams.
So how should organizations approach the skills and team structure needed for Kubernetes? represents practice, not a job title. This doesn't mean you don't require dedicated people doing operational work. as a movement is about collaboration, automation, and shared responsibility, breaking silos between developers and operations. The idea is to empower teams to build, test, and deploy on their own. Kubernetes doesn't magically create this happen. In some cases, it reintroduces silos because the complexity is so high that only a few people can manage it.
Ironically, you end up needing a dedicated
, even if everyone is doing DevOps. Instead of asking who should own DevOps, we should ask how to structure teams so that product teams can ship safely, quickly, and autonomously, without drowning in complex configurations. Sometimes the answer could be Kubernetes with a platform team, sometimes it's app services and a
pipeline, and sometimes it's best not to utilize Kubernetes at all. The article suggests starting with
before considering Kubernetes.
This seems to go against the microservices-first trend. What's your take on this evolutionary approach to architecture? Modular monolith
as the first architectural step isn't nostalgia or fear of scale, it's about simplicity and evolutionary design. It's about giving your system room to grow without forcing you to maintain 15 distributed services before your product has even gained traction. Why begin with a monolith? Because it gives you the best of both worlds: deployment simplicity of a monolith and logical separation stability of
.
It's crucial to have a system built in a modular way, so the ability to refactor and extract services later is justified. You can transition to microservices more easily if your architecture is clean internally. You can extract modules into services later, deploy features independently within one artifact, and keep
,
## Debugging
Consider extracting microservices when a module is on a hot path and scaling disproportionately—for example, if it has a pricing engine or encoding process.
Or when you hit team-level friction where multiple groups interfere with the same code deployment, or require isolation and deployment independence. Do this based on real signals, not hype.
Too many teams chase microservices early because it sounds cool and they desire to mimic big tech, thinking it's the only modern way to scale. But they often lack the engineers to support multiple deployments, proper
Without a real product-market fit, you might end up building a
that is tightly coupled with fragile integration points. The key is to obtain the structure right, keep deployment simple, and evolve based on real needs, not external pressure.
Container services
AWS Fargate
Azure Container Apps
are positioned as a middle ground between traditional deployment and full Kubernetes orchestration. How do you evaluate when these solutions are sufficient versus needing full orchestration? Services like AWS Fargate
Azure Container Apps
represent a sweet spot for most teams. They provide containerization benefits like isolation and packaging without the pain of managing Kubernetes control plane or worrying about nodes, pools, ingress controllers, and cluster upgrades.
These services are often sufficient for 80 to 90% of utilize cases. You can stick with these tools when you have stateless services, APIs, or background jobs, and require simple autoscaling while wanting to move fast and deploy without infrastructure concerns. They work best when your app fits the cloud provider's opinionated runtime, with no require for advanced scheduling or sidecars, and you're focused on cost efficiency and developer velocity. For example, the team I worked with deployed over 10 services across environments using
Azure Container Apps
for lightweight messaging.
They didn't require full Kubernetes and shipped features weeks faster with fewer operational headaches. Kubernetes should be an exception, not the starting point. For most teams, managed container services are more than enough. The article implies that Kubernetes adoption is often driven by resume building or following trends rather than solving real problems. How can technical leaders create more objective
Two main infrastructure decisions
begin with "everyone's using Kubernetes, we should too" instead of asking what problem we are trying to solve and whether Kubernetes is the best fit.
As technical leaders, our job isn't to chase innovation, but to protect focus and simplicity. That means filtering out the hype and making deliberate and grounded choices. We should ask:
## What is
? We can utilize a cost model that stands for complexity, ownership, skills, and time. For complexity, ask what new cognitive or system complexity it adds. For ownership, consider who will maintain it after the initial excitement fades. Regarding skills, do we have the necessary in-house expertise, or will we become dependent on a hero engineer?
On the time scale,
## What is
? One of the healthiest habits leaders can cultivate is running small-scoped experiments. attempt Kubernetes for one non-critical service first, and measure not just performance, but deployment velocity, developer happiness, and maintenance overhead. Run the same service on a managed container platform like
AWS Fargate
Azure Container Apps
and compare. Pay attention to the new documental glue code and tribal knowledge required to run it in Kubernetes—that's the hidden cost.
Some strategies for separating hype from value:
Challenge buzzword fluency by asking people to describe technical choices in plain terms
If the case for Kubernetes can't be made without acronyms, the case probably isn't strong
Involve product teams and ask what they require to ship faster
If they don't mention custom
, that tells you something
Hold technical justification reviews before adopting any tool
The team must answer what pain is being solved and what happens if the tool isn't adopted
If Kubernetes is the right tool, great.
But if it's not, don't be afraid to choose boring, reliable, and well-understood technology. While the article is critical, Kubernetes has become the de facto standard for
container orchestration
. Kubernetes has brought massive, undeniable innovation to the industry, even if it's been over-applied in places it doesn't belong. We should give credit where it's due. Before Kubernetes, we had a wild west of do-it-yourself scripts, hand-rolled batch deployments, and other challenges.
What Kubernetes gave the industry represents common vocabulary:
, etc. It provided a standard API for interacting with clusters and a modular model that allows extensibility via
, and controllers. This created a level of portability and predictability across teams, clouds, and companies that simply didn't exist before. It's the reason we now expect a containerized app to just work across environments. Kubernetes embraced the idea of immutable deployments—build once and run everywhere.
It introduced declarative infrastructure, version control with
as a source of truth, tools like
culture, and self-healing behavior through controllers and reconciliation loops. These ideas weren't new, but Kubernetes turned them into practical defaults. From this came modern practices like progressive delivery, infrastructure drift detection and correction, and GitOps tools. Interestingly, Kubernetes also succeeded by highlighting its own complexity.
This pressure led to the rise of cloud provider solutions like
Azure Container Apps
Google Cloud Run
AWS Fargate
, and lightweight Kubernetes platforms like
, and serverless Kubernetes platforms. Abstractions like
, and others appeared as responses to Kubernetes' complexity, helping users navigate its challenges. My argument isn't that Kubernetes is bad, but that it's a powerful tool that should be used with intent, not by default. Kubernetes set a bar that others wanted to meet and exceed, but in a more user-friendly way.
Last year we celebrated 10 years of Kubernetes, and we've asked quite a few people about what they expect to happen in the future. Looking forward, do you observe the pendulum swinging back towards simpler solutions, or will the complexity of Kubernetes eventually be abstracted away enough to create it accessible to all teams? I observe this pendulum swinging back towards simplification, and it's already happening. The industry is starting to collectively admit that Kubernetes is powerful, but do we truly require all of this for most workloads?
We are seeing shifts from raw orchestration power towards better developer experience, some abstractions, and just enough infrastructure. Platforms like
Azure Container Apps
Google Cloud Run
are taking about 80-20 of Kubernetes and baking it into a zero-ops experience. You bring the container, and they handle scaling, networking, certifications, and routing. Other tools like internal developer platforms (
) are abstracting Kubernetes. They call it golden path templates and cells with rapid, self-service UIs so that developers never even touch configuration YAMLs.
Projects like
, and lightweight Kubernetes runtimes like
, and others are emerging. I think we'll still be using Kubernetes under the hood, but most developers won't understand or care. It shall be part of platform-as-a-service with opinionated defaults and tight feedback loops. We won't cease using Kubernetes; we'll cease talking about it. It's more about having less Kubernetes in your face—using abstractions and tooling that are developer-friendly without needing to deep dive and comprehend everything behind it.
One counter argument to the article's thesis is that
represents necessary complexity, that the problems it solves are inherently difficult and simpler solutions are often incomplete. Do you observe this complexity as fundamental to the problem space, or will we eventually observe tools that can abstract it away while maintaining the same capabilities? Distributed systems
are inherently complex because you require to deal with partial failures, network partitions, scheduling, container scaling, and security.
There is no silver bullet. In this sense, Kubernetes isn't introducing complexity; it's surfacing and managing it. However, it exposes a firehose of knobs and levers that most teams aren't equipped to handle.
Kubernetes tries to abstract away some essential complexity, like scheduling and auto-healing, but it also introduces a lot of accidental complexity—
, Kargo, etc. The danger is that teams adopting Kubernetes thinking they are getting simplicity are actually trading one class of complexity for another, probably harder to debug one. Can we abstract Kubernetes without losing power? We are getting closer, and emerging patterns demonstrate promise. Big platforms like
Azure Container Apps
AWS App Runner
hide the cluster entirely while offering auto-scaling, metrics, and networking.
These services are Kubernetes-backed but users never touch YAML. Internal platforms can build paved roads and self-service portals that surface just enough control without exposing every Kubernetes knob. Tools like
let teams orchestrate declaratively across clouds without handwriting manifests. You can abstract complexity but can't delete it. Kubernetes solves hard problems, but that doesn't mean you have those problems. Its complexity is often justified at certain scales, but premature exposure to that complexity hurts teams.
The future isn't about dumbing down; it's about designing platforms that let us grow into complexity only when we actually require it. Smart tools, sensible defaults, and fewer footguns will win—not less Kubernetes, but less Kubernetes in your face. With that in mind, what's next for Kubernetes, what's next for you? I continue working on migrating legacy applications to cloud-native, refining our architecture to align with
and related streams. On my side projects, I truly desire to discover time and dive deeper into the beautiful world of
actor model to comprehend its true power of fault tolerance and scalability.
The topic you chose for today may cause a little bit of controversy. I'm sure you've already experienced people with opinions. But whether people disagree or agree, what's the best way to obtain in touch with you? I'm on LinkedIn, so feel free to drop me a message. I'm often checking it, which will probably be the best way to reach me. I noticed that the provided transcript is highly short and seems to be a closing remark from the host to the guest. Without more context from the full conversation, I cannot confidently add hyperlinks.
Could you provide more of the transcript or context about the discussion? I noticed that the provided transcript is extremely short and contains only "Thank you highly much." Without more context from the full transcript, I cannot confidently apply hyperlinks. Could you provide more of the transcript or context about the conversation? Listen anywhere
Apple Podcast
YouTube Music
Amazon Music
Pocket Casts
SOURCE 6: https://veducate.co.uk/bridging-old-hypervisors-to-cloud-native-platforms-what-youll-learn-at-cisco-live-amsterdam/
Are you planning or already facing a migration from legacy hypervisors to modern, cloud native platforms?
The toughest part usually isn’t compute or storage, it’s
. If static IPs, subnet constraints, and complex topologies are slowing you down, Cisco Live Amsterdam is the place to fix that. This year, I’m delivering two brand-new sessions focused on making VM migrations faster, safer, and more predictable using
, and cloud native technologies like

1. Breakout Session: Taking Away the Network Pain from Cross-Hypervisor Migrations with Isovalent Network Bridge [BRKCLD-1713]

Thursday, Feb 12, 1:00 PM – 2:30 PM CET
Session Type:
Technical Level:
Introductory
Technology:
Observability, Cloud Native, Data Center
Session Link:
View BRKCLD-1713 in the Cisco Live catalog
Migrating VMs between hypervisors and across datacenter boundaries is rarely “lift-and-shift.” Most tools assist you move configurations and storage, but leave you with the hardest challenge:
network accessibility, routing, and security
This session focuses on removing that network friction using
Isovalent Network Bridge
eBPF-powered
solution that enables seamless connectivity between VM and Kubernetes workloads, regardless of where they are placed or migrated to.
In this session you will learn how to:
virtual machines and containers together
using cloud native hypervisors built on Kubernetes and KubeVirt. cloud native networking
datacenter connectivity
migration tooling, risks, and considerations
for cross-hypervisor moves. when migrating VMs from legacy hypervisor platforms. Leverage Cisco’s latest networking products to
simplify VM network migration
If you are an infrastructure, networking, or platform engineer and you desire to de-risk VM moves without endless re-IP and downtime, this breakout session is for you.
→ Reserve your spot:
BRKCLD-1713 – Taking Away the Network Pain from Cross-Hypervisor Migrations

2. Technical Seminar: Being Successful Moving from Legacy Hypervisors to Cloud Native Platforms with Cisco and Isovalent [TECCLD-1773]

Monday, Feb 9, 2:15 PM – 6:45 PM CET
Session Type:
Technical Seminar
Technical Level:
Introductory
Technology:
Observability, Cloud Native, Data Center
Session Link:
View TECCLD-1773 in the Cisco Live catalog
Note: Technical Seminars are priced in addition to your Full Conference or IT Leadership pass and may be added via the Cisco Live registration portal.
This in-depth
Technical Seminar
is designed for teams who desire to build a clear, actionable roadmap from
legacy hypervisors to cloud native platforms
We will compare traditional hypervisors with emerging
cloud native hypervisors
Kubernetes networking fundamentals
, and demonstrate how network and security design decisions directly impact the success of your migration strategy. We then go deeper into
Isovalent Network Bridge
and how it preserves network identity to keep workloads reachable and secure, even as they move across
OpenShift Virtualization
, and other platforms.
By the end of this seminar, you will:
comprehend the current state of
VM migration between hypervisors
and where networking complexity arises. Gain working knowledge of
Kubernetes networking
and how it applies to virtual machines. strategies to minimize downtime
and disruption during migrations. Isovalent Network Bridge
simplifies workload mobility across platforms. of bridging datacenter networks with cloud native networking and security. This seminar is ideal if you are responsible for
VM operations, datacenter networking, or platform engineering
and desire a practical, end-to-end view of modernizing your stack.
→ Add this seminar to your registration:
TECCLD-1773 – Being Successful Moving from Legacy Hypervisors to Cloud Native Platforms
Why You Should Attend Both Sessions
Together, these two sessions provide a
TECCLD-1773
gives you the big-picture strategy, foundational knowledge, and detailed demos for moving from legacy hypervisors to cloud native platforms. BRKCLD-1713
network migration pain points
and shows how to solve them with
Isovalent Network Bridge and Cisco networking
If your organization is planning a migration, modernizing your datacenter, or exploring cloud native architectures, these sessions will assist you:
Reduce risk and downtime during migrations.
Avoid costly re-IP and redesign work. Align network, platform, and application teams around a unified approach. Join Me at Cisco Live Amsterdam
Cisco Live is the perfect place to learn, ask questions, and benchmark your strategy against what others in the industry are doing. If you desire to
turn VM migration from a risky project into a repeatable, well-understood process
, create sure you add these sessions to your schedule:
BRKCLD-1713:
Taking Away the Network Pain from Cross-Hypervisor Migrations with Isovalent Network Bridge –
View & enroll
TECCLD-1773:
Being Successful Moving from Legacy Hypervisors to Cloud Native Platforms with Cisco and Isovalent –
View & add to your registration
I’m looking forward to meeting you in Amsterdam and diving into how we can create your next migration smoother, safer, and truly cloud native.
If you’d like to stay in touch or discuss your migration plans,
feel free to connect with me on LinkedIn. Found this useful? Then share:
Click to share on X (Opens in new window)
Click to share on LinkedIn (Opens in new window)
Click to email a link to a friend (Opens in new window)
Click to share on Facebook (Opens in new window)
Click to share on Reddit (Opens in new window)
Click to print (Opens in new window)
SOURCE 7: https://bibleandbookcenter.com/read/mastering-kubernetes/
Gigi Sayfan
Mastering Kubernetes
: Gigi Sayfan
Packt Publishing Ltd
DOWNLOAD NOW »
Exploit design, deployment, and management of large-scale containers Key Features Explore the latest features available in Kubernetes 1.10 Ensure that your clusters are always available, scalable, and up to date Master the skills of designing and deploying large clusters on various cloud platforms Book Description Kubernetes is an open source system that is used to automate the deployment, scaling, and management of containerized applications.
If you are running more containers or desire automated management of your containers, you require Kubernetes at your disposal. To put things into perspective, Mastering Kubernetes walks you through the advanced management of Kubernetes clusters. To begin with, you will learn the fundamentals of both Kubernetes architecture and Kubernetes design in detail. You will discover how to run complex stateful microservices on Kubernetes including advanced features such as horizontal pod autoscaling, rolling updates, resource quotas, and persistent storage backend.
Using real-world utilize cases, you will explore the options for network configuration, and comprehend how to set up, operate, and troubleshoot various Kubernetes networking plugins. In addition to this, you will obtain to grips with custom resource development and utilization in automation and maintenance workflows.
To scale up your knowledge of Kubernetes, you will encounter some additional concepts based on the Kubernetes 1.10 release, such as Promethus, Role-based access control, API aggregation, and more.
By the end of this book, you’ll understand everything you require to graduate from intermediate to advanced level of
## Understanding
SOURCE 8: https://thenewstack.io/migrating-vms-to-kubernetes-a-roadmap-for-cloud-native-enterprises/
Join our community of software engineering leaders and aspirational developers. Always
stay in-the-understand by getting the most crucial news and exclusive content delivered
fresh to your inbox to learn more about at-scale software development.
EMAIL ADDRESS
RESUBSCRIPTION REQUIRED
It seems that you've previously unsubscribed from our newsletter
in the past. Click the button below to open the re-subscribe form
in a new tab. When you're done, simply close that tab and continue
with this form to complete your subscription. RE-SUBSCRIBE
The New Stack does not sell your information or share it with
unaffiliated third parties. By continuing, you agree to our
Terms of utilize
Privacy Policy
Welcome and thank you for joining The New Stack community!
Please answer a few simple questions to assist us deliver the news and resources you are interested in. COMPANY NAME
United States
United Kingdom
Afghanistan
American Samoa
Antigua and Barbuda
Asia/Pacific Region
Bonaire, Sint Eustatius and Saba
Bosnia and Herzegovina
Bouvet Island
British Indian Ocean Territory
Brunei Darussalam
Burkina Faso
Cayman Islands
Central African Republic
Christmas Island
Cocos (Keeling) Islands
Congo, The Democratic Republic of the
Cook Islands
Czech Republic
Côte d'Ivoire
Dominican Republic
El Salvador
Equatorial Guinea
Falkland Islands (Malvinas)
Faroe Islands
French Guiana
French Polynesia
French Southern Territories
Guinea-Bissau
Heard Island and Mcdonald Islands
Holy observe (Vatican City State)
Iran, Islamic Republic Of
Isle of Man
Korea, Republic of
Libyan Arab Jamahiriya
Liechtenstein
Marshall Islands
Micronesia, Federated States of
Moldova, Republic of
Netherlands
Netherlands Antilles
New Caledonia
New Zealand
Norfolk Island
North Korea
North Macedonia
Northern Mariana Islands
Palestinian Territory, Occupied
Papua New Guinea
Philippines
Pitcairn Islands
Puerto Rico
Russian Federation
Saint Barthélemy
Saint Helena
Saint Kitts and Nevis
Saint Lucia
Saint Martin
Saint Martin
Saint Pierre and Miquelon
Saint Vincent and the Grenadines
Sao Tome and Principe
Saudi Arabia
Serbia and Montenegro
Sierra Leone
Sint Maarten
Solomon Islands
South Africa
South Georgia and the South Sandwich Islands
South Sudan
Svalbard and Jan Mayen
Switzerland
Syrian Arab Republic
Tanzania, United Republic of
Timor-Leste
Trinidad and Tobago
Turkmenistan
Turks and Caicos Islands
United Arab Emirates
United Kingdom
United States
United States Minor Outlying Islands
Virgin Islands, British
Virgin Islands, U.S.
Wallis and Futuna
Western Sahara
Åland Islands
Great to meet you! Tell us a bit about your job so we can cover the topics you discover most relevant. VP/Director
Manager/Supervisor
Mid Level or Senior Non-Managerial Staff
Entry Level/Junior Staff
Freelancer/Contractor
Student/Intern
Which of these most closely describes your job role? Developer/Software Engineer
SysAdmin/Operations/SRE
Security Professional
DevOps Engineer/Team
Community Manager/Developer Advocate
IT management, including CIO/CISO/CTO
Business Development/Marketing/Sales
Enthusiast/Hobbyist
How many employees are in the organization you work with?
Self-employed
I am not working
What option best describes the type of organization you work for? “End user” organization that primarily uses IT products and services to support their business deliverables
Hardware / software vendor or supplier
Cloud service provider or managed service provider
System integrator or IT consulting firm
Which of the following best describes your organization's primary industry? Advertising/Marketing
Aerospace/Aviation
Agriculture
Biotech/Pharmaceutical
Business Services (accounting, consulting, etc.)
Computers/Information Technology
Construction
Facilities/Service Industry
Finance/Financial Services (banking, insurance, etc.)
Human Resources
Life sciences (biotech, pharmaceuticals, etc.)
Manufacturing
Real Estate
Retail/Consumer Goods
Telecommunications
Transportation/Logistics
Travel/Hospitality/Entertainment
Utility/Energy
LINKEDIN PROFILE URL
We’re so glad you’re here.
You can expect all the best TNS content to arrive
Monday through Friday to keep you on top of the news and at the top of your game. Check your inbox for a confirmation email where you can adjust your preferences
and even join additional groups.
Follow TNS on your favorite social media networks. TNS follower on LinkedIn
the latest featured and trending stories
while you wait for your
first TNS newsletter. SOURCE 10: https://www.ibm.com/think/insights/kubernetes-migration
Kubernetes migration strategy and best practices
Published 29 October 2025
Stephanie Susnjara
Staff Writer
Ian Smalley
Staff Editor
Kubernetes migration strategy and best practices
migration strategy consists of a step-by-step plan for moving applications and workloads to a containerized environment, including best practices for a successful outcome.
As businesses modernize applications and adopt cloud-based technologies like
container orchestration
platforms to reliably and efficiently manage
As the dominant orchestration platform, Kubernetes enables enterprises to
journeys, facilitating the transition of legacy applications to
According to a 2024 Cloud Native Computing Foundation (CNCF) study, cloud-native adoption has reached 89%, with 93% of organizations now using, piloting or evaluating Kubernetes.
To carry out a seamless Kubernetes migration, organizations require a robust strategy that involves careful planning designed to capture business and technology opportunities while overcoming challenges. Industry newsletter
The latest tech news, backed by expert insights
Stay up to date on the most crucial—and intriguing—industry trends on AI, automation, data and beyond with the Think newsletter. observe the
IBM Privacy Statement
Thank you! You are subscribed.
Your subscription shall be delivered in English. You will discover an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe
. Refer to our
IBM Privacy Statement
for more information. Overview of Kubernetes
Originally developed by Google, Kubernetes is an
container orchestration platform maintained by the Cloud Native Computing Foundation (CNCF) since 2015. Also known as k8s or kube, this platform schedules and
the deployment, management and scaling of
Before Kubernetes, applications typically ran on dedicated servers or
virtual machines (VMs)
, which made scaling expensive and time-consuming.
In modern containerized settings, a runtime engine (typically
) allows developers to build, deploy, run, update and manage
. Kubernetes provides the orchestration layer needed to manage hundreds or thousands of containers at scale. Today, Docker and Kubernetes are the leading containerization tools. Kubernetes deployment
occurs through clusters made up of nodes—each representing a physical machine or VM. Every cluster has a main node that manages a control plane (including the
database).
Kubernetes applications run in pods, which are the smallest deployable units. They typically contain
based containers that share
Key features of Kubernetes include deployments for managing app lifecycles and replica sets,
for service delivery, and namespaces for resource isolation. The Kubernetes API server (accessed through the kubectl command-line tool) manages configuration and orchestrates communication between components. Persistent volumes handle storage needs.
Kubernetes is open source, which allows organizations to avoid vendor lock-in. and other teams benefit from a global community that contributes improvements and security patches. All major cloud service providers offer managed Kubernetes services, including Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform and IBM Cloud®. Red Hat OpenShift AI on IBM Cloud: Deploy AI workloads
utilize AI capabilities with Red Hat OpenShift on IBM Cloud.
This video explores how to build, deploy and manage AI workloads efficiently with a scalable machine learning operations platform. Explore OpenShift
Benefits of Kubernetes migration
A Kubernetes migration delivers both technical and organizational benefits, including:
Better resource utilize and cost efficiency
Improved reliability and high availability
Faster deployment
Enhanced team productivity and autonomy
Better resource utilization and cost efficiency
applications up or down based on demand.
For instance, during peak traffic periods, such as a flash sale on an
website, it spins up more pods to handle
. When the sale is over and demand decreases, Kubernetes scales back down to conserve resources. Improved reliability and high availability
Kubernetes helps keep apps stable and available through distribution across clusters and automatic recovery from failures. This helps maintain
Faster deployment
Kubernetes enables
at scale to assist streamline
and allow teams to create changes whenever needed.
Rolling updates deploy new versions gradually, monitoring for issues before completing the rollout. If problems emerge, rollbacks happen quickly. Enhanced team productivity and autonomy
With Kubernetes, teams own specific services and can work independently. This includes deploying on independent schedules, making technology choices that fit their needs and innovating at the right pace. Platform teams provide shared services like monitoring, logging and a
that all teams utilize, creating consistency without constraining innovation.
Planning a Kubernetes migration
Successful migrations begin with thorough planning that involves the following steps:
Assess your current environment. Evaluate team skills and training needs. Choose your migration approach. Plan your cluster strategy. Assess your current environment
begin by cataloging your existing applications,
and dependencies. Identify which applications are good candidates for early migration. Typically, stateless applications with well-defined APIs work best as starting points, while stateful apps require more planning.
Document dependencies between applications, including
and external services that require consideration during migration. Evaluate your current infrastructure to determine whether a managed Kubernetes service from a cloud provider or a self-hosted solution makes more sense for your organization. Consider factors like internal expertise, compliance requirements and budget constraints. Evaluate team skills and training needs
Assess your team’s current capabilities with containers, Kubernetes and cloud-native practices.
According to an
IBM Institute for Business Value 2023 report
, approximately 58% of global decision-makers report that cloud skills remain a considerable challenge. Identify skill gaps early and develop training plans that include hands-on experience. Choose your migration approach
Decide between different migration strategies based on application characteristics and business priorities.
A
approach containerizes existing applications with minimal changes, allowing faster migration but potentially missing optimization opportunities. Refactoring applications to be cloud-native takes longer but delivers better performance, scalability and cost efficiency. Many organizations adopt a phased approach. This approach entails starting with simple applications to build experience, then tackling more complex workloads as team expertise grows. Plan your cluster strategy
Determine whether you should utilize a single cluster or multiple clusters in your new environment.
Single clusters are simpler to manage but might not meet requirements for isolation, compliance or geographic distribution. Multiple clusters provide better isolation and can improve reliability, but require more sophisticated configuration management that uses tools like
Implementing a Kubernetes migration strategy and best practices
After the planning phase, Kubernetes migration can begin, guided by these best practices:
Ready applications for Kubernetes.
Build CI/CD pipelines for automated deployments. Monitor and log effectively. Secure Kubernetes clusters. Ready applications for Kubernetes
Container images
begin by building
with multi-stage builds. This technique keeps build dependencies separate from what runs in production, shrinking your image size and reducing security vulnerabilities. Run containers as non-root users. In this case, if something gets compromised, the damage stays limited. utilize consistent tagging and organize your registry clearly so that you can easily track versions.
Health checks
Proper health checks assist Kubernetes manage your applications effectively. Liveness probes tell Kubernetes whether a pod needs to restart, while readiness probes indicate when pods can accept traffic. Applications should expose endpoints that verify application health by checking database connectivity, external dependencies or internal state. For networking, configure service discovery and load balancing by using Kubernetes services so applications can discover dependencies through service names rather than hardcoded IP addresses.
For external traffic, implement
resources with appropriate controllers that handle SSL termination and routing. Configuration
Externalize all configuration from application code by using YAML files. utilize ConfigMaps for non-sensitive configuration and Secrets for sensitive data like database credentials and API keys, enabling the same container image to run across development, staging and production with different configurations. Build CI/CD pipelines for automated deployments
CI/CD pipelines
that handle everything from code commit to production deployment, with automated testing that includes
and deployment verification.
Container images obtain built automatically and pushed to registries with proper versioning. Monitor and log effectively
Deploy comprehensive monitoring covering resource utilize (
, memory and storage), app performance and business metrics, with dashboards for visualization. Centralized logging helps debug issues across many pods and services. Secure Kubernetes clusters
role-based access control (RBAC)
for defining permissions, network policies to control traffic between pods, and
for data at rest and in transit.
Scan container images regularly for vulnerabilities and keep base images updated. Kubernetes migration challenges
Even well-planned Kubernetes migrations face obstacles. On-premises legacy applications often weren’t designed for containerized environments and might rely on outdated configurations, specific server setups or local storage that don’t work in Kubernetes. External systems like databases and third-party services also add complexity. Thorough testing and validation in staging environments assist identify and resolve compatibility issues before production.
Protecting data is paramount. Run old and new systems in parallel temporarily, syncing data between environments and validating functionality before fully cutting over to Kubernetes. This running in parallel helps minimize downtime and reduce data loss. Kubernetes migration tools and services
Cloud providers and other technology companies offer a range of Kubernetes migration tools and services, including the following. Migration and deployment services
Cloud service providers offer managed services for assessing workloads, containerization and automated deployment.
For example, Amazon EKS, Microsoft Azure AKS and IBM Cloud Kubernetes Service all offer migration services. infrastructure-as-code (IaC)
assist developers automate cluster
and add-on installation and configuration management. Monitoring and observability platforms
These tools provide monitoring,
and troubleshooting capabilities so teams can gain insights into the behavior, health and performance of their Kubernetes environments. Consulting services
Technology and consulting firms provide Kubernetes expertise that complements migration tools.
Such services can guide strategic decisions from initial assessment through post-migration optimization, helping organizations navigate complex technical and organizational challenges. Link copied
obtain Kubernetes pricing options
Deploy and manage your containerized apps with ease by using IBM Kubernetes Service. Customize your infrastructure, choose your orchestration platform and optimize your workload with secure, scalable solutions tailored to your business needs.
Explore the service
Product documentation
Master Kubernetes on IBM Cloud
obtain started quickly with IBM Cloud Kubernetes Service and deploy containerized applications at scale. This step-by-step guide walks you through the essentials, from preparing your account to deploying your first cluster and app. obtain started
Build a resilient hybrid cloud strategy
Unlock the full potential of your business with a flexible, secure and resilient hybrid cloud.
IBM’s open hybrid cloud approach enables you to build and manage workloads without vendor lock-in, ensuring flexibility and performance across your IT landscape. Explore hybrid cloud
## What is
? Serverless computing enables developers to build and run application code without provisioning or managing servers or backend infrastructure. Read the article
Master YAML basics for Kubernetes
Learn the essentials of YAML and how it simplifies Kubernetes configuration.
This guide covers the key structures and tips to write effective YAML files for managing your Kubernetes deployments. begin the tutorial
Containers unleashed: The power behind modern applications
Learn how containers revolutionize the way businesses develop, deploy and manage applications. Discover why this technology represents game-changer for scalability, security and efficiency in today’s IT landscape. Read the article
ExxonMobil fuels innovation with IBM Cloud
Discover how ExxonMobil transformed customer experience at the pump by using IBM Cloud for scalability, speed and innovation.
Learn how cloud technology helped them reduce costs and streamline operations. Read the case study
Related solutions
IBM Red Hat OpenShift
Red Hat OpenShift on IBM Cloud represents fully managed OpenShift Container Platform (OCP). Explore Red Hat OpenShift
Container Solutions
Container solutions run and scale-up containerized workloads with security, open source innovation, and rapid deployment. Explore containers
Cloud Consulting Services
Unlock new capabilities and drive business agility with IBM’s cloud consulting services.
Discover how to co-create solutions, accelerate digital transformation, and optimize performance through hybrid cloud strategies and expert partnerships. Cloud services
Take the next step
obtain started with a fully managed Red Hat OpenShift platform or explore the flexibility of the IBM Cloud Kubernetes ecosystem. Accelerate your development and deployment process with scalable, secure solutions tailored to your needs. Explore Red Hat OpenShift
Explore Kubernetes
CNCF Research Reveals How Cloud Native Technology is Reshaping Global Business and Innovation, Cloud Native Computing Foundation (CNCF)
, 1 April 2025
This information is tailored for the alertmend.io platform, providing comprehensive insights and solutions.

